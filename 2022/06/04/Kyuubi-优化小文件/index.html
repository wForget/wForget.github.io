<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">
<meta name="google-site-verification" content="nPT3ONUGntVHfQCZH2D5GcUMgg5DH4IReN4GIs0GrW8" />








<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Kyuubi,Spark," />










<meta name="description" content="Spark 小文件问题Hive 表中太多的小文件会影响数据的查询性能和效率，同时加大了  HDFS NameNode 的压力。Hive (on MapReduce) 一般可以简单的通过一些参数来控制小文件，而 Spark 中并没有提供小文件合并的功能。下面我们来简单了解一下 Spark 小文件问题，以及如何处理小文件。 环境Kyuubi 版本：1.6.0-SNAPSHOT Spark 版本：3.1">
<meta property="og:type" content="article">
<meta property="og:title" content="Kyuubi 优化小文件">
<meta property="og:url" content="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/index.html">
<meta property="og:site_name" content="wForget&#39;s blog">
<meta property="og:description" content="Spark 小文件问题Hive 表中太多的小文件会影响数据的查询性能和效率，同时加大了  HDFS NameNode 的压力。Hive (on MapReduce) 一般可以简单的通过一些参数来控制小文件，而 Spark 中并没有提供小文件合并的功能。下面我们来简单了解一下 Spark 小文件问题，以及如何处理小文件。 环境Kyuubi 版本：1.6.0-SNAPSHOT Spark 版本：3.1">
<meta property="og:locale">
<meta property="og:image" content="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/1.png">
<meta property="og:image" content="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/2.png">
<meta property="og:image" content="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/3.png">
<meta property="og:image" content="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/4.png">
<meta property="og:image" content="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/5.png">
<meta property="og:image" content="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/6.png">
<meta property="og:image" content="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/11.png">
<meta property="og:image" content="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/10.png">
<meta property="og:image" content="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/7.png">
<meta property="og:image" content="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/8.png">
<meta property="og:image" content="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/9.png">
<meta property="og:image" content="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/15.png">
<meta property="og:image" content="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/14.png">
<meta property="og:image" content="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/13.png">
<meta property="og:image" content="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/12.png">
<meta property="article:published_time" content="2022-06-04T14:42:27.000Z">
<meta property="article:modified_time" content="2022-10-18T12:17:03.768Z">
<meta property="article:author" content="wangz">
<meta property="article:tag" content="Kyuubi">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://wforget.github.io/2022/06/04/Kyuubi-优化小文件/"/>





  <title>Kyuubi 优化小文件 | wForget's blog</title>
  








<meta name="generator" content="Hexo 6.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">wForget's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Kyuubi 优化小文件</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-06-04T22:42:27+08:00">
                2022-06-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Spark-小文件问题"><a href="#Spark-小文件问题" class="headerlink" title="Spark 小文件问题"></a>Spark 小文件问题</h2><p>Hive 表中太多的小文件会影响数据的查询性能和效率，同时加大了  HDFS NameNode 的压力。Hive (on MapReduce) 一般可以简单的通过一些参数来控制小文件，而 Spark 中并没有提供小文件合并的功能。下面我们来简单了解一下 Spark 小文件问题，以及如何处理小文件。</p>
<h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>Kyuubi 版本：1.6.0-SNAPSHOT</p>
<p>Spark 版本：3.1.3、3.2.0</p>
<h3 id="TPCDS-数据集"><a href="#TPCDS-数据集" class="headerlink" title="TPCDS 数据集"></a>TPCDS 数据集</h3><p>Kyuubi 中提供了一个 TPCDS Spark Connector，可以通过配置 Catalog 的方式，在读取时自动生成 TPCDS 数据。</p>
<p>只需要将 <code>kyuubi-spark-connector-tpcds_2.12-1.6.0-SNAPSHOT.jar</code> 包放入 Spark jars 目录中，并配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.catalog.tpcds=org.apache.kyuubi.spark.connector.tpcds.TPCDSCatalog;</span><br></pre></td></tr></table></figure>

<p>这样我们就可以直接读取 TPCDS 数据集：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">use tpcds;</span><br><span class="line">show databases;</span><br><span class="line">use sf3000;</span><br><span class="line">show tables;</span><br><span class="line">select * from sf300.catalog_returns limit 10;</span><br></pre></td></tr></table></figure>

<h3 id="小文件产生"><a href="#小文件产生" class="headerlink" title="小文件产生"></a>小文件产生</h3><p>首先我们在 Hive 中创建一个 <code>sample.catalog_returns</code> 表，用于写入生成的 TPCDS <code>catalog_returns</code> 数据，并添加一个 <code>hash</code> 字段作为分区。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">create table sample.catalog_returns</span><br><span class="line">(</span><br><span class="line">    cr_returned_date_sk      bigint,</span><br><span class="line">    cr_returned_time_sk      bigint,</span><br><span class="line">    cr_item_sk               bigint,</span><br><span class="line">    cr_refunded_customer_sk  bigint,</span><br><span class="line">    cr_refunded_cdemo_sk     bigint,</span><br><span class="line">    cr_refunded_hdemo_sk     bigint,</span><br><span class="line">    cr_refunded_addr_sk      bigint,</span><br><span class="line">    cr_returning_customer_sk bigint,</span><br><span class="line">    cr_returning_cdemo_sk    bigint,</span><br><span class="line">    cr_returning_hdemo_sk    bigint,</span><br><span class="line">    cr_returning_addr_sk     bigint,</span><br><span class="line">    cr_call_center_sk        bigint,</span><br><span class="line">    cr_catalog_page_sk       bigint,</span><br><span class="line">    cr_ship_mode_sk          bigint,</span><br><span class="line">    cr_warehouse_sk          bigint,</span><br><span class="line">    cr_reason_sk             bigint,</span><br><span class="line">    cr_order_number          bigint,</span><br><span class="line">    cr_return_quantity       int,</span><br><span class="line">    cr_return_amount         decimal(7, 2),</span><br><span class="line">    cr_return_tax            decimal(7, 2),</span><br><span class="line">    cr_return_amt_inc_tax    decimal(7, 2),</span><br><span class="line">    cr_fee                   decimal(7, 2),</span><br><span class="line">    cr_return_ship_cost      decimal(7, 2),</span><br><span class="line">    cr_refunded_cash         decimal(7, 2),</span><br><span class="line">    cr_reversed_charge       decimal(7, 2),</span><br><span class="line">    cr_store_credit          decimal(7, 2),</span><br><span class="line">    cr_net_loss              decimal(7, 2)</span><br><span class="line">) PARTITIONED BY(hash int)</span><br><span class="line">stored as parquet;</span><br></pre></td></tr></table></figure>

<p>我们先关闭 Kyuubi 的优化，读取 catalog_returns 数据并写入 Hive：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=false;</span><br><span class="line"></span><br><span class="line">insert overwrite sample.catalog_returns partition (hash=0) select * from tpcds.sf300.catalog_returns;</span><br></pre></td></tr></table></figure>

<p><strong>Spark SQL 最终产生的文件数最多可能是最后一个写入的 Stage 的 Task 数乘以动态分区的数量。</strong>我们可以看到由于读取输入表的 Task 数是 44 个，所以最终产生了 44 个文件，每个文件大小约 69 M。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/1.png" class="">
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/2.png" class="">

<h3 id="改变分区数（Repartition）"><a href="#改变分区数（Repartition）" class="headerlink" title="改变分区数（Repartition）"></a>改变分区数（Repartition）</h3><p>由于写入的文件数跟最终写入 Stage 的 Task 数据有关，那么我们可以通过添加一个 Repartition 操作，来减少最终写入的 task 数，从而控制小文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite sample.catalog_returns partition (hash=0) select /*+ REPARTITION(10) */ * from tpcds.sf300.catalog_returns;</span><br></pre></td></tr></table></figure>

<p>添加 <code>REPARTITION(10)</code> 后，会在读取后做一个 Repartition 操作，将 partition 数变成 10，所以最终写入的文件数变成 10 个。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/3.png" class="">
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/4.png" class="">

<h3 id="Spark-AQE-自动合并小分区"><a href="#Spark-AQE-自动合并小分区" class="headerlink" title="Spark AQE 自动合并小分区"></a>Spark AQE 自动合并小分区</h3><p>Spark 3.0 以后引入了自适应查询优化（Adaptive Query Execution, AQE），可以自动合并较小的分区。</p>
<p>开启 AQE，并通过添加 <code>distribute by cast(rand() * 100 as int)</code> 触发 Shuffle 操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">set spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=false;</span><br><span class="line">set spark.sql.adaptive.enabled=true;</span><br><span class="line">set spark.sql.adaptive.advisoryPartitionSizeInBytes=512M;</span><br><span class="line">set spark.sql.adaptive.coalescePartitions.minPartitionNum=1;</span><br><span class="line"></span><br><span class="line">insert overwrite sample.catalog_returns partition (hash=0) select * from tpcds.sf300.catalog_returns distribute by cast(rand() * 100 as int);</span><br></pre></td></tr></table></figure>

<p>默认 Shuffle 分区数 <code>spark.sql.shuffle.partitions=200</code>，如果不开启 AQE 会产生 200 个小文件，开启 AQE 后，会自动合并小分区，根据 <code>spark.sql.adaptive.advisoryPartitionSizeInBytes=512M</code> 配置合并较小的分区，最终产生 <code>12</code> 个文件。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/5.png" class="">
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/6.png" class="">

<h2 id="Kyuubi-小文件优化分析"><a href="#Kyuubi-小文件优化分析" class="headerlink" title="Kyuubi 小文件优化分析"></a>Kyuubi 小文件优化分析</h2><p>Apache Kyuubi (Incubating) 作为增强版的 Spark Thrift Server 服务，可通过 Spark SQL 进行大规模的数据处理分析。Kyuubi 通过 Spark SQL Extensions 实现了很多的 Spark 优化，其中包括了 <code>RepartitionBeforeWrite</code> 的优化，再结合 Spark AQE 可以自动优化小文件问题，下面我们具体分析一下 Kyuubi 如何实现小文件优化。</p>
<h3 id="Kyuubi-如何优化小文件"><a href="#Kyuubi-如何优化小文件" class="headerlink" title="Kyuubi 如何优化小文件"></a>Kyuubi 如何优化小文件</h3><p>Kyuubi 提供了在写入前加上 <code>Repartition</code> 操作的优化，我们只需要将 <code>kyuubi-extension-spark-3-1_2.12-1.6.0-SNAPSHOT.jar</code> 放入 Spark jars 目录中，并配置 <code>spark.sql.extensions=org.apache.kyuubi.sql.KyuubiSparkSQLExtension</code>。相关配置：</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Default Value</th>
<th>Description</th>
<th>Since</th>
</tr>
</thead>
<tbody><tr>
<td>spark.sql.optimizer.insertRepartitionBeforeWrite.enabled</td>
<td>true</td>
<td>Add repartition node at the top of query plan. An approach of merging small files.</td>
<td>1.2.0</td>
</tr>
<tr>
<td>spark.sql.optimizer.insertRepartitionNum</td>
<td>none</td>
<td>The partition number if <code>spark.sql.optimizer.insertRepartitionBeforeWrite.enabled</code> is enabled. If AQE is disabled, the default value is <code>spark.sql.shuffle.partitions</code>. If AQE is enabled, the default value is none that means depend on AQE.</td>
<td>1.2.0</td>
</tr>
<tr>
<td>spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum</td>
<td>100</td>
<td>The partition number of each dynamic partition if <code>spark.sql.optimizer.insertRepartitionBeforeWrite.enabled</code> is enabled. We will repartition by dynamic partition columns to reduce the small file but that can cause data skew. This config is to extend the partition of dynamic partition column to avoid skew but may generate some small files.</td>
<td>1.2.0</td>
</tr>
</tbody></table>
<p>通过 <code>spark.sql.optimizer.insertRepartitionNum</code> 参数可以配置最终插入 Repartition 的分区数，当不开启 AQE，默认为 <code>spark.sql.shuffle.partitions</code> 的值。<strong>需要注意，当我们设置此配置会导致 AQE 失效，所以开启 AQE 不建议设置此值。</strong></p>
<p>对于动态分区写入，会根据动态分区字段进行 Repartition，并添加一个随机数来避免产生数据倾斜，<code>spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum</code> 用来配置随机数的范围，不过添加随机数后，由于加大了动态分区的基数，还是可能会导致小文件。这个操作类似在 SQL 中添加 <code>distribute by DYNAMIC_PARTITION_COLUMN, cast(rand() * 100 as int)</code>。</p>
<h3 id="静态分区写入"><a href="#静态分区写入" class="headerlink" title="静态分区写入"></a>静态分区写入</h3><p>开启 Kyuubi 优化和 AQE，测试静态分区写入：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">set spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=true;</span><br><span class="line"></span><br><span class="line">set spark.sql.adaptive.enabled=true;</span><br><span class="line">set spark.sql.adaptive.advisoryPartitionSizeInBytes=512M;</span><br><span class="line">set spark.sql.adaptive.coalescePartitions.minPartitionNum=1;</span><br><span class="line"></span><br><span class="line">insert overwrite sample.catalog_returns partition (hash=0) select * from tpcds.sf300.catalog_returns;</span><br></pre></td></tr></table></figure>

<p>可以看到 AQE 生效了，很好的控制了小文件，产生了 <code>11</code> 个文件，文件大小 <code>314.5 M</code> 左右。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/11.png" class="">
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/10.png" class="">

<h3 id="动态分区写入"><a href="#动态分区写入" class="headerlink" title="动态分区写入"></a>动态分区写入</h3><p>我们测试一下动态分区写入的情况，先关闭 Kyuubi 优化，并生成 10 个 hash 分区：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=false;</span><br><span class="line"></span><br><span class="line">insert overwrite sample.catalog_returns partition (hash) select *, cast(rand() * 10 as int) as hash from tpcds.sf300.catalog_returns;</span><br></pre></td></tr></table></figure>

<p>产生了 <code>44 × 10 = 440</code> 个文件，文件大小 <code>8 M</code> 左右。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/7.png" class="">

<p>开启 Kyuubi 优化和 AQE：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">set spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=true;</span><br><span class="line">set spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum=100;</span><br><span class="line"></span><br><span class="line">set spark.sql.adaptive.enabled=true;</span><br><span class="line">set spark.sql.adaptive.advisoryPartitionSizeInBytes=512M;</span><br><span class="line">set spark.sql.adaptive.coalescePartitions.minPartitionNum=1;</span><br><span class="line"></span><br><span class="line">insert overwrite sample.catalog_returns partition (hash) select *, cast(rand() * 10 as int) as hash from tpcds.sf300.catalog_returns;</span><br></pre></td></tr></table></figure>

<p>产生了 <code>12 × 10 = 120</code> 个文件，文件大小 <code>30 M</code> 左右，可以看到小文件有所改善，不过任然不够理想。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/8.png" class="">

<p>此案例中 hash 分区由 <code>rand</code> 函数产生，分布比较均匀，所以我们将 <code>spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum</code> 设置成 <code>0</code>，重新运行，同时将动态分区数设置为 <code>5</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">set spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=true;</span><br><span class="line">set spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum=0;</span><br><span class="line"></span><br><span class="line">set spark.sql.adaptive.enabled=true;</span><br><span class="line">set spark.sql.adaptive.advisoryPartitionSizeInBytes=512M;</span><br><span class="line">set spark.sql.adaptive.coalescePartitions.minPartitionNum=1;</span><br><span class="line"></span><br><span class="line">insert overwrite sample.catalog_returns partition (hash) select *, cast(rand() * 5 as int) as hash from tpcds.sf300.catalog_returns;</span><br></pre></td></tr></table></figure>

<p>由于动态分区数只有 <code>5</code> 个，所以实际上只有 <code>5</code> 个 Task 有数据写入，每个 Task 对应一个分区，导致最终每个分区只有一个较大的大文件。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/9.png" class="">

<p>通过上面的分析可以看到，对于动态分区写入，Repartition 的优化可以缓解小文件，配置 <code>spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum=100</code> 解决了数据倾斜问题，不过同时还是可能会有小文件。</p>
<h3 id="Rebalance-优化"><a href="#Rebalance-优化" class="headerlink" title="Rebalance 优化"></a>Rebalance 优化</h3><p>Spark 3.2+ 引入了 <code>Rebalance</code> 操作，借助于 Spark AQE 来平衡分区，进行小分区合并和倾斜分区拆分，避免分区数据过大或过小，能够很好的处理小文件问题。</p>
<p>Kyuubi 对于 Spark 3.2+ 的优化，是在写入前插入 Rebalance 操作，对于动态分区，则指定动态分区列进行 Rebalance 操作。不再需要 <code>spark.sql.optimizer.insertRepartitionNum</code> 和 <code>spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum</code> 配置。</p>
<p><strong>测试静态分区写入</strong>，使用 Spark 3.2.0 开启 Kyuubi 优化和 AQE：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">set spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=true;</span><br><span class="line"></span><br><span class="line">set spark.sql.adaptive.enabled=true;</span><br><span class="line">set spark.sql.adaptive.advisoryPartitionSizeInBytes=512M;</span><br><span class="line">set spark.sql.adaptive.coalescePartitions.minPartitionNum=1;</span><br><span class="line"></span><br><span class="line">insert overwrite sample.catalog_returns partition (hash=0) select * from tpcds.sf300.catalog_returns;</span><br></pre></td></tr></table></figure>

<p>Repartition 操作自动合并了小分区，产生了 <code>11</code> 个文件，文件大小 <code>334.6 M</code> 左右，解决了小文件的问题。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/15.png" class="">
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/14.png" class="">

<p><strong>测试动态分区写入</strong>，使用 Spark 3.2.0 开启 Kyuubi 优化和 AQE，生成 <code>5</code> 个动态分区：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">set spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=true;</span><br><span class="line"></span><br><span class="line">set spark.sql.adaptive.enabled=true;</span><br><span class="line">set spark.sql.adaptive.advisoryPartitionSizeInBytes=512M;</span><br><span class="line">set spark.sql.adaptive.coalescePartitions.minPartitionNum=1;</span><br><span class="line"></span><br><span class="line">insert overwrite sample.catalog_returns partition (hash) select *, cast(rand() * 5 as int) as hash from tpcds.sf300.catalog_returns;</span><br></pre></td></tr></table></figure>

<p>Repartition 操作自动拆分较大分区，产生了 <code>2 × 5 = 10</code> 个文件，文件大小 <code>311 M</code> 左右，很好的解决的倾斜问题。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/13.png" class="">
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/12.png" class="">

<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>从上面的分析可以看到，对于 Spark 3.2+，Kyuubi 结合 Rebalance 能够很好的解决小文件问题，对于 Spark 3.1，Kyuubi 也能自动优化小文件，不过动态分区写入的情况还是可能存在问题。</p>
<p>相关的配置总结：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 配置 Kyuubi Spark SQL Extension</span><br><span class="line">spark.sql.extensions=org.apache.kyuubi.sql.KyuubiSparkSQLExtension</span><br><span class="line"></span><br><span class="line"># 开启 RepartitionBeforeWrite 优化（默认开启）</span><br><span class="line">spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=true;</span><br><span class="line"></span><br><span class="line"># 配置 AQE</span><br><span class="line">spark.sql.adaptive.enabled=true;</span><br><span class="line">spark.sql.adaptive.advisoryPartitionSizeInBytes=512M;</span><br><span class="line">spark.sql.adaptive.coalescePartitions.minPartitionNum=1;</span><br></pre></td></tr></table></figure>

<p>更多 AQE 配置可以参考：<a target="_blank" rel="noopener" href="https://kyuubi.apache.org/docs/latest/deployment/spark/aqe.html">How To Use Spark Adaptive Query Execution (AQE) in Kyuubi</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Kyuubi/" rel="tag"># Kyuubi</a>
          
            <a href="/tags/Spark/" rel="tag"># Spark</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2021/11/23/Kyuubi-%E8%B0%83%E7%A0%94%E6%B5%8B%E8%AF%95/" rel="next" title="Kyuubi 调研测试">
                <i class="fa fa-chevron-left"></i> Kyuubi 调研测试
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2022/07/17/Kyuubi-%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E6%B7%B7%E5%90%88%E8%AE%A1%E7%AE%97/" rel="prev" title="Kyuubi 多数据源混合计算">
                Kyuubi 多数据源混合计算 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

   
    <div id="gitalk-container"></div>
	
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">44</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">27</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/wForget" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-%E5%B0%8F%E6%96%87%E4%BB%B6%E9%97%AE%E9%A2%98"><span class="nav-number">1.</span> <span class="nav-text">Spark 小文件问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83"><span class="nav-number">1.1.</span> <span class="nav-text">环境</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TPCDS-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.2.</span> <span class="nav-text">TPCDS 数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E6%96%87%E4%BB%B6%E4%BA%A7%E7%94%9F"><span class="nav-number">1.3.</span> <span class="nav-text">小文件产生</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%B9%E5%8F%98%E5%88%86%E5%8C%BA%E6%95%B0%EF%BC%88Repartition%EF%BC%89"><span class="nav-number">1.4.</span> <span class="nav-text">改变分区数（Repartition）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-AQE-%E8%87%AA%E5%8A%A8%E5%90%88%E5%B9%B6%E5%B0%8F%E5%88%86%E5%8C%BA"><span class="nav-number">1.5.</span> <span class="nav-text">Spark AQE 自动合并小分区</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kyuubi-%E5%B0%8F%E6%96%87%E4%BB%B6%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90"><span class="nav-number">2.</span> <span class="nav-text">Kyuubi 小文件优化分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Kyuubi-%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6"><span class="nav-number">2.1.</span> <span class="nav-text">Kyuubi 如何优化小文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9D%99%E6%80%81%E5%88%86%E5%8C%BA%E5%86%99%E5%85%A5"><span class="nav-number">2.2.</span> <span class="nav-text">静态分区写入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E5%86%99%E5%85%A5"><span class="nav-number">2.3.</span> <span class="nav-text">动态分区写入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Rebalance-%E4%BC%98%E5%8C%96"><span class="nav-number">2.4.</span> <span class="nav-text">Rebalance 优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">2.5.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wangz</span>

  
</div>









        







  <div style="display: none;">
    <script src="//s23.cnzz.com/z_stat.php?id=1276876819&web_id=1276876819" language="JavaScript"></script>
  </div>



        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
  <script src="/js/src/md5.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: 'b865ed71dd6039d51290',
          clientSecret: '40e9912d3048a9f67b6470357f92ecd46dc52567',
          repo: 'wForget.github.io',
          owner: 'wForget',
          admin: ['wForget'],
		  id: md5(location.pathname),
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')           
       </script>


  





  

  

  

  
  

  

  

  

</body>
</html>
