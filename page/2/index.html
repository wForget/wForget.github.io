<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">
<meta name="google-site-verification" content="nPT3ONUGntVHfQCZH2D5GcUMgg5DH4IReN4GIs0GrW8" />








<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="好记性不如烂笔头！">
<meta property="og:type" content="website">
<meta property="og:title" content="wForget&#39;s blog">
<meta property="og:url" content="https://wforget.github.io/page/2/index.html">
<meta property="og:site_name" content="wForget&#39;s blog">
<meta property="og:description" content="好记性不如烂笔头！">
<meta property="og:locale">
<meta property="article:author" content="wangz">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://wforget.github.io/page/2/"/>





  <title>wForget's blog</title>
  








<meta name="generator" content="Hexo 6.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">wForget's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2020/12/14/Gremlin-Server-Console-%E9%80%82%E9%85%8D-Atlas-JanusGraph/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/14/Gremlin-Server-Console-%E9%80%82%E9%85%8D-Atlas-JanusGraph/" itemprop="url">Gremlin Server/Console 适配 Atlas JanusGraph</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-12-14T19:00:20+08:00">
                2020-12-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Gremlin-Server-x2F-Console-适配-Atlas-JanusGraph"><a href="#Gremlin-Server-x2F-Console-适配-Atlas-JanusGraph" class="headerlink" title="Gremlin Server&#x2F;Console 适配 Atlas JanusGraph"></a>Gremlin Server&#x2F;Console 适配 Atlas JanusGraph</h3><p>Atlas 底层存储使用的 JanusGraph，由于对于 Atlas 底层数据结构并不太清楚，所以希望能够通过 Gremlin Console 来操作 Atlas 的 JanusGraph，使用 Gremlin Query Language 执行一些更加灵活的查询，并直观的查询数据结构。适配的思路参考了 <a target="_blank" rel="noopener" href="https://github.com/sburn/docker-apache-atlas">docker-apache-atlas</a> 项目。</p>
<h3 id="Gremlin-Server"><a href="#Gremlin-Server" class="headerlink" title="Gremlin Server"></a>Gremlin Server</h3><ol>
<li>下载相同版本的 gremlin server ，解压，并添加 Atlas 相关的依赖包</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ATLAS_HOME=/home/anchor/apache-atlas-2.1.0/</span><br><span class="line">GREMLIN_SERVER_HOME=/home/anchor/gremlin/apache-tinkerpop-gremlin-server-3.4.6</span><br><span class="line">ln -s $&#123;ATLAS_HOME&#125;/server/webapp/atlas/WEB-INF/lib/*.jar $&#123;GREMLIN_SERVER_HOME&#125;/lib 2&gt;/dev/null</span><br><span class="line">rm -f $&#123;GREMLIN_SERVER_HOME&#125;/lib/atlas-webapp-2.1.0.jar</span><br><span class="line">rm -f $&#123;GREMLIN_SERVER_HOME&#125;/lib/netty-3.10.5.Final.jar</span><br><span class="line">rm -f $&#123;GREMLIN_SERVER_HOME&#125;/lib/netty-all-4.0.52.Final.jar</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>gremlin server 配置</li>
</ol>
<p>gremlin-server-atlas-wshttp.yaml 配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">host: 0.0.0.0</span><br><span class="line">port: 8182</span><br><span class="line">scriptEvaluationTimeout: 30000</span><br><span class="line">#channelizer: org.apache.tinkerpop.gremlin.server.channel.WebSocketChannelizer</span><br><span class="line">channelizer: org.apache.tinkerpop.gremlin.server.channel.WsAndHttpChannelizer</span><br><span class="line">graphs: &#123;</span><br><span class="line">  graph: conf/janusgraph-hbase-es.properties</span><br><span class="line">&#125;</span><br><span class="line">scriptEngines: &#123;</span><br><span class="line">  gremlin-groovy: &#123;</span><br><span class="line">    plugins: &#123; org.apache.tinkerpop.gremlin.server.jsr223.GremlinServerGremlinPlugin: &#123;&#125;,</span><br><span class="line">               org.apache.tinkerpop.gremlin.tinkergraph.jsr223.TinkerGraphGremlinPlugin: &#123;&#125;,</span><br><span class="line">               org.apache.tinkerpop.gremlin.jsr223.ImportGremlinPlugin: &#123;classImports: [java.lang.Math], methodImports: [java.lang.Math#*]&#125;,</span><br><span class="line">               org.apache.tinkerpop.gremlin.jsr223.ScriptFileGremlinPlugin: &#123;files: [scripts/empty-sample.groovy]&#125;&#125;&#125;&#125;</span><br><span class="line"># JanusGraph sets default serializers. You need to uncomment the following lines, if you require any custom serializers.</span><br><span class="line">#</span><br><span class="line"># serializers:</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GraphBinaryMessageSerializerV1, config: &#123; ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry] &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GraphBinaryMessageSerializerV1, config: &#123; serializeResultToString: true &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV3d0, config: &#123; ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry] &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV3d0, config: &#123; serializeResultToString: true &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GraphSONMessageSerializerV3d0, config: &#123; ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry] &#125;&#125;</span><br><span class="line">#   # Older serialization versions for backwards compatibility:</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV1d0, config: &#123; ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry] &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GryoLiteMessageSerializerV1d0, config: &#123;ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry] &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV1d0, config: &#123; serializeResultToString: true &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GraphSONMessageSerializerV2d0, config: &#123; ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry] &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GraphSONMessageSerializerGremlinV1d0, config: &#123; ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistryV1d0] &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GraphSONMessageSerializerV1d0, config: &#123; ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistryV1d0] &#125;&#125;</span><br><span class="line">processors:</span><br><span class="line">  - &#123; className: org.apache.tinkerpop.gremlin.server.op.session.SessionOpProcessor, config: &#123; sessionTimeout: 28800000 &#125;&#125;</span><br><span class="line">  - &#123; className: org.apache.tinkerpop.gremlin.server.op.traversal.TraversalOpProcessor, config: &#123; cacheExpirationTime: 600000, cacheMaxSize: 1000 &#125;&#125;</span><br><span class="line">metrics: &#123;</span><br><span class="line">  consoleReporter: &#123;enabled: true, interval: 180000&#125;,</span><br><span class="line">  csvReporter: &#123;enabled: true, interval: 180000, fileName: /tmp/gremlin-server-metrics.csv&#125;,</span><br><span class="line">  jmxReporter: &#123;enabled: true&#125;,</span><br><span class="line">  slf4jReporter: &#123;enabled: true, interval: 180000&#125;,</span><br><span class="line">  graphiteReporter: &#123;enabled: false, interval: 180000&#125;&#125;</span><br><span class="line">maxInitialLineLength: 4096</span><br><span class="line">maxHeaderSize: 8192</span><br><span class="line">maxChunkSize: 8192</span><br><span class="line">maxContentLength: 65536</span><br><span class="line">maxAccumulationBufferComponents: 1024</span><br><span class="line">resultIterationBatchSize: 64</span><br><span class="line">writeBufferLowWaterMark: 32768</span><br><span class="line">writeBufferHighWaterMark: 65536</span><br></pre></td></tr></table></figure>

<p>janusgraph-hbase-es.properties 配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">gremlin.graph=org.janusgraph.core.JanusGraphFactory</span><br><span class="line">storage.backend=hbase</span><br><span class="line">storage.hostname=127.0.0.1:2181</span><br><span class="line">cache.db-cache = true</span><br><span class="line">cache.db-cache-clean-wait = 20</span><br><span class="line">cache.db-cache-time = 180000</span><br><span class="line">cache.db-cache-size = 0.5</span><br><span class="line">storage.hbase.table=apache_atlas_janus</span><br><span class="line">storage.hbase.ext.hbase.security.authentication=kerberos</span><br><span class="line">storage.hbase.ext.hbase.security.authorization=true</span><br><span class="line"></span><br><span class="line">index.search.backend=elasticsearch</span><br><span class="line">index.search.hostname=127.0.0.1:9200</span><br></pre></td></tr></table></figure>

<p>将 HBASE_CONF_DIR 加入 gramlin-server classpath 中（避免 kerberos 认证，连接失败等问题）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># vim gremlin-server.sh</span><br><span class="line">CP=&quot;$GREMLIN_HOME/conf/:$HBASE_CONF_DIR&quot;</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>启动 gramlin server</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup bin/gremlin-server.sh conf/gremlin-server-atlas-wshttp.yaml &gt; gramlin-server.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>


<h3 id="Gremlin-Console"><a href="#Gremlin-Console" class="headerlink" title="Gremlin Console"></a>Gremlin Console</h3><ol>
<li>下载并解压 gremlin console ，启动</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/gremlin.sh</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>连接 gremlin server</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">:remote connect tinkerpop.server conf/remote.yaml session</span><br><span class="line">:remote console</span><br></pre></td></tr></table></figure>

<h3 id="Gremlin-Console-操作"><a href="#Gremlin-Console-操作" class="headerlink" title="Gremlin Console 操作"></a>Gremlin Console 操作</h3><img src="/2020/12/14/Gremlin-Server-Console-%E9%80%82%E9%85%8D-Atlas-JanusGraph/console.png" class="" title="[Gremlin Console]">

<p>查询一个 hive_table 节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// hive_table where qualifiedName = &quot;aaa.test@test&quot;</span><br><span class="line">g = graph.traversal()</span><br><span class="line">g.V().has(&quot;__typeName&quot;, &quot;hive_table&quot;).has(&quot;Referenceable.qualifiedName&quot;, &quot;aaa.test@test&quot;).values()</span><br></pre></td></tr></table></figure>

<p>查看 Graph 的一些信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mgmt = graph.openManagement()</span><br><span class="line"></span><br><span class="line">mgmt.printVertexLabels()	// 打印 VertexLabels 信息</span><br><span class="line">mgmt.printEdgeLabels()		// 打印 EdgeLabels 信息</span><br><span class="line">mgmt.printPropertyKeys()	// 打印 PropertyKeys 信息</span><br><span class="line">mgmt.printIndexes()			// 打印所有索引信息</span><br><span class="line"></span><br><span class="line">mgmt.printSchema()    // 打印 Schema，包括上面的所有信息</span><br><span class="line"></span><br><span class="line">index = mgmt.getGraphIndex(&quot;vertex_index&quot;);  // 获取索引对象</span><br></pre></td></tr></table></figure>

<p>查询 Patch 节点信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">g.V().has(&quot;patch.type&quot;, &quot;TYPEDEF_PATCH&quot;)</span><br><span class="line"></span><br><span class="line">g.V().has(&quot;patch.type&quot;, &quot;JAVA_PATCH&quot;)</span><br></pre></td></tr></table></figure>

<h3 id="Graphexp-安装"><a href="#Graphexp-安装" class="headerlink" title="Graphexp 安装"></a>Graphexp 安装</h3><img src="/2020/12/14/Gremlin-Server-Console-%E9%80%82%E9%85%8D-Atlas-JanusGraph/graphexp.png" class="" title="[Graphexp]">

<p>Graphexp 是一个前端项目，结合 gremlin server 提供图数据的可视化。项目地址：<a target="_blank" rel="noopener" href="https://github.com/bricaud/graphexp">https://github.com/bricaud/graphexp</a></p>
<p>拉取 github 代码，安装 Nginx 并进行如下配置。完成后访问：<a target="_blank" rel="noopener" href="http://localhost:9990/graphexp.html">http://localhost:9990/graphexp.html</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># vim /etc/nginx/conf.d/graphexp-9990.conf</span><br><span class="line">server &#123;</span><br><span class="line">	keepalive_requests 120; #单连接请求上限次数。</span><br><span class="line">	listen       9990;   #监听端口</span><br><span class="line">	location  ~*^.+$ &#123;       #请求的url过滤，正则匹配，~为区分大小写，~*为不区分大小写。</span><br><span class="line">	   root /data/nginx/graphexp;  #插件目录</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h3><ol>
<li>Max frame length of 65536 has been exceeded.</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># io.netty.handler.codec.http.websocketx.CorruptedWebSocketFrameException: Max frame length of 65536 has been exceeded.</span><br><span class="line"># vim conf/remote.yaml</span><br><span class="line">connectionPool: &#123;maxContentLength: 655360&#125;</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2020/11/01/Linkis-%E6%96%B0%E5%BC%95%E6%93%8E%E5%AE%9E%E7%8E%B0%E5%88%86%E4%BA%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/01/Linkis-%E6%96%B0%E5%BC%95%E6%93%8E%E5%AE%9E%E7%8E%B0%E5%88%86%E4%BA%AB/" itemprop="url">Linkis 新引擎实现分享</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-11-01T20:34:12+08:00">
                2020-11-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Linkis-新引擎实现分享"><a href="#Linkis-新引擎实现分享" class="headerlink" title="Linkis 新引擎实现分享"></a>Linkis 新引擎实现分享</h3><p>在社区大佬的帮助下，我们完成了 0.11 版本的开发，实现了 ElasticSearch 和 Presto 引擎。具体的开发文档可以参考： <a target="_blank" rel="noopener" href="https://github.com/WeBankFinTech/Linkis/wiki/Linkis%E5%BC%95%E6%93%8E%E5%BC%80%E5%8F%91%E6%96%87%E6%A1%A3">Linkis引擎开发文档</a> </p>
<h3 id="执行引擎架构的选择"><a href="#执行引擎架构的选择" class="headerlink" title="执行引擎架构的选择"></a>执行引擎架构的选择</h3><p>目前 Linkis 的架构可以分为两种，一种是 Entrance-EngineManger-Engine 的模式，一种是 Entrance 模式。统一执行服务的架构可以参考官方文档： <a target="_blank" rel="noopener" href="https://github.com/WeBankFinTech/Linkis/wiki/Linkis-UJES%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3">Linkis-UJES设计文档</a> </p>
<p>Entrance 服务作为执行的入口，主要负责任务的持久化工作，日志的输出，进行脚本的校验和变量替换，并与 Engine、EngineManager 服务交互，向可用的 Engine 发送执行任务的请求，或者向 EngineManager 发送启动 Engine 的请求。</p>
<p>EngineManager 服务主要负责 Engine 的启动，进行 Engine 请求资源的请求与释放，并持续监控 Engine 的状态。</p>
<p>Engine 服务负责任务的具体执行，包括了任务执行的一些初始化操作、任务脚本的切分、任务的执行、任务的进度监控和结果集的保存等工作。</p>
<p>Spark、Hive 引擎是 Entrance-EngineManger-Engine 模式实现，在这个模式中 Engine 作为 Spark 、Hive 任务的 Driver 端，向外暴露接口可持续的接受 Entrance 发来的请求，完成任务的执行。这个模式中不仅实现了多租户的任务隔离，还提供了单用户的引擎复用，尽量减少 Engine 的启动，大大提高了执行的效率。</p>
<p>上面的各个服务可以看到每个服务的职责非常的明确，不过多个服务也让整个的架构变的比较重，有一些轻量的执行没有必要通过 Entrance-EngineManger-Engine 模式进行实现。例如 Linkis JDBC 引擎的实现就是通过 Entrance 的模式。JDBC 引擎的职责就是作为 JDBC 连接的客户端向服务端发送请求，并进行连接的维护。JDBC 连接的维护是比较轻量级的，而且 JDBC 连接的复用也不是根据平台用户进行区分的，所以单独为每个用户启动一个引擎是没有必要的。</p>
<p>ElasticSearch 和 Presto 的客户端实际上就是 Http Client，所以 ElasticSearch 和 Presto 引擎的实现也应该是比较轻量的，最终我们实现的 ElasticSearch 和 Presto 引擎也是通过 Entrance 的模式实现的。</p>
<h3 id="引擎资源控制"><a href="#引擎资源控制" class="headerlink" title="引擎资源控制"></a>引擎资源控制</h3><p>Linkis 的资源管理服务，用来管理用户、系统的资源和并发的控制，实现新的引擎需要考虑到引擎资源相关接口的实现。具体架构可参考：<a target="_blank" rel="noopener" href="https://github.com/WeBankFinTech/Linkis/wiki/Linkis-RM%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3">Linkis RM设计文档</a></p>
<h4 id="Entrance-EngineManger-Engine-模式资源控制"><a href="#Entrance-EngineManger-Engine-模式资源控制" class="headerlink" title="Entrance-EngineManger-Engine 模式资源控制"></a>Entrance-EngineManger-Engine 模式资源控制</h4><p>Entrance-EngineManger-Engine 的模式，资源相关主要需要下面两个实现：</p>
<ol>
<li>EngineManger 注册资源<br>Linkis RM设计文档中可以看到，EngineManger 作为 Engine 资源的管理者，需要先向 ResourceManger 进行管理资源的注册。<br>Linkis 已经将 EngineManger 注册资源的逻辑进行了抽象，实现的时候只需要在  SpringConfiguration 中进行配置创建 resources 的 spring bean 对象，可以参考 SparkEngineManagerSpringConfiguration 的实现。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// com.webank.wedatasphere.linkis.enginemanager.configuration.SparkEngineManagerSpringConfiguration</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkEngineManagerSpringConfiguration</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Bean</span>(<span class="type">Array</span>(<span class="string">&quot;resources&quot;</span>))</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createResource</span></span>(): <span class="type">ModuleInfo</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> totalResource = <span class="keyword">new</span> <span class="type">DriverAndYarnResource</span>(</span><br><span class="line">      <span class="keyword">new</span> <span class="type">LoadInstanceResource</span>(<span class="type">ENGINE_MANAGER_MAX_MEMORY_AVAILABLE</span>.getValue.toLong,</span><br><span class="line">        <span class="type">ENGINE_MANAGER_MAX_CORES_AVAILABLE</span>.getValue, <span class="type">ENGINE_MANAGER_MAX_CREATE_INSTANCES</span>.getValue),</span><br><span class="line">      <span class="literal">null</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> protectedResource = <span class="keyword">new</span> <span class="type">DriverAndYarnResource</span>(</span><br><span class="line">      <span class="keyword">new</span> <span class="type">LoadInstanceResource</span>(<span class="type">ENGINE_MANAGER_PROTECTED_MEMORY</span>.getValue.toLong, <span class="type">ENGINE_MANAGER_PROTECTED_CORES</span>.getValue,</span><br><span class="line">        <span class="type">ENGINE_MANAGER_PROTECTED_INSTANCES</span>.getValue),</span><br><span class="line">      <span class="literal">null</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="type">ModuleInfo</span>(<span class="type">Sender</span>.getThisServiceInstance, totalResource, protectedResource, <span class="type">ResourceRequestPolicy</span>.<span class="type">DriverAndYarn</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>EngineResourceFactory 实现<br>EngineManager 创建 Engine 的时候需要先向 ResourceManger 去请求资源，所以新引擎需要提供 EngineResourceFactory 的实现，用来初始化创新 Engine 所需要的资源，再向 ResourceManger 进行请求。<br>Linkis 中提供了 AbstractEngineResourceFactory 的抽象，实现的时候只需要从 AbstractEngineResourceFactory 继承。具体可参考 SparkEngineResourceFactory 的实现：</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// com.webank.wedatasphere.linkis.enginemanager.configuration.SparkEngineResourceFactory</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span>(<span class="string">&quot;engineResourceFactory&quot;</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkEngineResourceFactory</span> <span class="keyword">extends</span> <span class="title">AbstractEngineResourceFactory</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getRequestResource</span></span>(properties: java.util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">DriverAndYarnResource</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> executorNum = <span class="type">DWC_SPARK_EXECUTOR_INSTANCES</span>.getValue(properties)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">DriverAndYarnResource</span>(</span><br><span class="line">      <span class="keyword">new</span> <span class="type">LoadInstanceResource</span>(<span class="type">ByteTimeUtils</span>.byteStringAsBytes(<span class="type">DWC_SPARK_DRIVER_MEMORY</span>.getValue(properties) + <span class="string">&quot;G&quot;</span>),</span><br><span class="line">        <span class="type">DWC_SPARK_DRIVER_CORES</span>,</span><br><span class="line">        <span class="number">1</span>),</span><br><span class="line">      <span class="keyword">new</span> <span class="type">YarnResource</span>(<span class="type">ByteTimeUtils</span>.byteStringAsBytes(<span class="type">DWC_SPARK_EXECUTOR_MEMORY</span>.getValue(properties) * executorNum + <span class="string">&quot;G&quot;</span>),</span><br><span class="line">        <span class="type">DWC_SPARK_EXECUTOR_CORES</span>.getValue(properties) * executorNum,</span><br><span class="line">        <span class="number">0</span>,</span><br><span class="line">        <span class="type">DWC_QUEUE_NAME</span>.getValue(properties))</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Entrance-模式并发控制"><a href="#Entrance-模式并发控制" class="headerlink" title="Entrance 模式并发控制"></a>Entrance 模式并发控制</h4><p>Lnkis 中将 Engine 的实例数作为资源的一种，目前用户请求的并发是通过 Engine 的实例数进行控制的，那么在 Entrance 的模式下，就没有很好的对用户的并发进行控制。</p>
<p>在 ElasticSearch 和 Presto 的实现中，我们参考了 EngineManager 的资源控制，将并发数作为资源的一种，在 Entrance 启动时进行模块资源注册。将每个执行作为一个实例，执行发生时先进行资源的请求和锁定，执行完成后进行资源的释放，从而达到用户并发的控制。</p>
<p>主要包括了以下步骤：</p>
<ol>
<li>Entrance 注册并发资源<br>Entrance 注册并发资源，需要创建资源实例，将并发作为资源的一部分，然后配合 @EnableResourceManager 和 @RegisterResource 注解进行资源注册。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义资源</span></span><br><span class="line"><span class="meta">@Bean</span>(<span class="type">Array</span>(<span class="string">&quot;resources&quot;</span>))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createResource</span></span>(): <span class="type">ModuleInfo</span> = &#123;</span><br><span class="line">  <span class="comment">// 创建并发资源实例，分为总资源和受保护的资源</span></span><br><span class="line">  <span class="keyword">val</span> totalResource = <span class="keyword">new</span> <span class="type">InstanceResource</span>(<span class="type">EsEntranceConfiguration</span>.<span class="type">ENTRANCE_MAX_JOB_INSTANCE</span>.getValue)</span><br><span class="line">  <span class="keyword">val</span> protectResource = <span class="keyword">new</span> <span class="type">InstanceResource</span>(<span class="type">EsEntranceConfiguration</span>.<span class="type">ENTRANCE_PROTECTED_JOB_INSTANCE</span>.getValue)</span><br><span class="line">  info(<span class="string">s&quot;create resource for es engine totalResource is <span class="subst">$totalResource</span>, protectResource is <span class="subst">$protectResource</span>&quot;</span>)</span><br><span class="line">  <span class="type">ModuleInfo</span>(<span class="type">Sender</span>.getThisServiceInstance, totalResource, protectResource, <span class="type">ResourceRequestPolicy</span>.<span class="type">Instance</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册资源</span></span><br><span class="line"><span class="meta">@RegisterResource</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">registerResources</span></span>(): <span class="type">ModuleInfo</span> = resources</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>执行前请求锁定资源<br>执行实例初始化前，先通过 ResourceManagerClient#requestResource 方法请求锁定并发实例资源。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">rmClient.requestResource(requestEngine.user, requestEngine.creator, <span class="keyword">new</span> <span class="type">InstanceResource</span>(<span class="number">1</span>)) <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">NotEnoughResource</span>(reason) =&gt;</span><br><span class="line">    <span class="comment">// 没有请求到资源，抛出异常</span></span><br><span class="line">    <span class="keyword">throw</span> <span class="type">EsEngineException</span>(<span class="type">LogUtils</span>.generateWarn(reason))</span><br><span class="line">  <span class="keyword">case</span> <span class="type">AvailableResource</span>(ticketId) =&gt; &#123;</span><br><span class="line">    <span class="comment">// 请求到资源，创建执行实例，并保存 ticketId 用于释放资源</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// 当资源被实例化后，返回实际占用的资源总量</span></span><br><span class="line">    rmClient.resourceInited(<span class="type">UserResultResource</span>(ticketId, requestEngine.user), <span class="keyword">new</span> <span class="type">InstanceResource</span>(<span class="number">1</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>执行完成释放资源<br>执行完成后销毁执行实例，并通过 ResourceManagerClient#resourceReleased 方法释放锁定的资源。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用 ticketId 释放对应的资源</span></span><br><span class="line">rmClient.resourceReleased(<span class="type">UserResultResource</span>(ticketId, requestEngine.user))</span><br></pre></td></tr></table></figure>


<h3 id="ElasticSearch-引擎的实现"><a href="#ElasticSearch-引擎的实现" class="headerlink" title="ElasticSearch 引擎的实现"></a>ElasticSearch 引擎的实现</h3><p>下面是微众王和平大佬帮忙画的 ElasticSearch 引擎整体的架构图：</p>
<img src="/2020/11/01/Linkis-%E6%96%B0%E5%BC%95%E6%93%8E%E5%AE%9E%E7%8E%B0%E5%88%86%E4%BA%AB/ElasticSearch%E5%BC%95%E6%93%8E%E6%9E%B6%E6%9E%84%E5%9B%BE.png" class="" title="ElasticSearch引擎架构图">

<p>Linkis 新引擎的实现还是比较容易的，ElasticSearch 引擎的代码结构如下，整体的代码量也是比较少。主要包括了资源的配置、执行器的实例化和ElasticSearch请求与结果解析的相关代码。</p>
<img src="/2020/11/01/Linkis-%E6%96%B0%E5%BC%95%E6%93%8E%E5%AE%9E%E7%8E%B0%E5%88%86%E4%BA%AB/Es%E5%BC%95%E6%93%8E%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84.png" class="" title="Es引擎代码结构">

<ol>
<li>资源注册<br>ElasticSearch 引擎需要考虑到用户请求的并发和 Entrance 整体并发的控制。<br>Entrance 启动时，需要对 Entrance 可用资源进行注册，主要包括了最大实例数和保护的阈值。在 EsSpringConfiguration 中生成资源的 bean 对象，并传入 EsEngineManager 进行注册，配置 @EnableResourceManager 和 @RegisterResource 就会自动进行注册。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// com.webank.wedatasphere.linkis.entrance.conf.EsSpringConfiguration</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EsSpringConfiguration</span> <span class="keyword">extends</span> <span class="title">Logging</span></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Bean</span>(<span class="type">Array</span>(<span class="string">&quot;resources&quot;</span>))</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createResource</span></span>(<span class="meta">@Autowired</span> rmClient: <span class="type">ResourceManagerClient</span>): <span class="type">ModuleInfo</span> = &#123;</span><br><span class="line">    <span class="comment">// Clean up resources before creating resources to prevent dirty data when exiting abnormally (创造资源之前进行资源清理，防止异常退出时产生了脏数据)</span></span><br><span class="line">    <span class="type">Utils</span>.tryQuietly(rmClient.unregister())</span><br><span class="line">    <span class="type">Utils</span>.addShutdownHook(&#123;</span><br><span class="line">      info(<span class="string">&quot;rmClient shutdown, unregister resource...&quot;</span>)</span><br><span class="line">      rmClient.unregister</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="keyword">val</span> totalResource = <span class="keyword">new</span> <span class="type">InstanceResource</span>(<span class="type">EsEntranceConfiguration</span>.<span class="type">ENTRANCE_MAX_JOB_INSTANCE</span>.getValue)</span><br><span class="line">    <span class="keyword">val</span> protectResource = <span class="keyword">new</span> <span class="type">InstanceResource</span>(<span class="type">EsEntranceConfiguration</span>.<span class="type">ENTRANCE_PROTECTED_JOB_INSTANCE</span>.getValue)</span><br><span class="line">    info(<span class="string">s&quot;create resource for es engine totalResource is <span class="subst">$totalResource</span>, protectResource is <span class="subst">$protectResource</span>&quot;</span>)</span><br><span class="line">    <span class="type">ModuleInfo</span>(<span class="type">Sender</span>.getThisServiceInstance, totalResource, protectResource, <span class="type">ResourceRequestPolicy</span>.<span class="type">Instance</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// com.webank.wedatasphere.linkis.entrance.execute.EsEngineManager</span></span><br><span class="line"><span class="meta">@EnableResourceManager</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EsEngineManager</span>(<span class="params">resources: <span class="type">ModuleInfo</span></span>) <span class="keyword">extends</span> <span class="title">EngineManager</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@RegisterResource</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">registerResources</span></span>(): <span class="type">ModuleInfo</span> = resources</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>请求执行器<br>EsEngineRequester 启动一个执行器，用于任务的执行，通过 request 方法对传入的 job 生成一个执行的 EsEntranceEngine，请求时先向 ResourceManager 请求并锁定一个实例的资源，在 EsEntranceEngine 执行结束后会进行释放。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// com.webank.wedatasphere.linkis.entrance.execute.EsEngineRequester</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EsEngineRequester</span>(<span class="params">groupFactory: <span class="type">GroupFactory</span>, rmClient: <span class="type">ResourceManagerClient</span></span>) <span class="keyword">extends</span> <span class="title">EngineRequester</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">request</span></span>(job: <span class="type">Job</span>): <span class="type">Option</span>[<span class="type">EntranceEngine</span>] = job <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> entranceJob: <span class="type">EntranceJob</span> =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> requestEngine = createRequestEngine(job);</span><br><span class="line">      <span class="comment">// request resource manager</span></span><br><span class="line">      rmClient.requestResource(requestEngine.user, requestEngine.creator, <span class="keyword">new</span> <span class="type">InstanceResource</span>(<span class="number">1</span>)) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">NotEnoughResource</span>(reason) =&gt;</span><br><span class="line">          <span class="keyword">throw</span> <span class="type">EsEngineException</span>(<span class="type">LogUtils</span>.generateWarn(reason))</span><br><span class="line">        <span class="keyword">case</span> <span class="type">AvailableResource</span>(ticketId) =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> engine = <span class="keyword">new</span> <span class="type">EsEntranceEngine</span>(idGenerator.incrementAndGet(), <span class="keyword">new</span> util.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>](requestEngine.properties)</span><br><span class="line">            , () =&gt; &#123;rmClient.resourceReleased(<span class="type">UserResultResource</span>(ticketId, requestEngine.user))&#125;)</span><br><span class="line">          engine.setGroup(groupFactory.getOrCreateGroup(getGroupName(requestEngine.creator, requestEngine.user)))</span><br><span class="line">          engine.setUser(requestEngine.user)</span><br><span class="line">          engine.setCreator(requestEngine.creator)</span><br><span class="line"><span class="comment">//          engine.updateState(ExecutorState.Starting, ExecutorState.Idle, null, null)</span></span><br><span class="line">          engine.setJob(entranceJob)</span><br><span class="line">          engine.init()</span><br><span class="line">          executorListener.foreach(_.onExecutorCreated(engine))</span><br><span class="line">          rmClient.resourceInited(<span class="type">UserResultResource</span>(ticketId, requestEngine.user), <span class="keyword">new</span> <span class="type">InstanceResource</span>(<span class="number">1</span>))</span><br><span class="line">          <span class="type">Option</span>(engine)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="type">None</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// com.webank.wedatasphere.linkis.entrance.execute.EsEntranceEngine</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EsEntranceEngine</span>(<span class="params">id: <span class="type">Long</span>, properties: <span class="type">JMap</span>[<span class="type">String</span>, <span class="type">String</span>], resourceRelease: (</span>) <span class="title">=&gt;</span> <span class="title">Unit</span>) <span class="keyword">extends</span> <span class="title">EntranceEngine</span>(<span class="params">id</span>) <span class="keyword">with</span> <span class="title">SingleTaskOperateSupport</span> <span class="keyword">with</span> <span class="title">SingleTaskInfoSupport</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">this</span>.job.setResultSize(<span class="number">0</span>)</span><br><span class="line">      <span class="keyword">this</span>.engineExecutor.close</span><br><span class="line">      <span class="comment">// 释放资源</span></span><br><span class="line">      resourceRelease()</span><br><span class="line">      <span class="comment">// ......</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>任务执行<br>EsEntranceEngine 是 com.webank.wedatasphere.linkis.entrance.execute.EntranceEngine 的实现，进行脚本的执行。在这里抽出一层 EsEngineExecutor 作为 Es 任务的具体执行。EsEntranceEngine 则负责 EsEngineExecutor 的初始化、脚本解析切分等实现。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EsEntranceEngine</span>(<span class="params">id: <span class="type">Long</span>, properties: <span class="type">JMap</span>[<span class="type">String</span>, <span class="type">String</span>], resourceRelease: (</span>) <span class="title">=&gt;</span> <span class="title">Unit</span>) <span class="keyword">extends</span> <span class="title">EntranceEngine</span>(<span class="params">id</span>) <span class="keyword">with</span> <span class="title">SingleTaskOperateSupport</span> <span class="keyword">with</span> <span class="title">SingleTaskInfoSupport</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> engineExecutor: <span class="type">EsEngineExecutor</span> = _</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">execute</span></span>(executeRequest: <span class="type">ExecuteRequest</span>): <span class="type">ExecuteResponse</span>   <span class="comment">// ...</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">executeLine</span></span>(code: <span class="type">String</span>): <span class="type">ExecuteResponse</span> = <span class="keyword">this</span>.engineExecutor.executeLine(code, storePath, <span class="string">s&quot;_<span class="subst">$codeLine</span>&quot;</span>)</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>ElasticSearch 脚本执行<br>entrance.executor 包中就是 ElasticSearch 客户端的封装、请求的封装和结果的解析等相关代码。<br>ElasticSearch 客户端封装在 EsClient 中，通过 EsClientFactory 进行实例化，并将 datasourceName 作为唯一 Key 进行缓存。<br>EsEngineExecutorImpl 是 EsEngineExecutor 的实现，用于任务的执行。<br>ResponseHandlerImpl 用于结果的处理，会根据 ElasticSearch 的返回类型进行反序列化，并保存为 Linkis 的 ResultSet。</li>
</ol>
<h3 id="DataSource-路由"><a href="#DataSource-路由" class="headerlink" title="DataSource 路由"></a>DataSource 路由</h3><p>在与微众大佬的讨论交流中得知后面 Linkis 的架构将会引入 DataSource 的概念，DataSource 模块维护引擎的连接信息和集群等信息，可以减少一些数据源运行配置，方便数据源配置和权限管理，为数据平台提供元数据信息，并可根据 DataSource 进行路由实现多集群的路由。</p>
<p>在 Linkis-0.11.0版本中添加了 linkis-gateway-ujes-datasource-ruler 模块，作为一个 Gateway 插件的形式简单实现了，请求和 Entrance 的路由。</p>
<h4 id="linkis-gateway-ujes-datasource-ruler-模块的实现"><a href="#linkis-gateway-ujes-datasource-ruler-模块的实现" class="headerlink" title="linkis-gateway-ujes-datasource-ruler 模块的实现"></a>linkis-gateway-ujes-datasource-ruler 模块的实现</h4><p>抽象出 EntranceGatewayRouterRuler 接口用于执行路由规则，在 Gateway 模块的 EntranceGatewayRouter 中注入 EntranceGatewayRouterRuler 实例。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EntranceGatewayRouter</span> <span class="keyword">extends</span> <span class="title">AbstractGatewayRouter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Autowired</span>(required = <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> rules: <span class="type">Array</span>[<span class="type">EntranceGatewayRouterRuler</span>] = _</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">route</span></span>(gatewayContext: <span class="type">GatewayContext</span>): <span class="type">ServiceInstance</span> = &#123;</span><br><span class="line">    gatewayContext.getGatewayRoute.getRequestURI <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">EntranceGatewayRouter</span>.<span class="type">ENTRANCE_REGEX</span>(_) =&gt;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">        serviceId.map(applicationName =&gt; &#123;</span><br><span class="line">          rules <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> array: <span class="type">Array</span>[<span class="type">EntranceGatewayRouterRuler</span>] =&gt; array.foreach(_.rule(applicationName, gatewayContext))</span><br><span class="line">            <span class="keyword">case</span> _ =&gt;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="type">ServiceInstance</span>(applicationName, gatewayContext.getGatewayRoute.getServiceInstance.getInstance)</span><br><span class="line">        &#125;).orNull</span><br><span class="line">      <span class="keyword">case</span> _ =&gt; <span class="literal">null</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>linkis-gateway-ujes-datasource-ruler 模块，主要是做了一个 DataSource 和 Entrance Instance 的映射，并保存在 Mysql 中。DatasourceGatewayRouterRuler 实现了具体的路由策略，DatasourceMapService 接口维护 DataSource 映射。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 维护 DataSource 映射的接口</span></span><br><span class="line">public interface <span class="type">DatasourceMapService</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="type">String</span> getInstanceByDatasource(<span class="type">String</span> datasourceName);</span><br><span class="line"></span><br><span class="line">    long countByInstance(<span class="type">String</span> instance);</span><br><span class="line"></span><br><span class="line">    <span class="type">String</span> insertDatasourceMap(<span class="type">String</span> datasourceName, <span class="type">String</span> instance, <span class="type">String</span> serviceId);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// EntranceGatewayRouterRuler 的实现类，执行具体的路由逻辑</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DatasourceGatewayRouterRuler</span> <span class="keyword">extends</span> <span class="title">EntranceGatewayRouterRuler</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 路由的方法</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">rule</span></span>(serviceId: <span class="type">String</span>, gatewayContext: <span class="type">GatewayContext</span>): <span class="type">Unit</span> = <span class="keyword">if</span>(<span class="type">StringUtils</span>.isNotBlank(gatewayContext.getRequest.getRequestBody)) &#123;</span><br><span class="line">    <span class="comment">// 从请求中获取 datasourceName</span></span><br><span class="line">    <span class="keyword">val</span> datasourceName = getDatasourceName(gatewayContext.getRequest.getRequestBody)</span><br><span class="line">    <span class="keyword">if</span> (<span class="type">StringUtils</span>.isBlank(datasourceName)) <span class="keyword">return</span></span><br><span class="line">    debug(<span class="string">s&quot;datasourceName: <span class="subst">$datasourceName</span>&quot;</span>)</span><br><span class="line">    <span class="comment">// 通过 datasourceName 获取映射</span></span><br><span class="line">    datasourceMapService.getInstanceByDatasource(datasourceName) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> i: <span class="type">String</span> <span class="keyword">if</span> <span class="type">StringUtils</span>.isNotBlank(i) =&gt; </span><br><span class="line">        <span class="comment">// 存在映射直接返回 Instance</span></span><br><span class="line">        gatewayContext.getGatewayRoute.getServiceInstance.setInstance(i)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt; &#123;</span><br><span class="line">        <span class="comment">// 不存在映射时，先获取 Instance 列表，并根据已经存在映射的数据按照从小到大排序，获取最少映射的 Instance，插入 DataSource 映射并返回</span></span><br><span class="line">        <span class="keyword">val</span> newInstance = <span class="type">ServiceInstanceUtils</span>.getRPCServerLoader.getServiceInstances(serviceId)</span><br><span class="line">          .map(item =&gt; (item, datasourceMapService.countByInstance(item.getInstance)))</span><br><span class="line">          .sortBy(_._2).map(_._1.getInstance).headOption <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="type">Some</span>(item) =&gt; datasourceMapService.insertDatasourceMap(datasourceName, item, serviceId)</span><br><span class="line">            <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="literal">null</span></span><br><span class="line">          &#125;</span><br><span class="line">        debug(<span class="string">s&quot;newInstance: <span class="subst">$newInstance</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> (<span class="type">StringUtils</span>.isNotBlank(newInstance)) &#123;</span><br><span class="line">          gatewayContext.getGatewayRoute.getServiceInstance.setInstance(newInstance)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>




          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2020/10/30/Linkis-%E6%9D%83%E9%99%90%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%8E%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/10/30/Linkis-%E6%9D%83%E9%99%90%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%8E%A5/" itemprop="url">Linkis 权限系统对接</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-10-30T13:39:30+08:00">
                2020-10-30
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Linkis-权限控制"><a href="#Linkis-权限控制" class="headerlink" title="Linkis 权限控制"></a>Linkis 权限控制</h2><p>目前 Linkis 的库表权限控制主要是依赖于 Hive 的权限控制。</p>
<p>下面分享一下 Linkis 接入第三方权限系统，进行库表权限的控制。主要做了三个模块的接入：Metadata 模块、Hive 引擎、Spark 引擎。</p>
<p>以下代码大多为伪代码，需要根据实际情况进行完善，AuthClient 为权限系统的客户端，主要与权限系统交互，获取和校验用户的库表权限。</p>
<h3 id="MetaData-模块对接"><a href="#MetaData-模块对接" class="headerlink" title="MetaData 模块对接"></a>MetaData 模块对接</h3><p>前端页面上数据开发面板中，展示的数据库和表是通过调用 linkis-metadata 模块进行获取的，当前的实现是直接查询 Hive 权限相关的表进行获取。</p>
<p>这里需要修改 com.webank.wedatasphere.linkis.metadata.service.impl.DataSourceServiceImpl 类，接入权限系统，根据用户获取有权限的库表。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">AuthClient</span> <span class="variable">authClient</span> <span class="operator">=</span> AuthClient.getInstance();</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> JsonNode <span class="title function_">getDbs</span><span class="params">(String userName)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    List&lt;String&gt; dbs = authClient.getDbsByUser(userName);</span><br><span class="line">    <span class="type">ArrayNode</span> <span class="variable">dbsNode</span> <span class="operator">=</span> jsonMapper.createArrayNode();</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">return</span> dbsNode;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> JsonNode <span class="title function_">getDbsWithTables</span><span class="params">(String userName)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">ArrayNode</span> <span class="variable">dbNodes</span> <span class="operator">=</span> jsonMapper.createArrayNode();</span><br><span class="line">    List&lt;String&gt; dbs = authClient.getDbsByUser(userName);</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">return</span> dbNodes;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> JsonNode <span class="title function_">queryTables</span><span class="params">(String database, String userName)</span> &#123;</span><br><span class="line">    List&lt;Map&lt;String, Object&gt;&gt; listTables = Lists.newArrayList();</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    List&lt;String&gt; authTables = authClient.getTablesByUser(database, userName);</span><br><span class="line"></span><br><span class="line">    <span class="type">ArrayNode</span> <span class="variable">tables</span> <span class="operator">=</span> jsonMapper.createArrayNode();</span><br><span class="line">    <span class="keyword">for</span> (Map&lt;String, Object&gt; table : listTables) &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">talbeName</span> <span class="operator">=</span> (String) table.get(<span class="string">&quot;NAME&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span> (!authTables.contains(talbeName)) &#123;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> tables;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Hive-引擎对接"><a href="#Hive-引擎对接" class="headerlink" title="Hive 引擎对接"></a>Hive 引擎对接</h3><p>Hive 引擎是通过 Hive Driver 进行任务的执行，那么可以通过实现 PreExecute 接口，配置到 hive.exec.pre.hooks 中，完成权限的校验，校验不通过时抛出异常进行拦截。</p>
<p>AuthHiveHook 为 PreExecute 的实现，具体实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AuthHiveHook</span> <span class="keyword">extends</span> <span class="title class_">PreExecute</span> &#123;</span><br><span class="line"></span><br><span class="line">  def user: String = DWCArgumentsParser.getDWCOptionMap(<span class="string">&quot;user&quot;</span>)</span><br><span class="line">  val authClient: AuthClient =  AuthClient.getInstance()</span><br><span class="line"></span><br><span class="line">  override def <span class="title function_">run</span><span class="params">(sess: SessionState, inputs: util.Set[ReadEntity], outputs: util.Set[WriteEntity], ugi: UserGroupInformation)</span>: Unit = &#123;</span><br><span class="line">    inputs.foreach(input =&gt; &#123;</span><br><span class="line">      <span class="type">val</span> <span class="variable">database</span> <span class="operator">=</span> input.getTable.getDbName</span><br><span class="line">      <span class="type">val</span> <span class="variable">table</span> <span class="operator">=</span> input.getTable.getTableName</span><br><span class="line">      <span class="title function_">if</span> <span class="params">(!authClient.checkReadTable(user, database, table)</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">AuthorizationException</span>(<span class="string">&quot;Authorization failed, user:&quot;</span> + user + <span class="string">&quot; database:&quot;</span> + database + <span class="string">&quot; table:&quot;</span> + table)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    outputs.foreach(output =&gt; &#123;</span><br><span class="line">      <span class="type">val</span> <span class="variable">database</span> <span class="operator">=</span> output.getTable.getDbName</span><br><span class="line">      <span class="type">val</span> <span class="variable">table</span> <span class="operator">=</span> output.getTable.getTableName</span><br><span class="line">      <span class="title function_">if</span> <span class="params">(!authClient.checkWriteTable(user, database, table)</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">AuthorizationException</span>(<span class="string">&quot;Authorization failed, user:&quot;</span> + user + <span class="string">&quot; database:&quot;</span> + database + <span class="string">&quot; table:&quot;</span> + table)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Spark-引擎对接"><a href="#Spark-引擎对接" class="headerlink" title="Spark 引擎对接"></a>Spark 引擎对接</h3><p>Spark 引擎的库表权限控制参考了 <a target="_blank" rel="noopener" href="https://github.com/yaooqinn/spark-authorizer.git">spark-authorizer</a> 的实现，通过实现 OptimizerRule，对 LogicalPlan 解析输入输出表，再进行权限校验和控制。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2020/10/28/Spark-%E5%86%99-Parquet-%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/10/28/Spark-%E5%86%99-Parquet-%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%E9%97%AE%E9%A2%98/" itemprop="url">Spark 写 Parquet 数据丢失问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-10-28T11:10:41+08:00">
                2020-10-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>数据同步任务是为了做跨集群的 Hive 数据同步，通过  Spark 读取源集群 Hive 数据源，再写入目标集群 Table Location 的 HDFS 路径。</p>
<p>用户批量执行 Spark 同步任务时（写同一个表的不同分区，大多是为了补历史数据的任务），部分分区数据丢失。</p>
<h2 id="问题定位"><a href="#问题定位" class="headerlink" title="问题定位"></a>问题定位</h2><p>通过 Spark sql 对比，源集群和目标集群的数据，发现部分分区确实存在数据丢失。<br>查询目标集群丢失数据分区的 HDFS 目录，发现确实缺少部分 parquet 文件（parquet 文件缺少部分序列）。<br>查看  NameNode 日志，发现 Parquet 文件是先写到  ${TableLocation}&#x2F;_temporary 目录中，在 rename 到目标目录。<br>继续在 NameNode 日志中查找丢失目录的 temp 文件，发现只有 Create 操作，没有 Delete 操作，不过发现了多个客户端在写同一个  _temporary 目录，并有删除操作。</p>
<h2 id="临时路径如何确定"><a href="#临时路径如何确定" class="headerlink" title="临时路径如何确定"></a>临时路径如何确定</h2><p>Spark 写文件的入口类为：org.apache.spark.sql.execution.datasources.FileFormatWriter</p>
<p>org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter 类是用来定义 MapReduce 任务 Job 的输出，包括 Job Task 输出路径的初始化、清理等工作。其中定义了temp 目录为 ${RealOutput}&#x2F;_temporary 。</p>
<p>Spark 任务为 FileOutputCommitter 做了一层代理 org.apache.spark.internal.io.FileCommitProtocol，部分场景使用到了 stagingDir 作为 output 路径。</p>
<p>org.apache.spark.internal.io.HadoopMapReduceCommitProtocol#newTaskTempFile，当 dynamicPartitionOverwrite 为 true 时，临时路径在 spark stagingDir 中则每个任务不会重复。</p>
<img src="/2020/10/28/Spark-%E5%86%99-Parquet-%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%E9%97%AE%E9%A2%98/1.png" class="">

<h2 id="Spark-通过-Hive-写入是如何保证的"><a href="#Spark-通过-Hive-写入是如何保证的" class="headerlink" title="Spark 通过 Hive 写入是如何保证的"></a>Spark 通过 Hive 写入是如何保证的</h2><p>Spark 中保存 Hive 表数据，实际上写入的路径改成了 hive staging 的路径，具体代码：org.apache.spark.sql.hive.execution.SaveAsHiveFile#getExternalTmpPath 中，然后进行 load partition。这个相当于直接将写入的 basePath 路径改变了，所以不会存在冲突。</p>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>方法一（没有采用）： 将 saveMode 设置为 Overwrite，partitionOverwriteMode 设置为 dynamic，这样写入的temp 目录就在 stagingDir 中，不过 overwrite 模式写入会先清空目标分区，所以没有采用。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">resultDF.write.partitionBy(partitionColumn: _*)</span><br><span class="line">  .mode(<span class="string">&quot;overwrite&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;partitionOverwriteMode&quot;</span>, <span class="string">&quot;dynamic&quot;</span>)</span><br><span class="line">  .parquet(hdfsTablePath)</span><br></pre></td></tr></table></figure>

<p>方法二：定义 Temp 目录为 ${TableLocation}&#x2F;${JobId}，确保写入路径不重复，在将 Temp 目录 merge 到 target 路径中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">resultDF.write.partitionBy(partitionColumn: _*).mode(writeMode)</span><br><span class="line">   .parquet(tempDir)</span><br><span class="line"><span class="comment">// merge temp dir to table path</span></span><br><span class="line"><span class="comment">// merge 方法参考 org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter#mergePaths</span></span><br><span class="line">mergePaths(tempDir, hdfsTablePath)</span><br></pre></td></tr></table></figure>

<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>写入对象存储的性能调优，也与 FileOutputCommitter 这块有关。参考： <a target="_blank" rel="noopener" href="https://www.infoq.cn/article/pGEeR83ptNQDxhGEIjr5">存算分离下写性能提升10倍以上，EMR Spark引擎是如何做到的？
</a> </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2020/09/30/%E3%80%8ASQL%E8%A7%A3%E6%9E%90%E3%80%8B-%E4%B8%80%E3%80%81%E8%A7%A3%E6%9E%90%E5%99%A8%E7%AE%80%E4%BB%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/30/%E3%80%8ASQL%E8%A7%A3%E6%9E%90%E3%80%8B-%E4%B8%80%E3%80%81%E8%A7%A3%E6%9E%90%E5%99%A8%E7%AE%80%E4%BB%8B/" itemprop="url">《SQL解析》 一、解析器简介</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-09-30T09:54:55+08:00">
                2020-09-30
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2020/09/18/SQL%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/18/SQL%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97/" itemprop="url">SQL解析系列</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-09-18T19:02:49+08:00">
                2020-09-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="SQL解析"><a href="#SQL解析" class="headerlink" title="SQL解析"></a>SQL解析</h2><h3 id="编译器简介"><a href="#编译器简介" class="headerlink" title="编译器简介"></a>编译器简介</h3><h3 id="Antlr4、JavaCC-–-Parser"><a href="#Antlr4、JavaCC-–-Parser" class="headerlink" title="Antlr4、JavaCC – Parser"></a>Antlr4、JavaCC – Parser</h3><h3 id="Apache-Calcite、Spark-Catalyst-–-（Parser-amp-Optimizer）"><a href="#Apache-Calcite、Spark-Catalyst-–-（Parser-amp-Optimizer）" class="headerlink" title="Apache Calcite、Spark Catalyst  – （Parser&amp;Optimizer）"></a>Apache Calcite、Spark Catalyst  – （Parser&amp;Optimizer）</h3><h3 id="HIVE-SQL、Spark-SQL-执行过程"><a href="#HIVE-SQL、Spark-SQL-执行过程" class="headerlink" title="HIVE SQL、Spark SQL 执行过程"></a>HIVE SQL、Spark SQL 执行过程</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2020/09/17/Spark-HiveMetastoreCatalog-Infer-Schema/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/17/Spark-HiveMetastoreCatalog-Infer-Schema/" itemprop="url">Spark HiveMetastoreCatalog Infer Schema</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-09-17T21:34:07+08:00">
                2020-09-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Spark-HiveMetastoreCatalog-Infer-Schema"><a href="#Spark-HiveMetastoreCatalog-Infer-Schema" class="headerlink" title="Spark HiveMetastoreCatalog Infer Schema"></a>Spark HiveMetastoreCatalog Infer Schema</h2><h3 id="问题一"><a href="#问题一" class="headerlink" title="问题一"></a>问题一</h3><h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><p>报错显示读取 Parquet footer 错误，不过所读取的文件不是查询条件指定的分区。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">Driver stacktrace:</span><br><span class="line">	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)</span><br><span class="line">	......</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:633)</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:241)</span><br><span class="line">	at org.apache.spark.sql.hive.HiveMetastoreCatalog.org$apache$spark$sql$hive$HiveMetastoreCatalog$$inferIfNeeded(HiveMetastoreCatalog.scala:239)</span><br><span class="line">	at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$4$$anonfun$5.apply(HiveMetastoreCatalog.scala:167)</span><br><span class="line">	at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$4$$anonfun$5.apply(HiveMetastoreCatalog.scala:156)</span><br><span class="line">	at scala.Option.getOrElse(Option.scala:121)</span><br><span class="line">	at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$4.apply(HiveMetastoreCatalog.scala:156)</span><br><span class="line">	at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$4.apply(HiveMetastoreCatalog.scala:148)</span><br><span class="line">	at org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:54)</span><br><span class="line">	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:148)</span><br><span class="line">	at org.apache.spark.sql.hive.RelationConversions.org$apache$spark$sql$hive$RelationConversions$$convert(HiveStrategies.scala:207)</span><br><span class="line">	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:239)</span><br><span class="line">	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:228)</span><br><span class="line">	at ......</span><br><span class="line">Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: </span><br><span class="line">	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)</span><br><span class="line">	at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:290)</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:538)</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$9.apply(ParquetFileFormat.scala:611)</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$9.apply(ParquetFileFormat.scala:603)</span><br><span class="line">	at ......</span><br><span class="line">Caused by: java.io.IOException: Could not read footer for file: FileStatus&#123;path=hdfs://hadoop-bdwg-ns01/hive/warehouse/cupid_bi.db/report_qixiao_tracking_event_count_daily/dt=2016-05-24/000000_0.gz; isDirectory=false; length=1191537; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false&#125;</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:551)</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:538)</span><br><span class="line">	at ......</span><br><span class="line">Caused by: java.lang.RuntimeException: hdfs://hadoop-bdwg-ns01/hive/warehouse/cupid_bi.db/report_qixiao_tracking_event_count_daily/dt=2016-05-24/000000_0.gz is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [78, 59, 23, 1]</span><br><span class="line">	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:524)</span><br><span class="line">	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:505)</span><br><span class="line">	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:499)</span><br><span class="line">	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:476)</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:544)</span><br><span class="line">	... 9 more</span><br></pre></td></tr></table></figure>

<h4 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h4><p>关键信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:241)</span><br><span class="line">org.apache.spark.sql.hive.HiveMetastoreCatalog.org$apache$spark$sql$hive$HiveMetastoreCatalog$$inferIfNeeded(HiveMetastoreCatalog.scala:239)</span><br></pre></td></tr></table></figure>

<p>通过对错误栈进行代码分析，锁定 ParquetFileFormat.inferSchema 和 HiveMetastoreCatalog.inferIfNeeded 两个方法。发现错误是发生在 Parquet 文件的 Schema 推断中。</p>
<h3 id="问题二"><a href="#问题二" class="headerlink" title="问题二"></a>问题二</h3><h4 id="说明-1"><a href="#说明-1" class="headerlink" title="说明"></a>说明</h4><p>Driver 连接不上，像是卡住了，然后就直接退出，查询Application日志没有错误日志，查看Executor 的日志显示 java.io.IOException: Connection reset by peer</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Application application_1574672087080_11986014 failed 1 times due to ApplicationMaster for attempt appattempt_1574672087080_11986014_000001 timed out. Failing the application.</span><br></pre></td></tr></table></figure>

<h4 id="排查-1"><a href="#排查-1" class="headerlink" title="排查"></a>排查</h4><p>这个错误没有什么关键的错误信息，一般看到 Connection reset by peer（连接被重置）错误和 timed out 错误，想到调整超时时间，设置参数： <strong>spark.network.timeout&#x3D;1200s</strong>，不过发现并没有用，还没有达到此时间就报错了。</p>
<p>查看 ApplicationMaster 所在的机器，对 ApplicationMaster（Driver） 的线程栈进行分析，jstack 打印线程栈信息，发现关键信息如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">&quot;Driver&quot; #38 prio=5 os_prio=0 tid=0x00002b504f82a000 nid=0x78b0 runnable [0x00002b50809a9000]</span><br><span class="line">   java.lang.Thread.State: RUNNABLE</span><br><span class="line">	at java.lang.String.indexOf(String.java:1769)</span><br><span class="line">	at java.lang.String.indexOf(String.java:1718)</span><br><span class="line">	at org.apache.commons.lang.StringUtils.replace(StringUtils.java:3807)</span><br><span class="line">	at org.apache.commons.lang.StringUtils.replace(StringUtils.java:3771)</span><br><span class="line">	at org.apache.hadoop.fs.Path.normalizePath(Path.java:240)</span><br><span class="line">	at org.apache.hadoop.fs.Path.initialize(Path.java:203)</span><br><span class="line">	at org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:172)</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$bulkListLeafFiles$3$$anonfun$7.apply(InMemoryFileIndex.scala:251)</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$bulkListLeafFiles$3$$anonfun$7.apply(InMemoryFileIndex.scala:244)</span><br><span class="line">	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)</span><br><span class="line">	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)</span><br><span class="line">	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)</span><br><span class="line">	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)</span><br><span class="line">	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)</span><br><span class="line">	at scala.collection.AbstractTraversable.map(Traversable.scala:104)</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$bulkListLeafFiles$3.apply(InMemoryFileIndex.scala:244)</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$bulkListLeafFiles$3.apply(InMemoryFileIndex.scala:243)</span><br><span class="line">	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)</span><br><span class="line">	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)</span><br><span class="line">	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)</span><br><span class="line">	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)</span><br><span class="line">	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)</span><br><span class="line">	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:243)</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:126)</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:91)</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.&lt;init&gt;(InMemoryFileIndex.scala:67)</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex.&lt;init&gt;(CatalogFileIndex.scala:118)</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.CatalogFileIndex.filterPartitions(CatalogFileIndex.scala:84)</span><br><span class="line">	at org.apache.spark.sql.execution.datasources.CatalogFileIndex.listFiles(CatalogFileIndex.scala:59)</span><br><span class="line">	at org.apache.spark.sql.hive.HiveMetastoreCatalog.org$apache$spark$sql$hive$HiveMetastoreCatalog$$inferIfNeeded(HiveMetastoreCatalog.scala:242)</span><br><span class="line">	at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$4$$anonfun$5.apply(HiveMetastoreCatalog.scala:167)</span><br><span class="line">	at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$4$$anonfun$5.apply(HiveMetastoreCatalog.scala:156)</span><br><span class="line">	at scala.Option.getOrElse(Option.scala:121)</span><br><span class="line">	at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$4.apply(HiveMetastoreCatalog.scala:156)</span><br><span class="line">	at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$4.apply(HiveMetastoreCatalog.scala:148)</span><br><span class="line">	at org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:54)</span><br><span class="line">	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:148)</span><br><span class="line">	at org.apache.spark.sql.hive.RelationConversions.org$apache$spark$sql$hive$RelationConversions$$convert(HiveStrategies.scala:207)</span><br><span class="line">	at ......</span><br></pre></td></tr></table></figure>

<p>根据问题一中的经验，查询 InMemoryFileIndex 日志，发现 InMemoryFileIndex 扫描两万多目录。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">InMemoryFileIndex: Listing leaf files and directories in parallel under: hdfs://.....</span><br></pre></td></tr></table></figure>

<h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><p>查看 org.apache.spark.sql.hive.HiveMetastoreCatalog#inferIfNeeded 代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">private def inferIfNeeded(</span><br><span class="line">     relation: HiveTableRelation,</span><br><span class="line">     options: Map[String, String],</span><br><span class="line">     fileFormat: FileFormat,</span><br><span class="line">     fileIndexOpt: Option[FileIndex] = None): CatalogTable = &#123;</span><br><span class="line">   val inferenceMode = sparkSession.sessionState.conf.caseSensitiveInferenceMode</span><br><span class="line">   val shouldInfer = (inferenceMode != NEVER_INFER) &amp;&amp; !relation.tableMeta.schemaPreservesCase</span><br><span class="line">   val tableName = relation.tableMeta.identifier.unquotedString</span><br><span class="line">   if (shouldInfer) &#123;</span><br><span class="line">     logInfo(s&quot;Inferring case-sensitive schema for table $tableName (inference mode: &quot; +</span><br><span class="line">       s&quot;$inferenceMode)&quot;)</span><br><span class="line">     val fileIndex = fileIndexOpt.getOrElse &#123;</span><br><span class="line">       val rootPath = new Path(relation.tableMeta.location)</span><br><span class="line">       new InMemoryFileIndex(sparkSession, Seq(rootPath), options, None)</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     val inferredSchema = fileFormat</span><br><span class="line">       .inferSchema(</span><br><span class="line">         sparkSession,</span><br><span class="line">         options,</span><br><span class="line">         fileIndex.listFiles(Nil, Nil).flatMap(_.files))</span><br><span class="line">       .map(mergeWithMetastoreSchema(relation.tableMeta.dataSchema, _))</span><br><span class="line"></span><br><span class="line">     inferredSchema match &#123;</span><br><span class="line">       case Some(dataSchema) =&gt;</span><br><span class="line">         if (inferenceMode == INFER_AND_SAVE) &#123;</span><br><span class="line">           updateDataSchema(relation.tableMeta.identifier, dataSchema)</span><br><span class="line">         &#125;</span><br><span class="line">         val newSchema = StructType(dataSchema ++ relation.tableMeta.partitionSchema)</span><br><span class="line">         relation.tableMeta.copy(schema = newSchema)</span><br><span class="line">       case None =&gt;</span><br><span class="line">         logWarning(s&quot;Unable to infer schema for table $tableName from file format &quot; +</span><br><span class="line">           s&quot;$fileFormat (inference mode: $inferenceMode). Using metastore schema.&quot;)</span><br><span class="line">         relation.tableMeta</span><br><span class="line">     &#125;</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">     relation.tableMeta</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>shouldInfer 变量为 true 时会进行 Schema 推断，那么如何设置让它不进行推断呢？有下面两种情况：</p>
<ol>
<li>设置 <strong>spark.sql.hive.caseSensitiveInferenceMode&#x3D;NEVER_INFER</strong></li>
<li>schemaPreservesCase 为 true 时也跳过 infer schema，需要 Hive 表有 <strong>spark.sql.sources.schema.*</strong> 相关配置，并且 schema 和 table.schema 相等（相关判断在org.apache.spark.sql.hive.HiveExternalCatalog#restoreHiveSerdeTable里面）。</li>
</ol>
<p>后面选择设置 <strong>spark.sql.hive.caseSensitiveInferenceMode&#x3D;NEVER_INFER</strong>，关闭 Schema 推断解决问题。在 Spark3.0 中已经将此配置默认设置为 NEVER_INFER。</p>
<h3 id="相关说明"><a href="#相关说明" class="headerlink" title="相关说明"></a>相关说明</h3><ol>
<li><p>为何有 Schema 推断<br>由于 Hive Schema 是不区分大小写，Parquet 文件的 Schema 是区分大小写的，读取有大小名称的 Parquet 文件时可能会导致结果有问题。</p>
</li>
<li><p>caseSensitiveInferenceMode 的三种模式说明<br>caseSensitiveInferenceMode 有三种模式<br>INFER_AND_SAVE：此模式会在第一次进行Schema推断，然后保持到Hive表的properties里面（spark.sql.sources.schema.*）。<br>INFER_ONLY：进行推断，不会保持<br>NEVER_INFER：不进行推断</p>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2020/06/08/Apache-Atlas-%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/06/08/Apache-Atlas-%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/" itemprop="url">Apache Atlas 元数据管理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-06-08T18:25:42+08:00">
                2020-06-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Apache-Atlas-元数据管理"><a href="#Apache-Atlas-元数据管理" class="headerlink" title="Apache Atlas 元数据管理"></a>Apache Atlas 元数据管理</h2><p>官网地址： <a target="_blank" rel="noopener" href="http://atlas.apache.org/">http://atlas.apache.org/</a></p>
<p>Apache Atlas 是开源的Hadoop体系的元数据管理和数据治理工具。此次探索主要进行 Apache Atlas 的安装，收集 Hive 元数据和 Spark 元数据。</p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>参考：<a target="_blank" rel="noopener" href="http://atlas.apache.org/#/Configuration">配置</a>、<a target="_blank" rel="noopener" href="http://atlas.apache.org/#/BuildInstallation">源码构建安装</a></p>
<p>说明：</p>
<ul>
<li>Atlas 2.0.0 版本，依赖 HBase 2.0，由于测试集群 HBase 是 1.2 ，所以重新选择 Atlas 1.2.0 的包</li>
<li>Graph Search Index 选择的是 ElasticSearch，官方包中 atlas.graph.index.search.hostname 配置名称错误需要注意。</li>
<li>Kafka、Zookeeper 都是另外安装的，需要注释掉 atlas.notification.embedded 和 atlas.kafka.data 配置</li>
</ul>
<p>完整配置(atlas-application.properties )：<a href="#%E9%99%84%E5%BD%951">附录1</a></p>
<p>用户配置：conf&#x2F;users-credentials.properties</p>
<h3 id="支持-Hive"><a href="#支持-Hive" class="headerlink" title="支持 Hive"></a>支持 Hive</h3><p>Atlas Hive hook，通过监听 Hive 的一些操作，生成元数据发送到 Kakfa。</p>
<h4 id="安装使用-Hive-Hook"><a href="#安装使用-Hive-Hook" class="headerlink" title="安装使用 Hive Hook"></a>安装使用 Hive Hook</h4><ol>
<li><p>修改 hive-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.exec.post.hooks&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.atlas.hive.hook.HiveHook&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加 Atlas Hive hook 的依赖包<br>复制 ${ATLAS_HOME}&#x2F;hook&#x2F;hive 里面的依赖包到 ${HIVE_HOME}&#x2F;auxlib 中，复制${ATLAS_HOME}&#x2F;conf&#x2F;atlas-application.properties 到 ${HIVE_CONF_DIR} 中</p>
</li>
</ol>
<h4 id="验证-Hive-Hook"><a href="#验证-Hive-Hook" class="headerlink" title="验证 Hive Hook"></a>验证 Hive Hook</h4><p>执行下面语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create table wangz_test004 as select * from wangz_test</span><br><span class="line">insert into wangz_test004 select name01,age01,null from wangz_test001</span><br></pre></td></tr></table></figure>
<p>可以看到操作、数据库、表和字段的一些元数据，血缘关系如下：</p>
<img src="/2020/06/08/Apache-Atlas-%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/hive-lineage.png" class="" title="[Hive Lineage]">

<h3 id="支持-Spark"><a href="#支持-Spark" class="headerlink" title="支持 Spark"></a>支持 Spark</h3><p>使用开源的 <a target="_blank" rel="noopener" href="https://github.com/hortonworks-spark/spark-atlas-connector">spark-atlas-connector</a> ，在使用过程中发现 spark sql 操作 hive 表时，无法生成 Lineage 信息，我做了一些修改 <a target="_blank" rel="noopener" href="https://github.com/wForget/spark-atlas-connector/tree/dev">wForget&#x2F;spark-atlas-connector&#x2F;tree&#x2F;dev</a></p>
<h4 id="安装使用-spark-atlas-connector"><a href="#安装使用-spark-atlas-connector" class="headerlink" title="安装使用 spark-atlas-connector"></a>安装使用 spark-atlas-connector</h4><p>下载源码  <a target="_blank" rel="noopener" href="https://github.com/wForget/spark-atlas-connector/tree/dev">wForget&#x2F;spark-atlas-connector&#x2F;tree&#x2F;dev</a> ，进行打包 (mvn clean -DskipTests package)。<br>复制 1100-spark_model.json 文件到 ${ATLAS_HOME}&#x2F;models&#x2F;1000-Hadoop目录下，重启 Atlas。<br>复制 atlas-application.properties 配置文件到 ${SPARK_HOME}&#x2F;conf 下面。<br>执行下面语句启动 spark shell（也可以直接配置到spark 默认配置中）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark-shell \</span><br><span class="line">--jars  $&#123;ATLAS_HOME&#125;/hook/spark/spark-atlas-connector-assembly-0.1.0-SNAPSHOT.jar \</span><br><span class="line">--conf spark.extraListeners=com.hortonworks.spark.atlas.SparkAtlasEventTracker \</span><br><span class="line">--conf spark.sql.queryExecutionListeners=com.hortonworks.spark.atlas.SparkAtlasEventTracker \</span><br><span class="line">--conf spark.sql.streaming.streamingQueryListeners=com.hortonworks.spark.atlas.SparkAtlasStreamingQueryEventTracker</span><br></pre></td></tr></table></figure>

<h4 id="验证-spark-atlas-connector"><a href="#验证-spark-atlas-connector" class="headerlink" title="验证 spark-atlas-connector"></a>验证 spark-atlas-connector</h4><p>执行下面语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;create table test.wangz_test012 as select * from test.wangz_test&quot;).show()</span><br><span class="line">spark.sql(&quot;insert into test.wangz_test012 select name01,age01,null from test.wangz_test001&quot;).show()</span><br></pre></td></tr></table></figure>
<p>可以看到操作、数据库、表和字段的一些元数据，血缘关系如下：</p>
<img src="/2020/06/08/Apache-Atlas-%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/spark-lineage.png" class="" title="[Spark Lineage]">

<h4 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h4><ul>
<li>Spark Sql 操作 Hive Table 时，经过对 spark-atlas-connector 简单的修改，是可以看到血缘关系，不过 table db 等类型还是 spark 类型，需要转换成 hive 类型。（已经完成，参考：<a target="_blank" rel="noopener" href="https://github.com/wForget/spark-atlas-connector/tree/dev-hive">wForget&#x2F;spark-atlas-connector&#x2F;tree&#x2F;dev-hive</a> ）</li>
</ul>
<h3 id="附录1"><a href="#附录1" class="headerlink" title="附录1"></a>附录1</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#########  Graph Database Configs  #########</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Graph Database</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#Configures the graph database to use.  Defaults to JanusGraph</span></span><br><span class="line"><span class="comment">#atlas.graphdb.backend=org.apache.atlas.repository.graphdb.janus.AtlasJanusGraphDatabase</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Graph Storage</span></span><br><span class="line"><span class="comment"># Set atlas.graph.storage.backend to the correct value for your desired storage</span></span><br><span class="line"><span class="comment"># backend. Possible values:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># hbase</span></span><br><span class="line"><span class="comment"># cassandra</span></span><br><span class="line"><span class="comment"># embeddedcassandra - Should only be set by building Atlas with  -Pdist,embedded-cassandra-solr</span></span><br><span class="line"><span class="comment"># berkeleyje</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See the configuration documentation for more information about configuring the various  storage backends.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="attr">atlas.graph.storage.backend</span>=<span class="string">hbase</span></span><br><span class="line"><span class="attr">atlas.graph.storage.hbase.table</span>=<span class="string">apache_atlas_janus</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#Hbase</span></span><br><span class="line"><span class="comment">#For standalone mode , specify localhost</span></span><br><span class="line"><span class="comment">#for distributed mode, specify zookeeper quorum here</span></span><br><span class="line"><span class="attr">atlas.graph.storage.hostname</span>=<span class="string">10.6.160.***,10.6.160.***,10.6.160.***</span></span><br><span class="line"><span class="attr">atlas.graph.storage.hbase.regions-per-server</span>=<span class="string">1</span></span><br><span class="line"><span class="attr">atlas.graph.storage.lock.wait-time</span>=<span class="string">10000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#In order to use Cassandra as a backend, comment out the hbase specific properties above, and uncomment the</span></span><br><span class="line"><span class="comment">#the following properties</span></span><br><span class="line"><span class="comment">#atlas.graph.storage.clustername=</span></span><br><span class="line"><span class="comment">#atlas.graph.storage.port=</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Gremlin Query Optimizer</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Enables rewriting gremlin queries to maximize performance. This flag is provided as</span></span><br><span class="line"><span class="comment"># a possible way to work around any defects that are found in the optimizer until they</span></span><br><span class="line"><span class="comment"># are resolved.</span></span><br><span class="line"><span class="comment">#atlas.query.gremlinOptimizerEnabled=true</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Delete handler</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># This allows the default behavior of doing &quot;soft&quot; deletes to be changed.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Allowed Values:</span></span><br><span class="line"><span class="comment"># org.apache.atlas.repository.store.graph.v1.SoftDeleteHandlerV1 - all deletes are &quot;soft&quot; deletes</span></span><br><span class="line"><span class="comment"># org.apache.atlas.repository.store.graph.v1.HardDeleteHandlerV1 - all deletes are &quot;hard&quot; deletes</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#atlas.DeleteHandlerV1.impl=org.apache.atlas.repository.store.graph.v1.SoftDeleteHandlerV1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Entity audit repository</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># This allows the default behavior of logging entity changes to hbase to be changed.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Allowed Values:</span></span><br><span class="line"><span class="comment"># org.apache.atlas.repository.audit.HBaseBasedAuditRepository - log entity changes to hbase</span></span><br><span class="line"><span class="comment"># org.apache.atlas.repository.audit.CassandraBasedAuditRepository - log entity changes to cassandra</span></span><br><span class="line"><span class="comment"># org.apache.atlas.repository.audit.NoopEntityAuditRepository - disable the audit repository</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="attr">atlas.EntityAuditRepository.impl</span>=<span class="string">org.apache.atlas.repository.audit.HBaseBasedAuditRepository</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># if Cassandra is used as a backend for audit from the above property, uncomment and set the following</span></span><br><span class="line"><span class="comment"># properties appropriately. If using the embedded cassandra profile, these properties can remain</span></span><br><span class="line"><span class="comment"># commented out.</span></span><br><span class="line"><span class="comment"># atlas.EntityAuditRepository.keyspace=atlas_audit</span></span><br><span class="line"><span class="comment"># atlas.EntityAuditRepository.replicationFactor=1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Graph Search Index</span></span><br><span class="line"><span class="comment">#atlas.graph.index.search.backend=solr</span></span><br><span class="line"><span class="attr">atlas.graph.index.search.backend</span>=<span class="string">elasticsearch</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#Solr</span></span><br><span class="line"><span class="comment">#Solr cloud mode properties</span></span><br><span class="line"><span class="comment">#atlas.graph.index.search.solr.mode=cloud</span></span><br><span class="line"><span class="comment">#atlas.graph.index.search.solr.zookeeper-url=</span></span><br><span class="line"><span class="comment">#atlas.graph.index.search.solr.zookeeper-connect-timeout=60000</span></span><br><span class="line"><span class="comment">#atlas.graph.index.search.solr.zookeeper-session-timeout=60000</span></span><br><span class="line"><span class="comment">#atlas.graph.index.search.solr.wait-searcher=true</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#Solr http mode properties</span></span><br><span class="line"><span class="comment">#atlas.graph.index.search.solr.mode=http</span></span><br><span class="line"><span class="comment">#atlas.graph.index.search.solr.http-urls=http://localhost:8983/solr</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># ElasticSearch support (Tech Preview)</span></span><br><span class="line"><span class="comment"># Comment out above solr configuration, and uncomment the following two lines. Additionally, make sure the</span></span><br><span class="line"><span class="comment"># hostname field is set to a comma delimited set of elasticsearch master nodes, or an ELB that fronts the masters.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Elasticsearch does not provide authentication out of the box, but does provide an option with the X-Pack product</span></span><br><span class="line"><span class="comment"># https://www.elastic.co/products/x-pack/security</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Alternatively, the JanusGraph documentation provides some tips on how to secure Elasticsearch without additional</span></span><br><span class="line"><span class="comment"># plugins: https://docs.janusgraph.org/latest/elasticsearch.html</span></span><br><span class="line"><span class="comment">#atlas.graph.index.search.hostname=10.18.40.230:9200</span></span><br><span class="line"><span class="attr">atlas.graph.index.search.hostname</span>=<span class="string">10.**.**.**:9200,10.**.**.**:9200,10.**.**.**:9200</span></span><br><span class="line"><span class="attr">atlas.graph.index.search.elasticsearch.client-only</span>=<span class="string">true</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Solr-specific configuration property</span></span><br><span class="line"><span class="attr">atlas.graph.index.search.max-result-set-size</span>=<span class="string">150</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#########  Notification Configs  #########</span></span><br><span class="line"><span class="comment">#atlas.notification.embedded=true</span></span><br><span class="line"><span class="attr">atlas.notification.embedded</span>=<span class="string">false</span></span><br><span class="line"><span class="comment">#atlas.kafka.data=$&#123;sys:atlas.home&#125;/data/kafka</span></span><br><span class="line"><span class="attr">atlas.kafka.zookeeper.connect</span>=<span class="string">localhost:12181</span></span><br><span class="line"><span class="attr">atlas.kafka.bootstrap.servers</span>=<span class="string">localhost:9092</span></span><br><span class="line"><span class="attr">atlas.kafka.zookeeper.session.timeout.ms</span>=<span class="string">400</span></span><br><span class="line"><span class="attr">atlas.kafka.zookeeper.connection.timeout.ms</span>=<span class="string">200</span></span><br><span class="line"><span class="attr">atlas.kafka.zookeeper.sync.time.ms</span>=<span class="string">20</span></span><br><span class="line"><span class="attr">atlas.kafka.auto.commit.interval.ms</span>=<span class="string">1000</span></span><br><span class="line"><span class="attr">atlas.kafka.hook.group.id</span>=<span class="string">atlas</span></span><br><span class="line"></span><br><span class="line"><span class="attr">atlas.kafka.enable.auto.commit</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">atlas.kafka.auto.offset.reset</span>=<span class="string">earliest</span></span><br><span class="line"><span class="attr">atlas.kafka.session.timeout.ms</span>=<span class="string">30000</span></span><br><span class="line"><span class="attr">atlas.kafka.offsets.topic.replication.factor</span>=<span class="string">1</span></span><br><span class="line"><span class="attr">atlas.kafka.poll.timeout.ms</span>=<span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">atlas.notification.create.topics</span>=<span class="string">true</span></span><br><span class="line"><span class="attr">atlas.notification.replicas</span>=<span class="string">1</span></span><br><span class="line"><span class="attr">atlas.notification.topics</span>=<span class="string">ATLAS_HOOK,ATLAS_ENTITIES</span></span><br><span class="line"><span class="attr">atlas.notification.log.failed.messages</span>=<span class="string">true</span></span><br><span class="line"><span class="attr">atlas.notification.consumer.retry.interval</span>=<span class="string">500</span></span><br><span class="line"><span class="attr">atlas.notification.hook.retry.interval</span>=<span class="string">1000</span></span><br><span class="line"><span class="comment"># Enable for Kerberized Kafka clusters</span></span><br><span class="line"><span class="comment">#atlas.notification.kafka.service.principal=kafka/_HOST@EXAMPLE.COM</span></span><br><span class="line"><span class="comment">#atlas.notification.kafka.keytab.location=/etc/security/keytabs/kafka.service.keytab</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">## Server port configuration</span></span><br><span class="line"><span class="attr">atlas.server.http.port</span>=<span class="string">21001</span></span><br><span class="line"><span class="comment">#atlas.server.https.port=21443</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#########  Security Properties  #########</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># SSL config</span></span><br><span class="line"><span class="attr">atlas.enableTLS</span>=<span class="string">false</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#truststore.file=/path/to/truststore.jks</span></span><br><span class="line"><span class="comment">#cert.stores.credential.provider.path=jceks://file/path/to/credentialstore.jceks</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#following only required for 2-way SSL</span></span><br><span class="line"><span class="comment">#keystore.file=/path/to/keystore.jks</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Authentication config</span></span><br><span class="line"></span><br><span class="line"><span class="attr">atlas.authentication.method.kerberos</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">atlas.authentication.method.file</span>=<span class="string">true</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#### ldap.type= LDAP or AD</span></span><br><span class="line"><span class="attr">atlas.authentication.method.ldap.type</span>=<span class="string">none</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#### user credentials file</span></span><br><span class="line"><span class="attr">atlas.authentication.method.file.filename</span>=<span class="string">$&#123;sys:atlas.home&#125;/conf/users-credentials.properties</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">### groups from UGI</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.ugi-groups=true</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######## LDAP properties #########</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.url=ldap://&lt;ldap server url&gt;:389</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.userDNpattern=uid=&#123;0&#125;,ou=People,dc=example,dc=com</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.groupSearchBase=dc=example,dc=com</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.groupSearchFilter=(member=uid=&#123;0&#125;,ou=Users,dc=example,dc=com)</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.groupRoleAttribute=cn</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.base.dn=dc=example,dc=com</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.bind.dn=cn=Manager,dc=example,dc=com</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.bind.password=&lt;password&gt;</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.referral=ignore</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.user.searchfilter=(uid=&#123;0&#125;)</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.default.role=&lt;default role&gt;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######### Active directory properties #######</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.ad.domain=example.com</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.ad.url=ldap://&lt;AD server url&gt;:389</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.ad.base.dn=(sAMAccountName=&#123;0&#125;)</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.ad.bind.dn=CN=team,CN=Users,DC=example,DC=com</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.ad.bind.password=&lt;password&gt;</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.ad.referral=ignore</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.ad.user.searchfilter=(sAMAccountName=&#123;0&#125;)</span></span><br><span class="line"><span class="comment">#atlas.authentication.method.ldap.ad.default.role=&lt;default role&gt;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#########  JAAS Configuration ########</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#atlas.jaas.KafkaClient.loginModuleName = com.sun.security.auth.module.Krb5LoginModule</span></span><br><span class="line"><span class="comment">#atlas.jaas.KafkaClient.loginModuleControlFlag = required</span></span><br><span class="line"><span class="comment">#atlas.jaas.KafkaClient.option.useKeyTab = true</span></span><br><span class="line"><span class="comment">#atlas.jaas.KafkaClient.option.storeKey = true</span></span><br><span class="line"><span class="comment">#atlas.jaas.KafkaClient.option.serviceName = kafka</span></span><br><span class="line"><span class="comment">#atlas.jaas.KafkaClient.option.keyTab = /etc/security/keytabs/atlas.service.keytab</span></span><br><span class="line"><span class="comment">#atlas.jaas.KafkaClient.option.principal = atlas/_HOST@EXAMPLE.COM</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#########  Server Properties  #########</span></span><br><span class="line"><span class="attr">atlas.rest.address</span>=<span class="string">http://localhost:21001</span></span><br><span class="line"><span class="comment"># If enabled and set to true, this will run setup steps when the server starts</span></span><br><span class="line"><span class="comment">#atlas.server.run.setup.on.start=false</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#########  Entity Audit Configs  #########</span></span><br><span class="line"><span class="attr">atlas.audit.hbase.tablename</span>=<span class="string">apache_atlas_entity_audit</span></span><br><span class="line"><span class="attr">atlas.audit.zookeeper.session.timeout.ms</span>=<span class="string">1000</span></span><br><span class="line"><span class="attr">atlas.audit.hbase.zookeeper.quorum</span>=<span class="string">10.6.160.***,10.6.160.***,10.6.160.***</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#########  High Availability Configuration ########</span></span><br><span class="line"><span class="attr">atlas.server.ha.enabled</span>=<span class="string">false</span></span><br><span class="line"><span class="comment">#### Enabled the configs below as per need if HA is enabled #####</span></span><br><span class="line"><span class="comment">#atlas.server.ids=id1</span></span><br><span class="line"><span class="comment">#atlas.server.address.id1=localhost:21000</span></span><br><span class="line"><span class="comment">#atlas.server.ha.zookeeper.connect=localhost:2181</span></span><br><span class="line"><span class="comment">#atlas.server.ha.zookeeper.retry.sleeptime.ms=1000</span></span><br><span class="line"><span class="comment">#atlas.server.ha.zookeeper.num.retries=3</span></span><br><span class="line"><span class="comment">#atlas.server.ha.zookeeper.session.timeout.ms=20000</span></span><br><span class="line"><span class="comment">## if ACLs need to be set on the created nodes, uncomment these lines and set the values ##</span></span><br><span class="line"><span class="comment">#atlas.server.ha.zookeeper.acl=&lt;scheme&gt;:&lt;id&gt;</span></span><br><span class="line"><span class="comment">#atlas.server.ha.zookeeper.auth=&lt;scheme&gt;:&lt;authinfo&gt;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######### Atlas Authorization #########</span></span><br><span class="line"><span class="attr">atlas.authorizer.impl</span>=<span class="string">simple</span></span><br><span class="line"><span class="attr">atlas.authorizer.simple.authz.policy.file</span>=<span class="string">atlas-simple-authz-policy.json</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#########  Type Cache Implementation ########</span></span><br><span class="line"><span class="comment"># A type cache class which implements</span></span><br><span class="line"><span class="comment"># org.apache.atlas.typesystem.types.cache.TypeCache.</span></span><br><span class="line"><span class="comment"># The default implementation is org.apache.atlas.typesystem.types.cache.DefaultTypeCache which is a local in-memory type cache.</span></span><br><span class="line"><span class="comment">#atlas.TypeCache.impl=</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#########  Performance Configs  #########</span></span><br><span class="line"><span class="comment">#atlas.graph.storage.lock.retries=10</span></span><br><span class="line"><span class="comment">#atlas.graph.storage.cache.db-cache-time=120000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#########  CSRF Configs  #########</span></span><br><span class="line"><span class="attr">atlas.rest-csrf.enabled</span>=<span class="string">true</span></span><br><span class="line"><span class="attr">atlas.rest-csrf.browser-useragents-regex</span>=<span class="string">^Mozilla.*,^Opera.*,^Chrome.*</span></span><br><span class="line"><span class="attr">atlas.rest-csrf.methods-to-ignore</span>=<span class="string">GET,OPTIONS,HEAD,TRACE</span></span><br><span class="line"><span class="attr">atlas.rest-csrf.custom-header</span>=<span class="string">X-XSRF-HEADER</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############ KNOX Configs ################</span></span><br><span class="line"><span class="comment">#atlas.sso.knox.browser.useragent=Mozilla,Chrome,Opera</span></span><br><span class="line"><span class="comment">#atlas.sso.knox.enabled=true</span></span><br><span class="line"><span class="comment">#atlas.sso.knox.providerurl=https://&lt;knox gateway ip&gt;:8443/gateway/knoxsso/api/v1/websso</span></span><br><span class="line"><span class="comment">#atlas.sso.knox.publicKey=</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">############ Atlas Metric/Stats configs ################</span></span><br><span class="line"><span class="comment"># Format: atlas.metric.query.&lt;key&gt;.&lt;name&gt;</span></span><br><span class="line"><span class="attr">atlas.metric.query.cache.ttlInSecs</span>=<span class="string">900</span></span><br><span class="line"><span class="comment">#atlas.metric.query.general.typeCount=</span></span><br><span class="line"><span class="comment">#atlas.metric.query.general.typeUnusedCount=</span></span><br><span class="line"><span class="comment">#atlas.metric.query.general.entityCount=</span></span><br><span class="line"><span class="comment">#atlas.metric.query.general.tagCount=</span></span><br><span class="line"><span class="comment">#atlas.metric.query.general.entityDeleted=</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#atlas.metric.query.entity.typeEntities=</span></span><br><span class="line"><span class="comment">#atlas.metric.query.entity.entityTagged=</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#atlas.metric.query.tags.entityTags=</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#########  Compiled Query Cache Configuration  #########</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># The size of the compiled query cache.  Older queries will be evicted from the cache</span></span><br><span class="line"><span class="comment"># when we reach the capacity.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#atlas.CompiledQueryCache.capacity=1000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Allows notifications when items are evicted from the compiled query</span></span><br><span class="line"><span class="comment"># cache because it has become full.  A warning will be issued when</span></span><br><span class="line"><span class="comment"># the specified number of evictions have occurred.  If the eviction</span></span><br><span class="line"><span class="comment"># warning threshold &lt;= 0, no eviction warnings will be issued.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#atlas.CompiledQueryCache.evictionWarningThrottle=0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#########  Full Text Search Configuration  #########</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#Set to false to disable full text search.</span></span><br><span class="line"><span class="comment">#atlas.search.fulltext.enable=true</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#########  Gremlin Search Configuration  #########</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#Set to false to disable gremlin search.</span></span><br><span class="line"><span class="attr">atlas.search.gremlin.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">########## Add http headers ###########</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#atlas.headers.Access-Control-Allow-Origin=*</span></span><br><span class="line"><span class="comment">#atlas.headers.Access-Control-Allow-Methods=GET,OPTIONS,HEAD,PUT,POST</span></span><br><span class="line"><span class="comment">#atlas.headers.&lt;headerName&gt;=&lt;headerValue&gt;</span></span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2020/05/12/%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F%E5%B7%A5%E4%BD%9C%E6%B5%81%EF%BC%88DAG%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/05/12/%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F%E5%B7%A5%E4%BD%9C%E6%B5%81%EF%BC%88DAG%EF%BC%89/" itemprop="url">调度系统工作流（DAG）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-05-12T13:50:38+08:00">
                2020-05-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="调度系统工作流（DAG）"><a href="#调度系统工作流（DAG）" class="headerlink" title="调度系统工作流（DAG）"></a>调度系统工作流（DAG）</h2><p>DAG是有向无环图的意思，调度系统中工作流就是以DAG的方式组织Task的依赖关系。</p>
<h2 id="公司自研调度系统工作流"><a href="#公司自研调度系统工作流" class="headerlink" title="公司自研调度系统工作流"></a>公司自研调度系统工作流</h2><h3 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h3><ol>
<li>Job<br> 定义执行的任务和调度策略。</li>
<li>DAG<br> 组织 Job  之间的依赖关系。</li>
<li>Task<br> 系统根据 Job 调度策略生成的执行任务。</li>
</ol>
<h3 id="调度过程"><a href="#调度过程" class="headerlink" title="调度过程"></a>调度过程</h3><ol>
<li>创建 Job （根据需要创建  DAG）</li>
<li>JobManager 根据 Job 的调度策略定时创建 Task，没有依赖的 Job 的 Task 直接标记为准备执行状态，有依赖的 Job 的 Task 标记为初始化的状态（父任务执行完成后触发状态修改）。</li>
<li>任务调度器获取准备执行状态的 Task，根据一些权限资源的限制和 Workers  的状态给 Task 分配最合适的 Worker，并将任务下发到 WorkerManager。</li>
<li>WorkerManager 接收到任务下发的请求，再将任务发送到指定的 Worker。</li>
<li>Worker 接收任务并执行，执行完成后向上回馈状态。</li>
<li>JobManager 接收到 Task 执行的回馈，触发子 Job 对应时间节点的 Task 状态修改。</li>
</ol>
<h2 id="Dolphin-Scheduler-工作流"><a href="#Dolphin-Scheduler-工作流" class="headerlink" title="Dolphin Scheduler 工作流"></a>Dolphin Scheduler 工作流</h2><h3 id="相关概念-1"><a href="#相关概念-1" class="headerlink" title="相关概念"></a>相关概念</h3><ol>
<li>流程定义<br> 以拖拽的方式组织任务节点间的依赖，形成可视化的 DAG。</li>
<li>流程实例<br> 流程实例是流程定义的实例化，通过定时调度或者手动调度可以对流程进行实例化并生成流程实例。</li>
<li>任务实例<br> 任务实例是流程定义中的任务节点的实例，关联着一个流程实例，是具体执行的最小单位。</li>
<li>调度方式<br> 调度方式包括定时调度和手动调度。定时调度是系统采用 Quartz 调度器根据配置的 Cron 策略定时生成流程实例的过程。手动调度是用户手动触发一次流程实例化的过程。</li>
</ol>
<h3 id="调度过程-1"><a href="#调度过程-1" class="headerlink" title="调度过程"></a>调度过程</h3><ol>
<li>定义流程</li>
<li>配置调度方式</li>
<li>系统根据调度的方式生成流程实例</li>
<li>Master 抽取需要执行的流程实例，并根据任务的依赖关系，生成一系列的任务实例，并放入任务的队列中（Master 会等待整个流程实例执行完成，根据依赖任务的执行状态，来控制任务实例的生成）。</li>
<li>Worker 节点根据自身的运行情况，去任务队列中获取需要执行的任务实例并运行。</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2019/12/12/Hive-SQL%E7%9A%84%E7%BC%96%E8%AF%91%E8%BF%87%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/12/12/Hive-SQL%E7%9A%84%E7%BC%96%E8%AF%91%E8%BF%87%E7%A8%8B/" itemprop="url">Hive SQL的编译过程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-12-12T20:05:07+08:00">
                2019-12-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/">&lt;i class&#x3D;&quot;fa fa-angle-left&quot;&gt;&lt;&#x2F;i&gt;</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/3/">&lt;i class&#x3D;&quot;fa fa-angle-right&quot;&gt;&lt;&#x2F;i&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">45</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">28</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/wForget" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wangz</span>

  
</div>









        







  <div style="display: none;">
    <script src="//s23.cnzz.com/z_stat.php?id=1276876819&web_id=1276876819" language="JavaScript"></script>
  </div>



        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  













  





  

  

  

  
  

  

  

  

</body>
</html>
