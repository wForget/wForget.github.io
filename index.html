<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">
<meta name="google-site-verification" content="nPT3ONUGntVHfQCZH2D5GcUMgg5DH4IReN4GIs0GrW8" />








<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="好记性不如烂笔头！">
<meta property="og:type" content="website">
<meta property="og:title" content="wForget&#39;s blog">
<meta property="og:url" content="https://wforget.github.io/index.html">
<meta property="og:site_name" content="wForget&#39;s blog">
<meta property="og:description" content="好记性不如烂笔头！">
<meta property="og:locale">
<meta property="article:author" content="wangz">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://wforget.github.io/"/>





  <title>wForget's blog</title>
  








<meta name="generator" content="Hexo 6.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">wForget's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2022/10/18/Kyuubi-Zorder-%E8%B0%83%E7%A0%94%E6%B5%8B%E8%AF%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/10/18/Kyuubi-Zorder-%E8%B0%83%E7%A0%94%E6%B5%8B%E8%AF%95/" itemprop="url">Kyuubi Zorder 调研测试</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-10-18T21:18:32+08:00">
                2022-10-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Kyuubi-Z-order-调研测试"><a href="#Kyuubi-Z-order-调研测试" class="headerlink" title="Kyuubi Z-order 调研测试"></a>Kyuubi Z-order 调研测试</h2><p>Z-order 是一种将多维数据映射到一维上的排序算法，排序后使得多维数据彼此临近，更有利于数据压缩和查询时 Data Skip。</p>
<p>所谓的 Z-order 优化实际上是在数据写入时添加 Z-order 排序，将数据进行聚类，很好的提升了数据的压缩率和查询性能。</p>
<p>Kyuubi 通过 Spark 插件的形式，实现了 Z-order 优化，具体使用参考：<a target="_blank" rel="noopener" href="https://kyuubi.apache.org/docs/latest/extensions/engines/spark/z-order.html">Z-Ordering Support</a>。</p>
<p>下面我参考了 Kyuubi 官方文档对 Z-order 功能进行调研测试：<a target="_blank" rel="noopener" href="https://kyuubi.apache.org/docs/latest/extensions/engines/spark/z-order-benchmark.html">Z-order Benchmark</a></p>
<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>使用 Beeline 连接 Kyuubi，准备测试数据。</p>
<h3 id="1、创建原始数据表："><a href="#1、创建原始数据表：" class="headerlink" title="1、创建原始数据表："></a>1、创建原始数据表：</h3><p>关闭 AQE ，便于控制输出文件数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 关闭 AQE</span><br><span class="line">set spark.sql.adaptive.enabled=false;</span><br><span class="line">set spark.sql.shuffle.partitions=200;</span><br></pre></td></tr></table></figure>

<p>创建原始表：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create table sample.conn_random (</span><br><span class="line">  src_ip string,</span><br><span class="line">  src_port int,</span><br><span class="line">  dst_ip string,</span><br><span class="line">  dst_port int) stored as parquet;</span><br></pre></td></tr></table></figure>

<p>使用 Scala 执行模式生成原始表数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// 切换成 Scala 执行模式</span><br><span class="line">set kyuubi.operation.language=scala;</span><br><span class="line"></span><br><span class="line">import scala.util.Random;</span><br><span class="line">def randomIPv4(r: Random) = Seq.fill(4)(r.nextInt(256)).mkString(&quot;.&quot;);</span><br><span class="line">def randomPort(r: Random) = r.nextInt(65536);</span><br><span class="line"></span><br><span class="line">case class ConnRecord(src_ip: String, src_port: Int, dst_ip: String, dst_port: Int);</span><br><span class="line">def randomConnRecord(r: Random) = ConnRecord(src_ip = randomIPv4(r), src_port = randomPort(r), dst_ip = randomIPv4(r), dst_port = randomPort(r));</span><br><span class="line"></span><br><span class="line">val numRecords = 100 * 1000 * 1000L;</span><br><span class="line">val numFiles = 200;</span><br><span class="line"></span><br><span class="line">val df = spark.range(0, numFiles, 1, numFiles).mapPartitions &#123; it =&gt;</span><br><span class="line">  val partitionID = it.toStream.head</span><br><span class="line">  val r = new Random(seed = partitionID)</span><br><span class="line">  Iterator.fill((numRecords / numFiles).toInt)(randomConnRecord(r))</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">// 创建原始测试数据表 sample.conn_random</span><br><span class="line">df.write.mode(&quot;overwrite&quot;).format(&quot;parquet&quot;).insertInto(&quot;sample.conn_random&quot;);</span><br><span class="line"></span><br><span class="line">// 切换成 SQL 执行模式</span><br><span class="line">spark.sql(&quot;set kyuubi.operation.language=sql&quot;);</span><br></pre></td></tr></table></figure>

<h3 id="2、生成-Order-by-排序的表："><a href="#2、生成-Order-by-排序的表：" class="headerlink" title="2、生成 Order by 排序的表："></a>2、生成 Order by 排序的表：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// conn_order_only_ip</span><br><span class="line">create table sample.conn_order_only_ip like sample.conn_random;</span><br><span class="line">INSERT overwrite sample.conn_order_only_ip select * from sample.conn_random order by src_ip, dst_ip;</span><br><span class="line"></span><br><span class="line">// conn_order</span><br><span class="line">create table sample.conn_order like sample.conn_random;</span><br><span class="line">INSERT overwrite sample.conn_order select * from sample.conn_random order by src_ip, src_port, dst_ip, dst_port;</span><br></pre></td></tr></table></figure>

<h3 id="2、生成-Z-order-优化的表："><a href="#2、生成-Z-order-优化的表：" class="headerlink" title="2、生成 Z-order 优化的表："></a>2、生成 Z-order 优化的表：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// conn_zorder_only_ip</span><br><span class="line">create table sample.conn_zorder_only_ip like sample.conn_random;</span><br><span class="line">insert overwrite sample.conn_zorder_only_ip select * from sample.conn_random;</span><br><span class="line">OPTIMIZE sample.conn_zorder_only_ip ZORDER BY src_ip, dst_ip;</span><br><span class="line"></span><br><span class="line">// conn_zorder</span><br><span class="line">create table sample.conn_zorder like sample.conn_random;</span><br><span class="line">insert overwrite sample.conn_zorder select * from sample.conn_random;</span><br><span class="line">OPTIMIZE sample.conn_zorder ZORDER BY src_ip, src_port, dst_ip, dst_port;</span><br></pre></td></tr></table></figure>

<h2 id="Zorder-优化效果对比分析"><a href="#Zorder-优化效果对比分析" class="headerlink" title="Zorder 优化效果对比分析"></a>Zorder 优化效果对比分析</h2><h3 id="1、存储分析"><a href="#1、存储分析" class="headerlink" title="1、存储分析"></a>1、存储分析</h3><table>
<thead>
<tr>
<th>表名</th>
<th>大小</th>
<th>生成时间</th>
</tr>
</thead>
<tbody><tr>
<td>conn_random</td>
<td>2.7 G</td>
<td>1.1 min</td>
</tr>
<tr>
<td>conn_order_only_ip</td>
<td>2.2 G</td>
<td>1.3 min</td>
</tr>
<tr>
<td>conn_order</td>
<td>2.2 G</td>
<td>52 s</td>
</tr>
<tr>
<td>conn_zorder_only_ip</td>
<td>1510.3 MiB</td>
<td>1.9 min</td>
</tr>
<tr>
<td>conn_zorder</td>
<td>1510.4 MiB</td>
<td>1.9 min</td>
</tr>
</tbody></table>
<h3 id="2、查询分析"><a href="#2、查询分析" class="headerlink" title="2、查询分析"></a>2、查询分析</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// query</span><br><span class="line">select count(*) from sample.conn_random where src_ip like &#x27;157%&#x27; and dst_ip like &#x27;216.%&#x27;;</span><br><span class="line">select count(*) from sample.conn_order_only_ip where src_ip like &#x27;157%&#x27; and dst_ip like &#x27;216.%&#x27;;</span><br><span class="line">select count(*) from sample.conn_order where src_ip like &#x27;157%&#x27; and dst_ip like &#x27;216.%&#x27;;</span><br><span class="line">select count(*) from sample.conn_zorder_only_ip where src_ip like &#x27;157%&#x27; and dst_ip like &#x27;216.%&#x27;;</span><br><span class="line">select count(*) from sample.conn_zorder where src_ip like &#x27;157%&#x27; and dst_ip like &#x27;216.%&#x27;;</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>表名</th>
<th>扫描数据量</th>
</tr>
</thead>
<tbody><tr>
<td>conn_random</td>
<td>100,000,000</td>
</tr>
<tr>
<td>conn_order_only_ip</td>
<td>400,000</td>
</tr>
<tr>
<td>conn_order</td>
<td>400,648</td>
</tr>
<tr>
<td>conn_zorder_only_ip</td>
<td>120,000</td>
</tr>
<tr>
<td>conn_zorder</td>
<td>120,000</td>
</tr>
</tbody></table>
<h3 id="3、总结"><a href="#3、总结" class="headerlink" title="3、总结"></a>3、总结</h3><p>从上面的测试结果中可以很直观的看到 Z-order 优化的一些特点：</p>
<ul>
<li><strong>排序消耗</strong>：由于需要对数据进行排序，所以对于数据生产任务有少许额为的排序消耗。</li>
<li><strong>减少空间</strong>：数据进行了聚类，使得相邻数据有很高的相似性，大幅提高数据压缩率，减少存储空间。</li>
<li><strong>提高查询性能</strong>：数据临近也让 RowGroup 数据范围比较小，加大了查询时 Data skip 效果，减少扫描的数据量，提升查询性能。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2022/07/17/Kyuubi-%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E6%B7%B7%E5%90%88%E8%AE%A1%E7%AE%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/07/17/Kyuubi-%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E6%B7%B7%E5%90%88%E8%AE%A1%E7%AE%97/" itemprop="url">Kyuubi 多数据源混合计算</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-07-17T11:44:43+08:00">
                2022-07-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Kyuubi-多数据源混合计算"><a href="#Kyuubi-多数据源混合计算" class="headerlink" title="Kyuubi 多数据源混合计算"></a>Kyuubi 多数据源混合计算</h2><p>Apache Kyuubi 作为一个分布式和多租户网关，用于在 Lakehouse 上提供 Serverless SQL。<br>借助于 Spark 引擎完整的生态，我们可以接入各种数据源，实现<strong>各种数据源的混合计算</strong>，以及<strong>不同数据源之间的数据同步</strong>。<br>第三方数据源通过实现 Spark DataSource 相关接口，提供 Spark Connector 依赖包来与 Spark 对接。Spark SQL 中可通过 <code>DataSource</code> 或 <code>Catalog</code> 的方式来定义数据源，进而通过 Spark SQL 读写数据源数据。</p>
<h3 id="DataSource"><a href="#DataSource" class="headerlink" title="DataSource"></a>DataSource</h3><p>Spark SQL 可使用 DataSource  配置，创建一个数据源的临时视图。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 从 JSON 文件创建一个临时视图</span><br><span class="line">CREATE TEMPORARY VIEW jsonTable</span><br><span class="line">USING org.apache.spark.sql.json</span><br><span class="line">OPTIONS (</span><br><span class="line">  path &#x27;examples/src/main/resources/people.json&#x27;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 查询定义的临时视图</span><br><span class="line">SELECT * FROM jsonTable</span><br></pre></td></tr></table></figure>

<h3 id="Catalog"><a href="#Catalog" class="headerlink" title="Catalog"></a>Catalog</h3><p>Spark 3 提供 CatalogPlugin 接口，可以为数据源实现自定义的 Catalog，通过相关配置加载数据源的 Catalog，进而访问该数据源的数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 定义一个名称为 test 的 catalog，catalog_class 为 CatalogPlugin 的实现类</span><br><span class="line">spark.sql.catalog.test  catalog_class</span><br><span class="line"></span><br><span class="line"># 配置 test catalog 的 options</span><br><span class="line">spark.sql.catalog.test.optKey1  optValue1</span><br><span class="line">spark.sql.catalog.test.optKey2  optValue2</span><br><span class="line"></span><br><span class="line"># 通过 `test.db.table` 访问自定义 catalog 下的库表</span><br><span class="line">select * from test.db.table</span><br></pre></td></tr></table></figure>

<h2 id="Spark-SQL-连接各种数据源"><a href="#Spark-SQL-连接各种数据源" class="headerlink" title="Spark SQL 连接各种数据源"></a>Spark SQL 连接各种数据源</h2><h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h3><p>Spark SQL 中默认只能加载一个 Hive 集群配置，对于有跨集群访问 Hive 的需求，我们可以使用文件类型的数据源，定义其它集群的 HDFS 路径，跨集群的操作 Hive 数据。</p>
<p>Spark 内置了一些文件类型的数据源，可以用于读写 HDFS 文件，如：Parquet、ORC、JSON、CSV 等，不需要额外的依赖。</p>
<p>对于开启了 Kerberos 认证的集群，需要使用 <code>kyuubi.credentials.hadoopfs.uris</code> 配置跨集群访问的 Hadoop FileSystems。不过目前这个参数是 Kyuubi Server 的参数，运行时不可修改，后续可以进优化。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kyuubi.credentials.hadoopfs.uris=hdfs://192.168.1.199</span><br></pre></td></tr></table></figure>

<p>通过 DataSource 方式访问 HDFS：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY VIEW parquetTable</span><br><span class="line">USING org.apache.spark.sql.parquet</span><br><span class="line">OPTIONS (</span><br><span class="line">  path &#x27;hdfs://192.168.1.199/user/hadoop/test/people.parquet&#x27;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">select * from parquetTable limit 10;</span><br></pre></td></tr></table></figure>

<h3 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h3><p>Spark 内置了 JDBC 的 DataSource 和 Catalog 实现，仅需要添加 JDBC Driver 依赖（我们 Spark jars 中添加了 MySQL Driver，所以不需要额外引入）。</p>
<p>通过 DataSource 方式访问 MySQL：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY VIEW jdbcTable</span><br><span class="line">USING org.apache.spark.sql.jdbc</span><br><span class="line">OPTIONS (</span><br><span class="line">  url &#x27;jdbc:mysql://192.168.1.199:3306/database&#x27;,</span><br><span class="line">  dbtable &#x27;database.tablename&#x27;,</span><br><span class="line">  user &#x27;username&#x27;,</span><br><span class="line">  password &#x27;password&#x27;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>通过 CatalogPlugin 方式访问 MySQL：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">set spark.sql.catalog.mysql=org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog;</span><br><span class="line">set spark.sql.catalog.mysql.url=jdbc:mysql://192.168.1.199/database;</span><br><span class="line">set spark.sql.catalog.mysql.user=username;</span><br><span class="line">set spark.sql.catalog.mysql.password=password;</span><br><span class="line"></span><br><span class="line">use database;</span><br><span class="line">select * from tablename;</span><br></pre></td></tr></table></figure>

<h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><p>使用 HBase Spark Connector 组件：<a target="_blank" rel="noopener" href="https://github.com/apache/hbase-connectors">hbase-connectors</a></p>
<p>HBase Shell 创建测试表：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">create &#x27;wangzhen_test&#x27;, &#x27;cf&#x27;</span><br><span class="line">put &#x27;wangzhen_test&#x27;, &#x27;row1&#x27;, &#x27;cf:a&#x27;, &#x27;value1&#x27;</span><br><span class="line">scan &#x27;wangzhen_test&#x27;</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://github.com/apache/hbase-connectors">hbase-connectors</a> 没有提供打入依赖的 shade 包，使用 maven-shade-plugin 插件制作完整的 shade 包便于添加 HBase Spark Connector 依赖，可以排除 Hadoop、Spark 等 provided 依赖包仅需要 HBase Client 相关依赖。<br>运行时可通过 <code>add jars</code> 命令加入依赖：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 添加 HBase Spark Connector 依赖</span><br><span class="line">add jars &quot;hdfs://XXX/user/hadoop/jars/hbase-spark-1.0.1-SNAPSHOT.jar&quot;;</span><br></pre></td></tr></table></figure>

<p>通过 DataSource 方式访问 HBase：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 创建 HBase 表临时视图：</span><br><span class="line">CREATE TEMPORARY VIEW hbaseTable</span><br><span class="line">USING org.apache.hadoop.hbase.spark</span><br><span class="line">OPTIONS (</span><br><span class="line">  &#x27;catalog&#x27; &#x27;&#123;&quot;table&quot;:&#123;&quot;namespace&quot;:&quot;default&quot;, &quot;name&quot;:&quot;wangzhen_test&quot;&#125;, &quot;rowkey&quot;:&quot;key1&quot;, &quot;columns&quot;:&#123;&quot;col1&quot;:&#123;&quot;cf&quot;:&quot;rowkey&quot;, &quot;col&quot;:&quot;key1&quot;, &quot;type&quot;:&quot;string&quot;&#125;, &quot;col2&quot;:&#123;&quot;cf&quot;:&quot;cf&quot;, &quot;col&quot;:&quot;a&quot;, &quot;type&quot;:&quot;string&quot;&#125;&#125;&#125;&#x27;,</span><br><span class="line">  &#x27;hbase.spark.config.location&#x27; &#x27;hbase-site.xml&#x27;,</span><br><span class="line">  &#x27;hbase.spark.use.hbasecontext&#x27; &#x27;false&#x27;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"># 读取 HBase 数据</span><br><span class="line">select * from hbaseTable;</span><br></pre></td></tr></table></figure>

<p>我们使用 Impersonation 模式运行 Kyuubi 服务，在 Kyuubi Server 中使用代理用户启动 Spark 引擎，Kyuubi Server 默认仅实现了获取 HDFS 和 Hive 的 Delegation Token。<br>我们 HBase 集群也开启了 Kerberos 认证，测试时通过指定 <code>spark.kerberos.keytab</code> 和 <code>spark.kerberos.principal</code> 方式运行 Spark Engine，后续需要在 Kyuubi Server 中实现 <code>HBaseDelegationTokenProvider</code> 来获取 HBase 的 Token。</p>
<h3 id="ClickHouse"><a href="#ClickHouse" class="headerlink" title="ClickHouse"></a>ClickHouse</h3><p>ClickHouse 提供了 JDBC 客户端，可以使用 Spark JDBC 的方式连接 ClickHouse。</p>
<p>另外开源的 <a target="_blank" rel="noopener" href="https://github.com/housepower/spark-clickhouse-connector">spark-clickhouse-connector</a> 组件，实现了 Spark ClickHouse Connector ，相对于 JDBC 的方式有更好的分区逻辑和更多的优化，对于读取操作应该有很好的性能提升，需要注意的是 spark-clickhouse-connector 是通过 GRPC 接口访问 ClickHouse 集群。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/housepower/spark-clickhouse-connector">spark-clickhouse-connector</a> 组件默认没有提供 Spark3.1的支持，我们通过复制 Spark3.3 实现的代码，简单的调整后测试可用。</p>
<p>使用 spark-clickhouse-connector 的 CatalogPlugin 方式访问 ClickHouse：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 添加 spark-clickhouse-connector 依赖</span><br><span class="line">add jars &#x27;hdfs://XXX/user/hadoop/jars/clickhouse-spark-runtime-3.1_2.12-0.4.0-SNAPSHOT.jar&#x27;;</span><br><span class="line"></span><br><span class="line"># 配置 clickhouse catalog</span><br><span class="line">set spark.sql.catalog.clickhouse=xenon.clickhouse.ClickHouseCatalog;</span><br><span class="line">set spark.sql.catalog.clickhouse.host=192.168.1.199;</span><br><span class="line">set spark.sql.catalog.clickhouse.grpc_port=9100;</span><br><span class="line">set spark.sql.catalog.clickhouse.user=username;</span><br><span class="line">set spark.sql.catalog.clickhouse.password=password;</span><br><span class="line">set spark.sql.catalog.clickhouse.database=database;</span><br><span class="line"></span><br><span class="line"># 查询 clickhouse 数据</span><br><span class="line">use clickhouse;</span><br><span class="line">select * from tablename;</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/" itemprop="url">Kyuubi 优化小文件</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-06-04T22:42:27+08:00">
                2022-06-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Spark-小文件问题"><a href="#Spark-小文件问题" class="headerlink" title="Spark 小文件问题"></a>Spark 小文件问题</h2><p>Hive 表中太多的小文件会影响数据的查询性能和效率，同时加大了  HDFS NameNode 的压力。Hive (on MapReduce) 一般可以简单的通过一些参数来控制小文件，而 Spark 中并没有提供小文件合并的功能。下面我们来简单了解一下 Spark 小文件问题，以及如何处理小文件。</p>
<h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>Kyuubi 版本：1.6.0-SNAPSHOT</p>
<p>Spark 版本：3.1.3、3.2.0</p>
<h3 id="TPCDS-数据集"><a href="#TPCDS-数据集" class="headerlink" title="TPCDS 数据集"></a>TPCDS 数据集</h3><p>Kyuubi 中提供了一个 TPCDS Spark Connector，可以通过配置 Catalog 的方式，在读取时自动生成 TPCDS 数据。</p>
<p>只需要将 <code>kyuubi-spark-connector-tpcds_2.12-1.6.0-SNAPSHOT.jar</code> 包放入 Spark jars 目录中，并配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.catalog.tpcds=org.apache.kyuubi.spark.connector.tpcds.TPCDSCatalog;</span><br></pre></td></tr></table></figure>

<p>这样我们就可以直接读取 TPCDS 数据集：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">use tpcds;</span><br><span class="line">show databases;</span><br><span class="line">use sf3000;</span><br><span class="line">show tables;</span><br><span class="line">select * from sf300.catalog_returns limit 10;</span><br></pre></td></tr></table></figure>

<h3 id="小文件产生"><a href="#小文件产生" class="headerlink" title="小文件产生"></a>小文件产生</h3><p>首先我们在 Hive 中创建一个 <code>sample.catalog_returns</code> 表，用于写入生成的 TPCDS <code>catalog_returns</code> 数据，并添加一个 <code>hash</code> 字段作为分区。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">create table sample.catalog_returns</span><br><span class="line">(</span><br><span class="line">    cr_returned_date_sk      bigint,</span><br><span class="line">    cr_returned_time_sk      bigint,</span><br><span class="line">    cr_item_sk               bigint,</span><br><span class="line">    cr_refunded_customer_sk  bigint,</span><br><span class="line">    cr_refunded_cdemo_sk     bigint,</span><br><span class="line">    cr_refunded_hdemo_sk     bigint,</span><br><span class="line">    cr_refunded_addr_sk      bigint,</span><br><span class="line">    cr_returning_customer_sk bigint,</span><br><span class="line">    cr_returning_cdemo_sk    bigint,</span><br><span class="line">    cr_returning_hdemo_sk    bigint,</span><br><span class="line">    cr_returning_addr_sk     bigint,</span><br><span class="line">    cr_call_center_sk        bigint,</span><br><span class="line">    cr_catalog_page_sk       bigint,</span><br><span class="line">    cr_ship_mode_sk          bigint,</span><br><span class="line">    cr_warehouse_sk          bigint,</span><br><span class="line">    cr_reason_sk             bigint,</span><br><span class="line">    cr_order_number          bigint,</span><br><span class="line">    cr_return_quantity       int,</span><br><span class="line">    cr_return_amount         decimal(7, 2),</span><br><span class="line">    cr_return_tax            decimal(7, 2),</span><br><span class="line">    cr_return_amt_inc_tax    decimal(7, 2),</span><br><span class="line">    cr_fee                   decimal(7, 2),</span><br><span class="line">    cr_return_ship_cost      decimal(7, 2),</span><br><span class="line">    cr_refunded_cash         decimal(7, 2),</span><br><span class="line">    cr_reversed_charge       decimal(7, 2),</span><br><span class="line">    cr_store_credit          decimal(7, 2),</span><br><span class="line">    cr_net_loss              decimal(7, 2)</span><br><span class="line">) PARTITIONED BY(hash int)</span><br><span class="line">stored as parquet;</span><br></pre></td></tr></table></figure>

<p>我们先关闭 Kyuubi 的优化，读取 catalog_returns 数据并写入 Hive：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=false;</span><br><span class="line"></span><br><span class="line">insert overwrite sample.catalog_returns partition (hash=0) select * from tpcds.sf300.catalog_returns;</span><br></pre></td></tr></table></figure>

<p><strong>Spark SQL 最终产生的文件数最多可能是最后一个写入的 Stage 的 Task 数乘以动态分区的数量。</strong>我们可以看到由于读取输入表的 Task 数是 44 个，所以最终产生了 44 个文件，每个文件大小约 69 M。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/1.png" class="">
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/2.png" class="">

<h3 id="改变分区数（Repartition）"><a href="#改变分区数（Repartition）" class="headerlink" title="改变分区数（Repartition）"></a>改变分区数（Repartition）</h3><p>由于写入的文件数跟最终写入 Stage 的 Task 数据有关，那么我们可以通过添加一个 Repartition 操作，来减少最终写入的 task 数，从而控制小文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite sample.catalog_returns partition (hash=0) select /*+ REPARTITION(10) */ * from tpcds.sf300.catalog_returns;</span><br></pre></td></tr></table></figure>

<p>添加 <code>REPARTITION(10)</code> 后，会在读取后做一个 Repartition 操作，将 partition 数变成 10，所以最终写入的文件数变成 10 个。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/3.png" class="">
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/4.png" class="">

<h3 id="Spark-AQE-自动合并小分区"><a href="#Spark-AQE-自动合并小分区" class="headerlink" title="Spark AQE 自动合并小分区"></a>Spark AQE 自动合并小分区</h3><p>Spark 3.0 以后引入了自适应查询优化（Adaptive Query Execution, AQE），可以自动合并较小的分区。</p>
<p>开启 AQE，并通过添加 <code>distribute by cast(rand() * 100 as int)</code> 触发 Shuffle 操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">set spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=false;</span><br><span class="line">set spark.sql.adaptive.enabled=true;</span><br><span class="line">set spark.sql.adaptive.advisoryPartitionSizeInBytes=512M;</span><br><span class="line">set spark.sql.adaptive.coalescePartitions.minPartitionNum=1;</span><br><span class="line"></span><br><span class="line">insert overwrite sample.catalog_returns partition (hash=0) select * from tpcds.sf300.catalog_returns distribute by cast(rand() * 100 as int);</span><br></pre></td></tr></table></figure>

<p>默认 Shuffle 分区数 <code>spark.sql.shuffle.partitions=200</code>，如果不开启 AQE 会产生 200 个小文件，开启 AQE 后，会自动合并小分区，根据 <code>spark.sql.adaptive.advisoryPartitionSizeInBytes=512M</code> 配置合并较小的分区，最终产生 <code>12</code> 个文件。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/5.png" class="">
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/6.png" class="">

<h2 id="Kyuubi-小文件优化分析"><a href="#Kyuubi-小文件优化分析" class="headerlink" title="Kyuubi 小文件优化分析"></a>Kyuubi 小文件优化分析</h2><p>Apache Kyuubi (Incubating) 作为增强版的 Spark Thrift Server 服务，可通过 Spark SQL 进行大规模的数据处理分析。Kyuubi 通过 Spark SQL Extensions 实现了很多的 Spark 优化，其中包括了 <code>RepartitionBeforeWrite</code> 的优化，再结合 Spark AQE 可以自动优化小文件问题，下面我们具体分析一下 Kyuubi 如何实现小文件优化。</p>
<h3 id="Kyuubi-如何优化小文件"><a href="#Kyuubi-如何优化小文件" class="headerlink" title="Kyuubi 如何优化小文件"></a>Kyuubi 如何优化小文件</h3><p>Kyuubi 提供了在写入前加上 <code>Repartition</code> 操作的优化，我们只需要将 <code>kyuubi-extension-spark-3-1_2.12-1.6.0-SNAPSHOT.jar</code> 放入 Spark jars 目录中，并配置 <code>spark.sql.extensions=org.apache.kyuubi.sql.KyuubiSparkSQLExtension</code>。相关配置：</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Default Value</th>
<th>Description</th>
<th>Since</th>
</tr>
</thead>
<tbody><tr>
<td>spark.sql.optimizer.insertRepartitionBeforeWrite.enabled</td>
<td>true</td>
<td>Add repartition node at the top of query plan. An approach of merging small files.</td>
<td>1.2.0</td>
</tr>
<tr>
<td>spark.sql.optimizer.insertRepartitionNum</td>
<td>none</td>
<td>The partition number if <code>spark.sql.optimizer.insertRepartitionBeforeWrite.enabled</code> is enabled. If AQE is disabled, the default value is <code>spark.sql.shuffle.partitions</code>. If AQE is enabled, the default value is none that means depend on AQE.</td>
<td>1.2.0</td>
</tr>
<tr>
<td>spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum</td>
<td>100</td>
<td>The partition number of each dynamic partition if <code>spark.sql.optimizer.insertRepartitionBeforeWrite.enabled</code> is enabled. We will repartition by dynamic partition columns to reduce the small file but that can cause data skew. This config is to extend the partition of dynamic partition column to avoid skew but may generate some small files.</td>
<td>1.2.0</td>
</tr>
</tbody></table>
<p>通过 <code>spark.sql.optimizer.insertRepartitionNum</code> 参数可以配置最终插入 Repartition 的分区数，当不开启 AQE，默认为 <code>spark.sql.shuffle.partitions</code> 的值。<strong>需要注意，当我们设置此配置会导致 AQE 失效，所以开启 AQE 不建议设置此值。</strong></p>
<p>对于动态分区写入，会根据动态分区字段进行 Repartition，并添加一个随机数来避免产生数据倾斜，<code>spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum</code> 用来配置随机数的范围，不过添加随机数后，由于加大了动态分区的基数，还是可能会导致小文件。这个操作类似在 SQL 中添加 <code>distribute by DYNAMIC_PARTITION_COLUMN, cast(rand() * 100 as int)</code>。</p>
<h3 id="静态分区写入"><a href="#静态分区写入" class="headerlink" title="静态分区写入"></a>静态分区写入</h3><p>开启 Kyuubi 优化和 AQE，测试静态分区写入：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">set spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=true;</span><br><span class="line"></span><br><span class="line">set spark.sql.adaptive.enabled=true;</span><br><span class="line">set spark.sql.adaptive.advisoryPartitionSizeInBytes=512M;</span><br><span class="line">set spark.sql.adaptive.coalescePartitions.minPartitionNum=1;</span><br><span class="line"></span><br><span class="line">insert overwrite sample.catalog_returns partition (hash=0) select * from tpcds.sf300.catalog_returns;</span><br></pre></td></tr></table></figure>

<p>可以看到 AQE 生效了，很好的控制了小文件，产生了 <code>11</code> 个文件，文件大小 <code>314.5 M</code> 左右。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/11.png" class="">
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/10.png" class="">

<h3 id="动态分区写入"><a href="#动态分区写入" class="headerlink" title="动态分区写入"></a>动态分区写入</h3><p>我们测试一下动态分区写入的情况，先关闭 Kyuubi 优化，并生成 10 个 hash 分区：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=false;</span><br><span class="line"></span><br><span class="line">insert overwrite sample.catalog_returns partition (hash) select *, cast(rand() * 10 as int) as hash from tpcds.sf300.catalog_returns;</span><br></pre></td></tr></table></figure>

<p>产生了 <code>44 × 10 = 440</code> 个文件，文件大小 <code>8 M</code> 左右。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/7.png" class="">

<p>开启 Kyuubi 优化和 AQE：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">set spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=true;</span><br><span class="line">set spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum=100;</span><br><span class="line"></span><br><span class="line">set spark.sql.adaptive.enabled=true;</span><br><span class="line">set spark.sql.adaptive.advisoryPartitionSizeInBytes=512M;</span><br><span class="line">set spark.sql.adaptive.coalescePartitions.minPartitionNum=1;</span><br><span class="line"></span><br><span class="line">insert overwrite sample.catalog_returns partition (hash) select *, cast(rand() * 10 as int) as hash from tpcds.sf300.catalog_returns;</span><br></pre></td></tr></table></figure>

<p>产生了 <code>12 × 10 = 120</code> 个文件，文件大小 <code>30 M</code> 左右，可以看到小文件有所改善，不过任然不够理想。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/8.png" class="">

<p>此案例中 hash 分区由 <code>rand</code> 函数产生，分布比较均匀，所以我们将 <code>spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum</code> 设置成 <code>0</code>，重新运行，同时将动态分区数设置为 <code>5</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">set spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=true;</span><br><span class="line">set spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum=0;</span><br><span class="line"></span><br><span class="line">set spark.sql.adaptive.enabled=true;</span><br><span class="line">set spark.sql.adaptive.advisoryPartitionSizeInBytes=512M;</span><br><span class="line">set spark.sql.adaptive.coalescePartitions.minPartitionNum=1;</span><br><span class="line"></span><br><span class="line">insert overwrite sample.catalog_returns partition (hash) select *, cast(rand() * 5 as int) as hash from tpcds.sf300.catalog_returns;</span><br></pre></td></tr></table></figure>

<p>由于动态分区数只有 <code>5</code> 个，所以实际上只有 <code>5</code> 个 Task 有数据写入，每个 Task 对应一个分区，导致最终每个分区只有一个较大的大文件。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/9.png" class="">

<p>通过上面的分析可以看到，对于动态分区写入，Repartition 的优化可以缓解小文件，配置 <code>spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum=100</code> 解决了数据倾斜问题，不过同时还是可能会有小文件。</p>
<h3 id="Rebalance-优化"><a href="#Rebalance-优化" class="headerlink" title="Rebalance 优化"></a>Rebalance 优化</h3><p>Spark 3.2+ 引入了 <code>Rebalance</code> 操作，借助于 Spark AQE 来平衡分区，进行小分区合并和倾斜分区拆分，避免分区数据过大或过小，能够很好的处理小文件问题。</p>
<p>Kyuubi 对于 Spark 3.2+ 的优化，是在写入前插入 Rebalance 操作，对于动态分区，则指定动态分区列进行 Rebalance 操作。不再需要 <code>spark.sql.optimizer.insertRepartitionNum</code> 和 <code>spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum</code> 配置。</p>
<p><strong>测试静态分区写入</strong>，使用 Spark 3.2.0 开启 Kyuubi 优化和 AQE：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">set spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=true;</span><br><span class="line"></span><br><span class="line">set spark.sql.adaptive.enabled=true;</span><br><span class="line">set spark.sql.adaptive.advisoryPartitionSizeInBytes=512M;</span><br><span class="line">set spark.sql.adaptive.coalescePartitions.minPartitionNum=1;</span><br><span class="line"></span><br><span class="line">insert overwrite sample.catalog_returns partition (hash=0) select * from tpcds.sf300.catalog_returns;</span><br></pre></td></tr></table></figure>

<p>Repartition 操作自动合并了小分区，产生了 <code>11</code> 个文件，文件大小 <code>334.6 M</code> 左右，解决了小文件的问题。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/15.png" class="">
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/14.png" class="">

<p><strong>测试动态分区写入</strong>，使用 Spark 3.2.0 开启 Kyuubi 优化和 AQE，生成 <code>5</code> 个动态分区：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">set spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=true;</span><br><span class="line"></span><br><span class="line">set spark.sql.adaptive.enabled=true;</span><br><span class="line">set spark.sql.adaptive.advisoryPartitionSizeInBytes=512M;</span><br><span class="line">set spark.sql.adaptive.coalescePartitions.minPartitionNum=1;</span><br><span class="line"></span><br><span class="line">insert overwrite sample.catalog_returns partition (hash) select *, cast(rand() * 5 as int) as hash from tpcds.sf300.catalog_returns;</span><br></pre></td></tr></table></figure>

<p>Repartition 操作自动拆分较大分区，产生了 <code>2 × 5 = 10</code> 个文件，文件大小 <code>311 M</code> 左右，很好的解决的倾斜问题。</p>
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/13.png" class="">
<img src="/2022/06/04/Kyuubi-%E4%BC%98%E5%8C%96%E5%B0%8F%E6%96%87%E4%BB%B6/12.png" class="">

<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>从上面的分析可以看到，对于 Spark 3.2+，Kyuubi 结合 Rebalance 能够很好的解决小文件问题，对于 Spark 3.1，Kyuubi 也能自动优化小文件，不过动态分区写入的情况还是可能存在问题。</p>
<p>相关的配置总结：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 配置 Kyuubi Spark SQL Extension</span><br><span class="line">spark.sql.extensions=org.apache.kyuubi.sql.KyuubiSparkSQLExtension</span><br><span class="line"></span><br><span class="line"># 开启 RepartitionBeforeWrite 优化（默认开启）</span><br><span class="line">spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=true;</span><br><span class="line"></span><br><span class="line"># 配置 AQE</span><br><span class="line">spark.sql.adaptive.enabled=true;</span><br><span class="line">spark.sql.adaptive.advisoryPartitionSizeInBytes=512M;</span><br><span class="line">spark.sql.adaptive.coalescePartitions.minPartitionNum=1;</span><br></pre></td></tr></table></figure>

<p>更多 AQE 配置可以参考：<a target="_blank" rel="noopener" href="https://kyuubi.apache.org/docs/latest/deployment/spark/aqe.html">How To Use Spark Adaptive Query Execution (AQE) in Kyuubi</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2021/11/23/Kyuubi-%E8%B0%83%E7%A0%94%E6%B5%8B%E8%AF%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/11/23/Kyuubi-%E8%B0%83%E7%A0%94%E6%B5%8B%E8%AF%95/" itemprop="url">Kyuubi 调研测试</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-11-23T14:31:47+08:00">
                2021-11-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>Kyuubi 是 Spark Thrift Server 增强版实现，实现 HiveServer2 协议，启动一个 Thrift 服务，通过 JDBC 方式接收 Spark SQL 请求并执行。我司通过提供 Kyuubi 服务， 实现 Hive SQL 到 Spark SQL 的迁移，同时提供 Ad-hoc 查询服务。</p>
<p>目前使用版本是 Kyuubi 0.7 版本，社区最新的 Kyuubi 1.4.0 版本带来了很大的架构优化，所以做如下的调研测试。</p>
<h2 id="二、测试环境部署"><a href="#二、测试环境部署" class="headerlink" title="二、测试环境部署"></a>二、测试环境部署</h2><h3 id="版本信息"><a href="#版本信息" class="headerlink" title="版本信息"></a>版本信息</h3><p>Kyuubi 版本：1.4.0 (未发布，master 分支)</p>
<p>Spark 版本：3.1.1</p>
<h3 id="打包编译"><a href="#打包编译" class="headerlink" title="打包编译"></a>打包编译</h3><p>拉取 Kyuubi 代码，并执行打包命令，完成后上传至服务器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./build/dist --tgz --spark-provided -Pkyuubi-extension-spark-3-1  # 打包</span><br></pre></td></tr></table></figure>

<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><ol>
<li><p>绑定IP和端口</p>
<p>配置 Kyuubi Server 服务 IP 和端口</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kyuubi.frontend.bind.host       0.0.0.0</span><br><span class="line">kyuubi.frontend.bind.port       10015</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置 KERBEROS 认证</p>
<p>Hadoop 集群开启了 Kerberos 认证，则配置 Kerberos 认证，并添加 Kerberos 相关配置，使用 hue 用户代理运行。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kyuubi.authentication   KERBEROS</span><br><span class="line">kyuubi.kinit.principal  hue/_HOST@***.COM</span><br><span class="line">kyuubi.kinit.keytab     /etc/kyuubi/conf/hue.keytab</span><br></pre></td></tr></table></figure>
</li>
<li><p>Zookeeper 配置</p>
<p> Kyuubi 依赖 Zookeeper 做服务发现和 HA，所以需要添加 Zookeeper 配置并使用了 DIGEST 认证。</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kyuubi.ha.zookeeper.quorum=192.168.1.100:2181</span><br><span class="line">kyuubi.ha.zookeeper.auth.type=DIGEST</span><br><span class="line">kyuubi.ha.zookeeper.auth.digest=hue:hue</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置 Namespace</p>
<p> 不同集群的 Kyuubi Server 使用同一个 Zookeeper 集群，配置不同 Namespace 隔离，后续连接时只需要指定 <code>zooKeeperNamespace</code> 访问不同集群。</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kyuubi.ha.zookeeper.namespace=kyuubi_cluster001</span><br><span class="line">kyuubi.session.engine.initialize.timeout=180000</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="三、功能调研"><a href="#三、功能调研" class="headerlink" title="三、功能调研"></a>三、功能调研</h2><h3 id="Beeline-连接-Kyuubi"><a href="#Beeline-连接-Kyuubi" class="headerlink" title="Beeline 连接 Kyuubi"></a>Beeline 连接 Kyuubi</h3><p>使用 beeline 工具连接 Kyuubi 进行测试，Kyuubi JDBC 链接包括了以下配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># zookeeper 连接</span><br><span class="line">ZOOKEEPER_QUORUM=192.168.1.100:2181</span><br><span class="line"># 集群名称</span><br><span class="line">CLUSTER=cluster001</span><br><span class="line"># spark 配置</span><br><span class="line">SPARKI_CONFS=spark.executor.instances=10;spark.executor.memory=3g</span><br><span class="line"># kyuubi 配置</span><br><span class="line">KYUUBI_CONFS=$SPARKI_CONFS;kyuubi.engine.share.level=CONNECTION</span><br><span class="line"></span><br><span class="line"># jdbc 链接</span><br><span class="line">JBDC_URL=&quot;jdbc:hive2://$&#123;ZOOKEEPER_QUORUM&#125;/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=kyuubi_$CLUSTER?$KYUUBI_CONFS&quot;</span><br><span class="line"></span><br><span class="line"># 使用 beeline 连接</span><br><span class="line">beeline -u &quot;$JBDC_URL&quot;</span><br></pre></td></tr></table></figure>

<p>相关说明：</p>
<ul>
<li>由于 JDBC 链接会有分号等字符，beeline 连接时 JDBC 链接需要带上引号。</li>
<li>Hive 和 Spark 中都有 beeline 命令，可能与 Kyuubi Server 存在兼容性问题，需要使用合适的 beeline 路径。</li>
</ul>
<h3 id="共享引擎策略"><a href="#共享引擎策略" class="headerlink" title="共享引擎策略"></a>共享引擎策略</h3><ol>
<li>引擎共享策略<br>Kyuubi 支持共享引擎，可通 <code>kyuubi.engine.share.level</code> 配置不同共享级别，共享级别定义如下：<ul>
<li>CONNECTION：连接级别，引擎适用于一次 jdbc connection，不做其他共享，此配置适用于离线 ETL 任务，使得不同任务之间相互隔离。</li>
<li>USER：用户级别共享，引擎可以在同一个用户的不同连接进行共享，适用于 AdHoc 查询和较小的任务，可以节省资源，并在有可用引擎时支持快速响应。</li>
<li>SERVER：服务级别共享（全局共享），引擎可以全局共享，所有连接可以共享一个引擎，不过启动引擎的用户需要具有较高权限才能满足访问不同用户的表。</li>
</ul>
</li>
<li>引擎使用单个 SparkSession<br>默认情况下，共享引擎对于新的 Connection 连接，使用的新的 SparkSession，不同连接共享 SparkContext 的资源，不过一些 session 级别的参数、函数、临时表等都是隔离开的。可以通过<code>kyuubi.engine.single.spark.session</code>参数，使用全局的 SparkSession，使得不同连接可以共享 Session 状态，包括参数、函数、临时表等。</li>
<li>引擎 TTL<br> 对于共享引擎，多个连接共享使用，并不由某个连接单独管理，在某个连接关闭后引擎不会马上退出，而是在引擎空闲的时间超过配置的超时时间后自动退出，通过<code>kyuubi.session.engine.idle.timeout</code> 参数进行配置。</li>
</ol>
<p>对于共享引擎，官方公众号有更详细介绍：<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/gi8w2oCsXqAiXn9blv9BBw">Apache Kyuubi：灵活运用引擎隔离共享，加速即席查询，支持大规模 ETL</a></p>
<h3 id="用户默认配置"><a href="#用户默认配置" class="headerlink" title="用户默认配置"></a>用户默认配置</h3><p>Kyuubi 支持用户级别的默认配置，可以为不同用户配置不同的参数，详见：<a target="_blank" rel="noopener" href="https://kyuubi.apache.org/docs/latest/deployment/settings.html#user-defaults">Settings: User Defaults</a></p>
<p>下面示例，给 user1 和 user2 设置了不同的队列和动态资源最大 Executor 数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># For system defaults</span><br><span class="line">spark.master=yarn;</span><br><span class="line">spark.submit.deployMode=cluster</span><br><span class="line">spark.dynamicAllocation.enabled=true;</span><br><span class="line"></span><br><span class="line"># user1</span><br><span class="line">__user1__.spark.yarn.queue=root.queue1;</span><br><span class="line">__user2__.spark.dynamicAllocation.maxExecutors=200;</span><br><span class="line"></span><br><span class="line"># user2</span><br><span class="line">__user2__.spark.yarn.queue=root.queue2;</span><br><span class="line">__user2__.spark.dynamicAllocation.maxExecutors=500;</span><br></pre></td></tr></table></figure>

<h3 id="Kyuubi-Spark-SQL-Extensions"><a href="#Kyuubi-Spark-SQL-Extensions" class="headerlink" title="Kyuubi Spark SQL Extensions"></a>Kyuubi Spark SQL Extensions</h3><p>Kyuubi 中实现了一些 Spark SQL 的优化，可通过 <code>spark.sql.extensions=org.apache.kyuubi.sql.KyuubiSparkSQLExtension</code> 配置开启，具体：<a target="_blank" rel="noopener" href="https://kyuubi.apache.org/docs/latest/sql/rules.html">Auxiliary SQL extension for Spark SQL</a>。</p>
<p><strong>解决小文件问题：</strong><br>KyuubiSparkSQLExtension 中定义了 <code>RepartitionBeforeWritingHive</code> 和 <code>RepartitionBeforeWritingDatasource</code> 规则，在写入 Hive 或 DataSource 前插入 Repartition 操作，来控制写入的分区数，可通过 <code>spark.sql.optimizer.insertRepartitionNum</code> 参数配置 Repartition 操作的分区数。<br>对于动态分区写入，加了一个随机数来解决 Repartition 可能带来的数据倾斜的问题，不过可能会导致小文件，通过 <code>spark.sql.optimizer.dynamicPartitionInsertionRepartitionNum</code> 配置可设置动态分区 Repartition 操作插入的随机分区数。</p>
<img src="/2021/11/23/Kyuubi-%E8%B0%83%E7%A0%94%E6%B5%8B%E8%AF%95/repartition.png" class="" title="[repartition]">

<h3 id="Kyuubi-Metrics"><a href="#Kyuubi-Metrics" class="headerlink" title="Kyuubi Metrics"></a>Kyuubi Metrics</h3><p>Kyuubi Server 中也定义了一些监控指标，用于监控 Kyuubi Server 的运行状况，支持了很多的 Reporter，包括 Prometheus，后续工作需要将指标投递到 Prometheus 中，对 Kyuubi 服务进行监控告警。具体参考：<a target="_blank" rel="noopener" href="https://kyuubi.apache.org/docs/latest/monitor/metrics.html"> Kyuubi Server Metrics</a>。</p>
<h3 id="Kyuubi-Ctl"><a href="#Kyuubi-Ctl" class="headerlink" title="Kyuubi Ctl"></a>Kyuubi Ctl</h3><p>Kyuubi 的 bin 目录中提供了 <code>kyuubi-ctl</code> 工具，目前主要用于维护 Server 和 Engine 实例的状态，可以获取和删除 Server 和 Engine 在 Zookeeper 上的注册信息。</p>
<p>目前包括了，下面一些命令，可执行 <code>bin/kyuubi-ctl --help</code> 获取完整帮助信息。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Command: get [server|engine] [options]</span><br><span class="line">	Get the service/engine node info, host and port needed.</span><br><span class="line">Command: delete [server|engine] [options]</span><br><span class="line">	Delete the specified service/engine node, host and port needed.</span><br><span class="line">Command: list [server|engine] [options]</span><br><span class="line">	List all the service/engine nodes for a particular domain.</span><br></pre></td></tr></table></figure>

<p>后续在服务做灰度升级时，可通过 <code>kyuubi-ctl </code> 命令，先下线 KyuubiServer 注册信息，切断 KyuubiServer 流量，等一段时间后该 KyuubiServer 上连接都关闭后，下线该服务。</p>
<h3 id="后续规划"><a href="#后续规划" class="headerlink" title="后续规划"></a>后续规划</h3><ol>
<li><p>共享策略</p>
<ul>
<li><strong>离线 SQL</strong>：对于离线 SQL 为了保证任务稳定性，不使用共享引擎，保证任务进行完全隔离不相互影响。</li>
<li><strong>Adhoc 任务</strong>：使用 User 级别共享，加大 TTL 时间，让引擎尽量常驻，使得 Adhoc 查询能够及时响应；需要考虑 Spark 调度策略，防止资源抢占导致响应慢。</li>
</ul>
</li>
<li><p>配置管理<br>目前考虑将配置交由上游系统管理，根据标签设置不同配置，任务提交时带上相应的标签即可。</p>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2021/07/28/Spark-Yarn-ApplicationMaster-%E8%B6%85%E6%97%B6%E9%80%80%E5%87%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/07/28/Spark-Yarn-ApplicationMaster-%E8%B6%85%E6%97%B6%E9%80%80%E5%87%BA/" itemprop="url">Spark Yarn ApplicationMaster 超时退出</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-07-28T12:00:00+08:00">
                2021-07-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="问题说明"><a href="#问题说明" class="headerlink" title="问题说明"></a>问题说明</h3><p>Spark 应用运行失败，查看 Spark UI 界面，发现任务执行正常，没有失败的 Job 和 Task。</p>
<h3 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h3><h4 id="1-查看-Yarn-ResourceManger-的日志"><a href="#1-查看-Yarn-ResourceManger-的日志" class="headerlink" title="1. 查看 Yarn ResourceManger 的日志"></a>1. 查看 Yarn ResourceManger 的日志</h4><p>从 Spark UI 界面中没有看到失败的 Job 和 Task，查看 Driver 日志看到 <code>ApplicationMaster: RECEIVED SIGNAL TERM</code> 信息，怀疑 Spark 应用被 kill 了，所以查看了 Yarn ResourceManger  日志，可以看到 Yarn 判断 ApplicationMaster 心跳超时导致，发送 TERM 信号。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Expired:appattempt_1623809535012_3914039_000001 Timed out after 90 secs</span><br></pre></td></tr></table></figure>

<h4 id="2-分析-Spark-ApplicationMaster-代码"><a href="#2-分析-Spark-ApplicationMaster-代码" class="headerlink" title="2. 分析 Spark ApplicationMaster 代码"></a>2. 分析 Spark ApplicationMaster 代码</h4><p><code>ApplicationMaster</code> 代码中启动 <code>Reporter </code> 线程调用 <code>org.apache.spark.deploy.yarn.ApplicationMaster#allocationThreadImpl</code> 方法，通过 while 循环向 Yarn 发送心跳。</p>
<p>怀疑是不是 Reporter 线程挂掉了，查看代码可确定日志中 <code>Reporter</code> 关键字，在 Driver 日志中并未发现异常退出信息。后直接在日志中查找 <code>ApplicationMaster </code> 关键字，发现如下信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">21/07/26 16:00:21 INFO ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0</span><br><span class="line"></span><br><span class="line">21/07/26 16:00:21 INFO SparkContext: Invoking stop() from shutdown hook</span><br><span class="line"></span><br><span class="line">........</span><br><span class="line"></span><br><span class="line">21/07/26 16:02:02 ERROR ApplicationMaster: RECEIVED SIGNAL TERM</span><br></pre></td></tr></table></figure>

<p>可以看到先打印了 <code>SUCCEEDED</code> 状态，刚好大约 90s 后接收到 <code>TERM</code> 信号。</p>
<p>查看 ApplicationMaster 代码，在用户包的 <code>main</code> 方法执行完成后，改变为成功状态，并设置 finished&#x3D;true，从而导致 Reporter 线程正常退出。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">val userThread = new Thread &#123;</span><br><span class="line">      override def run() &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">          if (!Modifier.isStatic(mainMethod.getModifiers)) &#123;</span><br><span class="line">            logError(s&quot;Could not find static main method in object $&#123;args.userClass&#125;&quot;)</span><br><span class="line">            finish(FinalApplicationStatus.FAILED, ApplicationMaster.EXIT_EXCEPTION_USER_CLASS)</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">            mainMethod.invoke(null, userArgs.toArray)</span><br><span class="line">            finish(FinalApplicationStatus.SUCCEEDED, ApplicationMaster.EXIT_SUCCESS)</span><br><span class="line">            logDebug(&quot;Done running user class&quot;)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; catch &#123;</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>成功状态后进程并未退出，可以看到 SparkContext.stop() 方法在 main 方法执行完成后在执行的，由于 SparkContext 中添加 ShutdownHook 执行 stop() 方法。</p>
<h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p>通过上面分析得出结论，由于在执行完用户包的 mian 方法后，Reporter 线程退出，执行 SparkContext.stop() ，并且 SparkContext.stop() 方法执行时间超过 90s (yarn.am.liveness-monitor.expiry-interval-ms)，导致 Yarn 认为 ApplicationMaster 超时，向 ApplicationMaster  发送 TERM 信号，停止任务。</p>
<p>根据以上结论，让用户在程序结束时显示调用 SparkContext.stop()  解决问题。</p>
<h3 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h3><p>继续排查 SparkContext.stop 慢的原因，发现大量 AppStatusListener 异常信息，怀疑是 AppStatusListener 接收到了太多的事件，导致处理过慢。</p>
<p>查看 SparkUI 界面，发现 Stage 的 task 数量很多，并且执行时间都非常短，查看用户配置，发现 <code>spark.sql.shuffle.partitions</code> 参数设置太大，用户修改后程序正常。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2021/07/02/Kyuubi-%E6%9E%B6%E6%9E%84%E5%AF%B9%E6%AF%94/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/07/02/Kyuubi-%E6%9E%B6%E6%9E%84%E5%AF%B9%E6%AF%94/" itemprop="url">Kyuubi 架构对比</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-07-02T15:37:20+08:00">
                2021-07-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>提供 JDBC 服务，迁移 Hive SQL 。</p>
<p>提供 Ad-Hoc 查询能力。</p>
<h3 id="Spark-Thrift-Server"><a href="#Spark-Thrift-Server" class="headerlink" title="Spark Thrift Server"></a>Spark Thrift Server</h3><p>Spark Thrift Server 是 Spark 社区基于 HiveServer2 协议实现的 Thrift 服务，提供 Spark SQL JDBC 服务。由于 Spark 进程常驻，提交请求后立即执行计算，响应迅速，可用于 Ad-Hoc 查询。</p>
<h4 id="Spark-Thrift-Server-架构"><a href="#Spark-Thrift-Server-架构" class="headerlink" title="Spark Thrift Server 架构"></a>Spark Thrift Server 架构</h4><p>Spark Thrift Server 是基于 SparkContext 多线程应用场景的实现。</p>
<p>Spark 中一次 Action 算子对应一个 Job，提交一个 Job 后会等待执行完成，所以在单线程的场景下，Job 是顺序执行的。SparkContext 通过多线程提交 Job 时，不同的 Job 可并发提交执行。</p>
<p>下图是 Spark Thrift Server 的架构。Spark Thrift Server 在启动时初始化一个 SparkContext。接受请求后，会新建或者复用 SparkSession, 并通过 SparkSession.sql() 进行执行，由于处理请求的线程不同，所以 SparkSession.sql() 提交的任务可以并发执行。</p>
<img src="/2021/07/02/Kyuubi-%E6%9E%B6%E6%9E%84%E5%AF%B9%E6%AF%94/sts.png" class="" title="SparkThriftServer">

<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>架构简单，方便部署运维。</li>
<li>启动后常驻 Spark 相关服务，响应快。</li>
<li>统一服务，隐藏集群配置，方便统一管理和优化。</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>单用户运行，不支持多用户访问</li>
<li>资源不隔离</li>
<li>不支持高可用</li>
</ul>
<h3 id="Kyuubi-0-7-0"><a href="#Kyuubi-0-7-0" class="headerlink" title="Kyuubi 0.7.0"></a>Kyuubi 0.7.0</h3><p>Kyuubi 是网易开源的增强版 Spark Thrift Server 实现。支持多租户、资源隔离、高可用等，使得 Spark Thrift Server 服务具有更好的可用性和稳定性。</p>
<h4 id="Kyuubi-0-7-0-架构"><a href="#Kyuubi-0-7-0-架构" class="headerlink" title="Kyuubi 0.7.0 架构"></a>Kyuubi 0.7.0 架构</h4><p>Kyuubi 0.7.0 版本，实现用户 Session 级别的 SparkContext 的初始化、注册、缓存、回收机制。</p>
<p>Kyuubi 接收到请求后可根据不同用户创建多个 SparkContext，启动多个  Spark 实例，从而实现多租户和资源隔离。</p>
<h4 id="Kyuubi-0-7-0-高可用"><a href="#Kyuubi-0-7-0-高可用" class="headerlink" title="Kyuubi 0.7.0 高可用"></a>Kyuubi 0.7.0 高可用</h4><p>Kyuubi 0.7.0 版本，支持 Load Balance Mode 、Active&#x2F;Standby Failover 两种模式的高可用，通过 Zookeeper 作服务发现。</p>
<p><strong>Load Balance Mode</strong>：负载均衡模式下，所有的 Kyuubi 服务都是活跃状态，这种模式下可以减轻 Kyuubi Server 的负载，提高服务的并发。不过会加大 Yarn 集群的负载，可能导致一个用户连接在不同的 Kyuubi Server 上，启动多个 SparkContext，造成集群资源浪费。</p>
<p><strong>Active&#x2F;Standby Failover</strong>：主备故障切换模式下，只有一个几点是 Active 的状态对外提供服务，当 Active 节点发生故障时，Standby 节点选举成功后变成 Active 状态对外提供服务，从而达到高可用，不过此模式不会加大整体并发能力。</p>
<img src="/2021/07/02/Kyuubi-%E6%9E%B6%E6%9E%84%E5%AF%B9%E6%AF%94/kyuubi_architecture_0_X.png" class="" title="kyuubi_architecture_0_X">

<h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><ul>
<li>支持多租户</li>
<li>支持代理用户（hive.server2.proxy.user）</li>
<li>支持 Session 级别配置</li>
<li>Executors 资源隔离</li>
<li>支持高可用</li>
</ul>
<h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>只支持 Yarn-Client 模式，共用 Driver 进程，可能成为服务瓶颈</li>
<li>高可用模式存在缺陷</li>
</ul>
<h3 id="Kyuubi-1-3-0"><a href="#Kyuubi-1-3-0" class="headerlink" title="Kyuubi 1.3.0"></a>Kyuubi 1.3.0</h3><p>Kyuubi 1.X 版本中，将 Kyuubi Server 和 SparkContext 进行解耦。引入 Spark SQL Engine 进行 SparkContext 初始化和 SQL 执行，在 Kyuubi Server 中通过 spark-submit 命令启动，启动后将自身状态保持在 Zookeeper 中。Kyuubi Server 接收到请求后通过 Zookeeper 寻找可用引擎或启动新引擎进行处理。</p>
<p>Spark SQL Engine 作为独立的 Spark 应用，可以以不同的方式执行，并支持 yarn-cluster 模式。状态持续在 Zookeeper 中，使得 Kyuubi Server 之间可以共用 Engine，提高了 Kyuubi Server 的扩展能力。</p>
<img src="/2021/07/02/Kyuubi-%E6%9E%B6%E6%9E%84%E5%AF%B9%E6%AF%94/kyuubi_architecture_new.png" class="" title="kyuubi_architecture_new">

<h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4><ul>
<li>支持多租户</li>
<li>支持资源隔离</li>
<li>支持不同级别的引擎共享策略（CONNECTION, USER, SERVER）</li>
<li>支持高可用</li>
</ul>
<h4 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>多一层服务，加大了服务的复杂性</li>
<li>增加依赖 Zookeeper，作为服务端和引擎端的服务发现</li>
</ul>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/sql-distributed-sql-engine.html">Distributed SQL Engine</a></li>
<li><a target="_blank" rel="noopener" href="https://kyuubi.readthedocs.io/en/latest/overview/kyuubi_vs_thriftserver.html">Kyuubi v.s. Spark Thrift JDBC&#x2F;ODBC Server (STS)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NetEase/kyuubi/blob/branch-0.7/docs/architecture.md">Kyuubi 0.7.0 Architecture</a></li>
<li><a target="_blank" rel="noopener" href="https://kyuubi.readthedocs.io/en/latest/overview/architecture.html">Kyuubi 1.X Architecture</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2021/06/10/Spark-%E8%B7%A8%E9%9B%86%E7%BE%A4%E8%AF%BB%E5%86%99-Iceberg/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/06/10/Spark-%E8%B7%A8%E9%9B%86%E7%BE%A4%E8%AF%BB%E5%86%99-Iceberg/" itemprop="url">Spark 跨集群读写 Iceberg</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-06-10T12:34:23+08:00">
                2021-06-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a>运行环境</h3><p>Spark: 2.4.3<br>Iceberg: 1.2.0</p>
<h3 id="IcebergSource"><a href="#IcebergSource" class="headerlink" title="IcebergSource"></a>IcebergSource</h3><p>Iceberg 提供 <code>DataSourceRegister</code> 接口的实现类 <code>org.apache.iceberg.spark.source.IcebergSource</code>，在 IcebergSource 中实现了 <code>createReader</code>、<code>createWriter</code>等方法进行 DataFrame 的读写。</p>
<p>IcebergSource 中的  <code>findTable</code> 方法是从 Catalog 中获取 Iceberg Table，此方法进一步通过 CustomCatalogs.table 获取 table， 在 CustomCatalogs 中 buildCatalog 是通过 <code>spark.sessionState().newHadoopConf</code> 获取 Hadoop、Hive 相关配置，那么默认的行为将没有办法进行多集群间的读写。</p>
<h3 id="自定义-IcebergSource"><a href="#自定义-IcebergSource" class="headerlink" title="自定义 IcebergSource"></a>自定义 IcebergSource</h3><p>IcebergSource 的 <code>findTable</code> 方法是 <code>protected</code> 的，可以从 IcebergSource 派生出自定义的 IcebergSource，在自定义的 IcebergSource 中维护多个集群的 Catalog，覆盖 <code>findTable</code> 方法从对应集群的 Catalog 中获取 Iceberg Table 对象，进而可实现多集群间的读写。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClusterIcebergSource</span> <span class="keyword">extends</span> <span class="title">IcebergSource</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">shortName</span></span>(): <span class="type">String</span> = <span class="type">SHORT_NAME</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">findTable</span></span>(options: <span class="type">DataSourceOptions</span>, conf: <span class="type">Configuration</span>): <span class="type">Table</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> path = options.get(<span class="string">&quot;path&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> cluster = options.get(<span class="string">&quot;cluster&quot;</span>)</span><br><span class="line">    <span class="type">Preconditions</span>.checkArgument(path.isPresent, <span class="string">&quot;Cannot open table: path is not set&quot;</span>.asInstanceOf[<span class="type">Object</span>])</span><br><span class="line">    <span class="type">Preconditions</span>.checkArgument(cluster.isPresent, <span class="string">&quot;Cannot open table: cluster is not set&quot;</span>.asInstanceOf[<span class="type">Object</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> catalog = loadClusterCatalog(cluster.get())</span><br><span class="line">    catalog.loadTable(tableIdentifier(path.get()))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ClusterIcebergSource</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> <span class="type">SHORT_NAME</span> = <span class="string">&quot;iceberg-cluster&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> catalogs: util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Catalog</span>] = <span class="keyword">new</span> <span class="type">ConcurrentHashMap</span>[<span class="type">String</span>, <span class="type">Catalog</span>]()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">loadClusterCatalog</span></span>(cluster: <span class="type">String</span>): <span class="type">Catalog</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!catalogs.containsKey(cluster)) catalogs synchronized &#123;</span><br><span class="line">      <span class="keyword">if</span> (!catalogs.containsKey(cluster)) &#123;</span><br><span class="line">        <span class="keyword">val</span> hiveCatalog = <span class="keyword">new</span> <span class="type">HiveCatalog</span>()</span><br><span class="line">        hiveCatalog.setConf(hadoopConf(cluster))</span><br><span class="line">        <span class="keyword">val</span> properties = <span class="keyword">new</span> util.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>]()</span><br><span class="line">        hiveCatalog.initialize(<span class="string">s&quot;iceberg_catalog_<span class="subst">$cluster</span>&quot;</span>, properties)</span><br><span class="line">        catalogs.put(cluster, hiveCatalog)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    catalogs.get(cluster)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">hadoopConf</span></span>(cluster: <span class="type">String</span>): <span class="type">Configuration</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO load cluster hadoop conf</span></span><br><span class="line">    <span class="literal">null</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">tableIdentifier</span></span>(path: <span class="type">String</span>): <span class="type">TableIdentifier</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> nameParts = path.split(<span class="string">&quot;\\.&quot;</span>)</span><br><span class="line">    <span class="type">TableIdentifier</span>.of(nameParts(<span class="number">0</span>), nameParts(<span class="number">1</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="注册自定义-IcebergSource"><a href="#注册自定义-IcebergSource" class="headerlink" title="注册自定义 IcebergSource"></a>注册自定义 IcebergSource</h3><p>添加 <code>META-INF/services/org.apache.spark.sql.sources.DataSourceRegister</code> 文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">com.***.ClusterIcebergSource</span><br></pre></td></tr></table></figure>

<h3 id="读写-Iceberg-表"><a href="#读写-Iceberg-表" class="headerlink" title="读写 Iceberg 表"></a>读写 Iceberg 表</h3><h4 id="Read-Iceberg-Table"><a href="#Read-Iceberg-Table" class="headerlink" title="Read Iceberg Table"></a>Read Iceberg Table</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.read.option(<span class="string">&quot;cluster&quot;</span>, cluster)</span><br><span class="line">  .format(<span class="type">ClusterIcebergSource</span>.<span class="type">SHORT_NAME</span>)</span><br><span class="line">  .load(<span class="string">s&quot;<span class="subst">$database</span>.<span class="subst">$table</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="Write-DataFrame"><a href="#Write-DataFrame" class="headerlink" title="Write DataFrame"></a>Write DataFrame</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df.write</span><br><span class="line">  .option(<span class="string">&quot;cluster&quot;</span>, cluster)</span><br><span class="line">  .option(<span class="type">SparkWriteOptions</span>.<span class="type">FANOUT_ENABLED</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .format(<span class="type">ClusterIcebergSource</span>.<span class="type">SHORT_NAME</span>)</span><br><span class="line">  .mode(<span class="string">&quot;append&quot;</span>)</span><br><span class="line">  .save(<span class="string">s&quot;<span class="subst">$database</span>.<span class="subst">$table</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="遇到问题"><a href="#遇到问题" class="headerlink" title="遇到问题"></a>遇到问题</h3><ol>
<li>loadTable 时 HiveMetaStore 初始化报错，<code>No suitable driver found for jdbc:mysql:***</code></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">java.sql.SQLException: No suitable driver found for jdbc:mysql://***:***/hive_db?createDatabaseIfNotExist=true</span><br><span class="line">	at java.sql.DriverManager.getConnection(DriverManager.java:689)</span><br><span class="line">	at java.sql.DriverManager.getConnection(DriverManager.java:208)</span><br><span class="line">	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:349)</span><br><span class="line">	at com.jolbox.bonecp.BoneCP.&lt;init&gt;(BoneCP.java:416)</span><br><span class="line">	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)</span><br><span class="line">	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)</span><br><span class="line">	at org.datanucleus.store.rdbms.RDBMSStoreManager.&lt;init&gt;(RDBMSStoreManager.java:298)</span><br></pre></td></tr></table></figure>

<p>解决：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DriverRegistry.register(&quot;com.mysql.jdbc.Driver&quot;)</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2021/03/08/JanusGraph-OLAP-Traversals-with-Spark-On-Yarn-Client-Mode/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/03/08/JanusGraph-OLAP-Traversals-with-Spark-On-Yarn-Client-Mode/" itemprop="url">JanusGraph: OLAP Traversals with Spark On Yarn-Client Mode</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-03-08T15:11:49+08:00">
                2021-03-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h3><p>JanusGraph 支持使用 Spark 进行一些 OLAP 计算。官方文档中只给出了 Spark Local 模式和 Spark Standalone Cluster 模式的例子。参考：<a target="_blank" rel="noopener" href="https://docs.janusgraph.org/advanced-topics/hadoop/">JanusGraph with TinkerPop’s Hadoop-Gremlin</a></p>
<p>由于我们的大部分 Spark 是运行在 Yarn 上面的，尝试通过 Gremlin Console 使用 Spark On Yarn-Client 模型运行 JanusGraph 的 OLAP 任务。</p>
<p>JanusGraph 我们是用于 Atlas 的存储，也做一些 Atlas 的适配。</p>
<p>版本信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JanusGraph: 0.5.1</span><br><span class="line">Spark: 2.4.0</span><br><span class="line">Hadoop: 2.7.7</span><br><span class="line">Atlas 2.1.0</span><br></pre></td></tr></table></figure>

<h3 id="环境适配"><a href="#环境适配" class="headerlink" title="环境适配"></a>环境适配</h3><h4 id="打包安装"><a href="#打包安装" class="headerlink" title="打包安装"></a>打包安装</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mvn clean install -Pjanusgraph-release -Dgpg.skip=true -DskipTests=true</span><br><span class="line"></span><br><span class="line"># 安装包</span><br><span class="line">janusgraph-dist/target/janusgraph-0.5.1.zip</span><br></pre></td></tr></table></figure>

<h4 id="依赖整理"><a href="#依赖整理" class="headerlink" title="依赖整理"></a>依赖整理</h4><p>由于 Janusgraph-Hadoop 并没有引入 spark-yarn 和  相关 jar 包。所以想要支持 Spark On Yarn 模式运行必须要加入相关的依赖。<br>直接下载 Spark 2.4.0 的安装包，将 jars 里面的 jar 包放在 ${JANUSGRAPH_HOME}&#x2F;ext&#x2F;spark&#x2F;jars 中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz</span><br><span class="line">tar xzvf spark-2.4.0-bin-hadoop2.7.tgz</span><br><span class="line">cp $&#123;SPARK_HOME&#125;/jars/* $&#123;JANUSGRAPH_HOME&#125;/ext/spark/jars</span><br><span class="line"></span><br><span class="line"># 去掉冲突 Jar 包</span><br><span class="line">rm -f $&#123;JANUSGRAPH_HOME&#125;/ext/spark/jars/guava-14.0.1.jar</span><br></pre></td></tr></table></figure>

<p>需要支持 Atlas，顾也加上 Atlas 相关 Jar 包。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cp $&#123;ATLAS_HOME&#125;/server/webapp/atlas/WEB-INF/lib/* $&#123;JANUSGRAPH_HOME&#125;/ext/atlas/jars</span><br><span class="line"></span><br><span class="line"># 去掉冲突 Jar 包</span><br><span class="line">rm -f $&#123;JANUSGRAPH_HOME&#125;/ext/atlas/jars/atlas-webapp-2.1.0.jar</span><br><span class="line">rm -f $&#123;JANUSGRAPH_HOME&#125;/ext/atlas/jars/netty-3.10.5.Final.jar</span><br><span class="line">rm -f $&#123;JANUSGRAPH_HOME&#125;/ext/atlas/jars/netty-all-4.0.52.Final.jar</span><br></pre></td></tr></table></figure>

<h4 id="启动脚本"><a href="#启动脚本" class="headerlink" title="启动脚本"></a>启动脚本</h4><p>编辑 gremlin.sh，做了如下配置：</p>
<ul>
<li>配置 SPARK_HOME</li>
<li>配置 HADOOP_CONF_DIR 和 HBASE_CONF_DIR 并加入 CLASSPATH</li>
<li>配置 HADOOP_GREMLIN_LIBS，通过 SparkContext addJar 方式引入依赖。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HOME=$&#123;JANUSGRAPH_HOME&#125;/ext/spark</span><br><span class="line">export HADOOP_GREMLIN_LIBS=$&#123;JANUSGRAPH_HOME&#125;/ext/spark:$&#123;JANUSGRAPH_HOME&#125;/ext/atlas:$&#123;JANUSGRAPH_HOME&#125;/lib</span><br><span class="line">export HADOOP_CONF_DIR=/etc/hadoop/conf</span><br><span class="line">export HBASE_CONF_DIR=/etc/hbase/conf</span><br><span class="line">export CLASSPATH=$CLASSPATH:$HADOOP_CONF_DIR:HBASE_CONF_DIR</span><br></pre></td></tr></table></figure>

<h3 id="JanusGraph-配置"><a href="#JanusGraph-配置" class="headerlink" title="JanusGraph 配置"></a>JanusGraph 配置</h3><h4 id="hadoop-graph-配置"><a href="#hadoop-graph-配置" class="headerlink" title="hadoop-graph 配置"></a>hadoop-graph 配置</h4><p>JanusGraph 后端使用 HBase 作为存储。</p>
<p>配置 ${JANUSGRAPH_HOME}&#x2F;conf&#x2F;hadoop-graph&#x2F;read-hbase.properties</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">#</span><br><span class="line"># Hadoop Graph Configuration</span><br><span class="line">#</span><br><span class="line">gremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph</span><br><span class="line">gremlin.hadoop.graphReader=org.janusgraph.hadoop.formats.hbase.HBaseInputFormat</span><br><span class="line">gremlin.hadoop.graphWriter=org.apache.hadoop.mapreduce.lib.output.NullOutputFormat</span><br><span class="line"></span><br><span class="line">gremlin.hadoop.jarsInDistributedCache=true</span><br><span class="line">gremlin.hadoop.inputLocation=none</span><br><span class="line">gremlin.hadoop.outputLocation=output</span><br><span class="line">gremlin.spark.persistContext=true</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># JanusGraph HBase InputFormat configuration</span><br><span class="line">#</span><br><span class="line">janusgraphmr.ioformat.conf.storage.backend=hbase</span><br><span class="line">janusgraphmr.ioformat.conf.storage.hostname=*****</span><br><span class="line">janusgraphmr.ioformat.conf.storage.hbase.table=apache_atlas_janus</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># SparkGraphComputer Configuration</span><br><span class="line">#</span><br><span class="line">#spark.master=local[*]</span><br><span class="line">#spark.executor.memory=1g</span><br><span class="line">spark.master=yarn-client</span><br><span class="line">spark.executor.memory=2g</span><br><span class="line">spark.executor.instances=4</span><br><span class="line">spark.yarn.principal=******</span><br><span class="line">spark.yarn.keytab=******.keytab</span><br><span class="line">spark.serializer=org.apache.spark.serializer.KryoSerializer</span><br><span class="line">spark.kryo.registrator=org.janusgraph.hadoop.serialize.JanusGraphKryoRegistrator</span><br></pre></td></tr></table></figure>

<h4 id="日志配置"><a href="#日志配置" class="headerlink" title="日志配置"></a>日志配置</h4><p>修改 ${JANUSGRAPH_HOME}&#x2F;conf&#x2F;log4j-console.properties 配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 添加 Rolling_File Log Appender</span><br><span class="line">log4j.appender.Rolling_File=org.apache.log4j.RollingFileAppender</span><br><span class="line">log4j.appender.Rolling_File.Threshold=INFO</span><br><span class="line">log4j.appender.Rolling_File.File=logs/gremlin.log</span><br><span class="line">log4j.appender.Rolling_File.Append=true</span><br><span class="line">log4j.appender.Rolling_File.MaxFileSize=100MB</span><br><span class="line">log4j.appender.Rolling_File.MaxBackupIndex=10</span><br><span class="line">log4j.appender.Rolling_File.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.Rolling_File.layout.ConversionPattern=%-d&#123;yyyy-MM-dd HH\:mm\:ss,SSS&#125; [%C]-[%p] %m%n</span><br><span class="line"></span><br><span class="line">log4j.rootLogger=$&#123;gremlin.log4j.level&#125;, A2, Rolling_File</span><br></pre></td></tr></table></figure>

<p>启动时可通过 -l 修改日志级别</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/gremlin.sh -l info</span><br></pre></td></tr></table></figure>

<h4 id="执行-Hive-Table-Count-任务"><a href="#执行-Hive-Table-Count-任务" class="headerlink" title="执行 Hive Table Count 任务"></a>执行 Hive Table Count 任务</h4><p>启动 Gremlin Console：bin&#x2F;gremlin.sh （可加上 -l info 指定日志级别）。</p>
<p>执行下面语句统计 hive_table 数量，可查看 logs&#x2F;gremlin.log 日志信息，可以看到 Spark on yarn 的 application 信息。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph = GraphFactory.open(&#x27;conf/hadoop-graph/read-hbase.properties&#x27;)</span><br><span class="line">g = graph.traversal().withComputer(SparkGraphComputer)</span><br><span class="line">g.V().has(&quot;__typeName&quot;, &quot;hive_table&quot;).count()</span><br></pre></td></tr></table></figure>

<h4 id="报错-groovy-脚本"><a href="#报错-groovy-脚本" class="headerlink" title="报错 groovy 脚本"></a>报错 groovy 脚本</h4><p>保存一个 groovy 初始化脚本，不用每次都初始化 graph。</p>
<p>编辑 ${JANUSGRAPH_HOME}&#x2F;scripts&#x2F;graph-spark.groovy </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">graph = GraphFactory.open(&#x27;conf/hadoop-graph/read-hbase.properties&#x27;)</span><br><span class="line">g = graph.traversal().withComputer(SparkGraphComputer)</span><br></pre></td></tr></table></figure>

<p>使用 graph-spark.groovy 初始化 脚本启动  gremlin console</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/gremlin.sh -i scripts/graph-spark.groovy</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2021/02/24/Atlas-1-1-0-Full-GC-%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/02/24/Atlas-1-1-0-Full-GC-%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/" itemprop="url">Atlas 1.1.0 Full GC 问题分析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-02-24T16:57:18+08:00">
                2021-02-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Atlas-1-1-0-Full-GC-问题分析"><a href="#Atlas-1-1-0-Full-GC-问题分析" class="headerlink" title="Atlas 1.1.0 Full GC 问题分析"></a>Atlas 1.1.0 Full GC 问题分析</h2><h3 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h3><p>频繁接收到 Atlas 拨测告警，拨测程序是访问 atlas 查询 entity 的接口，一段时间都是 502 的返回，持续几分钟后恢复。查看 Atlas gc 日志，发现 Full GC 日志，并且持续时间和服务不可用时间吻合，基本确定就是由于 Full GC 导致。</p>
<h3 id="GC-LOG-分析"><a href="#GC-LOG-分析" class="headerlink" title="GC LOG 分析"></a>GC LOG 分析</h3><h4 id="JVM-相关参数调整"><a href="#JVM-相关参数调整" class="headerlink" title="JVM 相关参数调整"></a>JVM 相关参数调整</h4><p>查看 GC 日志发现，（concurrent mode failure）导致的 Full GC。</p>
<img src="/2021/02/24/Atlas-1-1-0-Full-GC-%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/full_gc_log_failure.png" class="">

<p>进行 JVM 参数调整，将 -XX:CMSInitiatingOccupancyFraction 设置为 50，即当老年代内存达到 50% 时，触发 CMS GC。<br>，参考：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/ca1b0d4107c5">CMS之promotion failed&amp;concurrent mode failure</a> </p>
<p>完整参数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:+CMSClassUnloadingEnabled -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+HeapDumpOnOutOfMemoryError -XX:CMSInitiatingOccupancyFraction=50 -XX:ParallelGCThreads=20 -XX:+CMSScavengeBeforeRemark -XX:MaxGCPauseMillis=400 -XX:HeapDumpPath=dumps/atlas_server.hprof -Xloggc:logs/gc-worker.log -verbose:gc -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=10m -XX:+PrintGCDetails -XX:+PrintGCDateStamps</span><br></pre></td></tr></table></figure>

<p>调整完参数后，执行以下命令，观察 GC 变化。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jstat -gcutil PID 3000</span><br></pre></td></tr></table></figure>

<p>发现 old Space 达到 50% 时，会触发一次 old GC，old Space 使用会下降，不过持续观察一段时间后，还是会出现 Full GC，调整内存还是会出现 Full GC 问题。</p>
<h4 id="GC-log-分析工具"><a href="#GC-log-分析工具" class="headerlink" title="GC log 分析工具"></a>GC log 分析工具</h4><p>可以使用 GC Log 分析工具，直观的看到 GC 变化。</p>
<h5 id="GCViewer"><a href="#GCViewer" class="headerlink" title="GCViewer"></a>GCViewer</h5><img src="/2021/02/24/Atlas-1-1-0-Full-GC-%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/gcviewer.png" class="">

<p>项目地址：<a target="_blank" rel="noopener" href="https://github.com/chewiebug/GCViewer">https://github.com/chewiebug/GCViewer</a><br>使用说明：<a target="_blank" rel="noopener" href="https://github.com/chewiebug/GCViewer/blob/develop/README.md#results-of-log-analysis">Results of log analysis</a></p>
<h5 id="GCEasy"><a href="#GCEasy" class="headerlink" title="GCEasy"></a>GCEasy</h5><p>在线 GC log 分析，地址: <a target="_blank" rel="noopener" href="https://blog.gceasy.io/">https://blog.gceasy.io/</a></p>
<h3 id="Dump-分析"><a href="#Dump-分析" class="headerlink" title="Dump 分析"></a>Dump 分析</h3><p>经过一些 JVM 参数的调整和 GC Log 分析，还是无法避免 Full GC，感觉可能还是由于创建大量存活对象，顾对程序进行 Dump 分析。</p>
<h4 id="生成-Dump-文件"><a href="#生成-Dump-文件" class="headerlink" title="生成 Dump 文件"></a>生成 Dump 文件</h4><p>生成 Dump 文件的方式有很多种，下面列举了几种方式。由于 jmap 生成 DUMP 文件时会导致服务挂起，对线上服务有影响，所以尝试使用 gcore 方式。不过将 gcore 导出的 core 文件转换成 bin 文件报错，没有具体研究。最终使用的 Arthas heapdump 命令（不太确定是否影响线上服务）。</p>
<blockquote>
<p> Jmap 命令</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 显示Java堆详细信息</span><br><span class="line">jmap -heap [pid]</span><br><span class="line"></span><br><span class="line"># 显示堆中对象统计信息</span><br><span class="line">jmap -histo:live [pid]</span><br><span class="line"></span><br><span class="line"># 获取 dump 文件</span><br><span class="line">jmap -dump:live,:format=b,file=文件名 [pid]</span><br></pre></td></tr></table></figure>

<blockquote>
<p> Arthas heapdump 命令</p>
</blockquote>
<p>具体参考：<a target="_blank" rel="noopener" href="https://arthas.aliyun.com/doc/heapdump.html">Arthas heapdump</a></p>
<blockquote>
<p> Gcore</p>
</blockquote>
<p>具体可参考：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/1c6bc8842463">记一次Java服务频繁Full GC的排查过程</a></p>
<h4 id="分析-Dump-文件"><a href="#分析-Dump-文件" class="headerlink" title="分析 Dump 文件"></a>分析 Dump 文件</h4><p>使用 jhat 分析 dump 文件。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># dump.hprof 为 Dump 文件路径</span><br><span class="line">jhat -J-d64 -J-mx8g -J-ms8g dump.hprof</span><br></pre></td></tr></table></figure>

<p>分析完成后，访问 <a target="_blank" rel="noopener" href="http://ip:7000/">http://IP:7000</a> ，查看详情。点击 ‘Show heap histogram’ 可查看堆中对象统计信息。</p>
<img src="/2021/02/24/Atlas-1-1-0-Full-GC-%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/jhat.png" class="">
<img src="/2021/02/24/Atlas-1-1-0-Full-GC-%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/heap_histogram.png" class="">

<p>点击占用内存较高的对象，并抽样分析引用该对象的对象（References to this object 中的对象），关注到大多都指向了 CacheEdge 和 CacheVertex 两个类型的对象，这两个对象应该是 JanusGraph 中顶底和边。</p>
<p>查看 CacheVertex 的关联对象，发现主要来自 LocalCache 和 CacheEdge 两个类型对象。</p>
<img src="/2021/02/24/Atlas-1-1-0-Full-GC-%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/vertex1.png" class="">
<img src="/2021/02/24/Atlas-1-1-0-Full-GC-%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/vertex2.png" class="">

<h4 id="JanusGraph-缓存"><a href="#JanusGraph-缓存" class="headerlink" title="JanusGraph 缓存"></a>JanusGraph 缓存</h4><p>分析 LocalCache 对象，来自 com.google.common.cache.LocalCache.LocalManualCache 对象，是 Guava 的缓存工具。从对象引用确定到与 JanusGraph 的两个地方的缓存有关：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">org.janusgraph.diskstorage.keycolumnvalue.cache.ExpirationKCVSCache#ExpirationKCVSCache</span><br><span class="line">org.janusgraph.graphdb.transaction.vertexcache.GuavaVertexCache#GuavaVertexCache</span><br></pre></td></tr></table></figure>

<p>ExpirationKCVSCache 用于 JanusGraph 全局的 KeyColumnValue 缓存，db-cache-size 参数表示缓存最大占用总堆内存的百分比（小于 1 时），或指定缓存大小（大于 1 时），默认为 0.3。</p>
<p>GuavaVertexCache 用于 Transaction 中的 Vertex 缓存。tx-cache-size 参数表示一次事务中最大缓存 Vertex 的大小，JanusGraph 中默认为 20000，Atlas 里面默认为 15000。</p>
<p>怀疑是否是缓存过大导致，顾对以下参数进行调整。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">atlas.graph.cache.db-cache-size=0.2 # </span><br><span class="line">atlas.graph.cache.tx-cache-size=100</span><br></pre></td></tr></table></figure>

<h4 id="查找元凶"><a href="#查找元凶" class="headerlink" title="查找元凶"></a>查找元凶</h4><p>对缓存进行调整后发现还是出现 Full GC 继续分析 Dump 文件。</p>
<p>CacheVertex 和 CacheEdge 对象相互引用，看到部分 CacheVertex 被大量 CacheEdge 引用。猜测是否由于一个定点有很多的边导致的。</p>
<img src="/2021/02/24/Atlas-1-1-0-Full-GC-%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/vertex.png" class="">

<p>使用 OQL 查询被引用对象超过 1000 的 CacheVertex，具体语法参考：<a target="_blank" rel="noopener" href="http://cr.openjdk.java.net/~sundar/8022483/webrev.01/raw_files/new/src/share/classes/com/sun/tools/hat/resources/oqlhelp.html">Object Query Language</a>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select &#123; name:v, id: v.id, count: count(referrers(v))&#125; from org.janusgraph.graphdb.vertices.CacheVertex v where count(referrers(v))  &gt; 1000</span><br></pre></td></tr></table></figure>

<img src="/2021/02/24/Atlas-1-1-0-Full-GC-%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/oql_count.png" class="">

<p>可以看到有个定点甚至有几万的边，使用 Gremlin 查询这些 Vertex，发现主要是下面两种情况，这里操作参考之前文章：<a href="/2020/12/14/Gremlin-Server-Console-%E9%80%82%E9%85%8D-Atlas-JanusGraph/" title="Gremlin Server&#x2F;Console 适配 Atlas JanusGraph">Gremlin Server&#x2F;Console 适配 Atlas JanusGraph</a>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">gremlin&gt; g.V(1006100712).properties(&quot;__typeName&quot;)</span><br><span class="line">==&gt;vp[__typeName-&gt;hive_table]</span><br><span class="line">gremlin&gt; g.V(1006100712).inE().count()</span><br><span class="line">==&gt;34436</span><br><span class="line">gremlin&gt; g.V(1006100712).inE()</span><br><span class="line">==&gt;e[51zpq-1mank-2711-gn08a0][2719856-__Process.inputs-&gt;1006100712]</span><br><span class="line">==&gt;e[635se-1yhyo-2711-gn08a0][3289200-__Process.inputs-&gt;1006100712]</span><br><span class="line">==&gt;e[7tmvi-2iycw-2711-gn08a0][4243568-__Process.inputs-&gt;1006100712]</span><br><span class="line">==&gt;e[b50r2-3p6k0-2711-gn08a0][6213744-__Process.inputs-&gt;1006100712]</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">gremlin&gt; g.V(98328).properties(&quot;__typeName&quot;)</span><br><span class="line">==&gt;vp[__typeName-&gt;hive_db]</span><br><span class="line">gremlin&gt; g.V(98328).inE().count()</span><br><span class="line">==&gt;5734</span><br><span class="line">gremlin&gt; g.V(98328).inE()</span><br><span class="line">==&gt;e[8o4f-2goo-acyd-23vc][114936-__hive_table.db-&gt;98328]</span><br><span class="line">==&gt;e[ej2i-44ls-acyd-23vc][192592-__hive_table.db-&gt;98328]</span><br><span class="line">==&gt;e[sl5n-6riw-acyd-23vc][315608-__hive_table.db-&gt;98328]</span><br><span class="line">==&gt;e[tpfy-9qwg-acyd-23vc][454768-__hive_table.db-&gt;98328]</span><br><span class="line">==&gt;e[c2oer-48hw8-acyd-23vc][7114904-__hive_table.db-&gt;98328]</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>可以看到 hive_db 的情况，应该是会查询出 hive_db 的所有 hive_table，正常的操作不应该查询出所有的关联对象，再对程序进行 jstack 分析。</p>
<h4 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h4><p>再对程序进行 jstack 分析，这里需要注意需要在程序正常运行是进行 jstack，不要在 Full GC 发生时进行 jstack。</p>
<p>通过查看 jstack log 注意到的 org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever#mapRelationshipAttributes 方法，这个方法会遍历 relationship 的对象，会导致大量对象的创建。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">&quot;NotificationHookConsumer thread-40&quot; #522 prio=5 os_prio=0 tid=0x00007f897794d000 nid=0x98e5 runnable [0x00007f69c4ecd000]</span><br><span class="line">   java.lang.Thread.State: RUNNABLE</span><br><span class="line">	// ......</span><br><span class="line">	at org.apache.tinkerpop.gremlin.structure.Vertex.property(Vertex.java:38)</span><br><span class="line">	at org.apache.atlas.repository.graphdb.janus.AtlasJanusElement.getProperty(AtlasJanusElement.java:65)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.AtlasGraphUtilsV2.getIdFromVertex(AtlasGraphUtilsV2.java:105)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapVertexToRelatedObjectId(EntityGraphRetriever.java:1025)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapRelatedVertexToObjectId(EntityGraphRetriever.java:991)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapVertexToRelationshipAttribute(EntityGraphRetriever.java:976)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapRelationshipAttributes(EntityGraphRetriever.java:944)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapVertexToAtlasEntity(EntityGraphRetriever.java:418)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapVertexToAtlasEntity(EntityGraphRetriever.java:395)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapVertexToObjectId(EntityGraphRetriever.java:900)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapVertexToCollectionEntry(EntityGraphRetriever.java:819)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapVertexToArray(EntityGraphRetriever.java:787)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapVertexToAttribute(EntityGraphRetriever.java:709)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapAttributes(EntityGraphRetriever.java:568)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapVertexToAtlasEntity(EntityGraphRetriever.java:415)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.toAtlasEntityWithExtInfo(EntityGraphRetriever.java:183)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.toAtlasEntityWithExtInfo(EntityGraphRetriever.java:178)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.toAtlasEntityWithExtInfo(EntityGraphRetriever.java:166)</span><br><span class="line">	at org.apache.atlas.repository.converters.AtlasInstanceConverter.getAndCacheEntity(AtlasInstanceConverter.java:300)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.AtlasEntityChangeNotifier.toAtlasEntities(AtlasEntityChangeNotifier.java:409)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.AtlasEntityChangeNotifier.notifyV2Listeners(AtlasEntityChangeNotifier.java:305)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.AtlasEntityChangeNotifier.notifyListeners(AtlasEntityChangeNotifier.java:275)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.AtlasEntityChangeNotifier.onEntitiesMutated(AtlasEntityChangeNotifier.java:108)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.AtlasEntityStoreV2.createOrUpdate(AtlasEntityStoreV2.java:732)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.AtlasEntityStoreV2.createOrUpdate(AtlasEntityStoreV2.java:253)</span><br><span class="line">	// ......</span><br><span class="line">	</span><br><span class="line">&quot;NotificationHookConsumer thread-11&quot; #464 prio=5 os_prio=0 tid=0x00007f8977919000 nid=0x98c8 runnable [0x00007f69c6beb000]</span><br><span class="line">   java.lang.Thread.State: RUNNABLE</span><br><span class="line">	// ......</span><br><span class="line">	at org.apache.tinkerpop.gremlin.structure.Vertex.property(Vertex.java:72)</span><br><span class="line">	at org.apache.tinkerpop.gremlin.structure.Vertex.property(Vertex.java:38)</span><br><span class="line">	at org.apache.atlas.repository.graphdb.janus.AtlasJanusElement.getProperty(AtlasJanusElement.java:65)</span><br><span class="line">	at org.apache.atlas.repository.graph.GraphHelper.getTypeName(GraphHelper.java:1090)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapSystemAttributes(EntityGraphRetriever.java:1121)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapEdgeToAtlasRelationship(EntityGraphRetriever.java:1080)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapEdgeToAtlasRelationship(EntityGraphRetriever.java:1070)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapVertexToRelatedObjectId(EntityGraphRetriever.java:1033)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapRelationshipArrayAttribute(EntityGraphRetriever.java:1010)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapVertexToRelationshipAttribute(EntityGraphRetriever.java:981)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapRelationshipAttributes(EntityGraphRetriever.java:944)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapVertexToAtlasEntity(EntityGraphRetriever.java:418)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.mapVertexToAtlasEntity(EntityGraphRetriever.java:395)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.EntityGraphRetriever.toAtlasEntity(EntityGraphRetriever.java:162)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.AtlasEntityStoreV2.createOrUpdate(AtlasEntityStoreV2.java:699)</span><br><span class="line">	at org.apache.atlas.repository.store.graph.v2.AtlasEntityStoreV2.createOrUpdate(AtlasEntityStoreV2.java:253)</span><br><span class="line">	// ......</span><br></pre></td></tr></table></figure>

<p>根据上面两个线程栈信息，定位在了下面两个方法，具体的修改参考 Atlas 2.1.0 代码进行修改，这里主要是去掉不必要的 Relationship 查询。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">org.apache.atlas.repository.store.graph.v2.AtlasEntityChangeNotifier#toAtlasEntities</span><br><span class="line"></span><br><span class="line">org.apache.atlas.repository.store.graph.v2.AtlasEntityStoreV2#createOrUpdate(org.apache.atlas.repository.store.graph.v2.EntityStream, boolean, boolean)</span><br></pre></td></tr></table></figure>

<p>修改后问题解决，消费性能大幅度提高，后续计划升级至 Atlas 2.1.0。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://wforget.github.io/2020/12/14/Gremlin-Server-Console-%E9%80%82%E9%85%8D-Atlas-JanusGraph/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wForget's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/12/14/Gremlin-Server-Console-%E9%80%82%E9%85%8D-Atlas-JanusGraph/" itemprop="url">Gremlin Server/Console 适配 Atlas JanusGraph</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-12-14T19:00:20+08:00">
                2020-12-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Gremlin-Server-x2F-Console-适配-Atlas-JanusGraph"><a href="#Gremlin-Server-x2F-Console-适配-Atlas-JanusGraph" class="headerlink" title="Gremlin Server&#x2F;Console 适配 Atlas JanusGraph"></a>Gremlin Server&#x2F;Console 适配 Atlas JanusGraph</h3><p>Atlas 底层存储使用的 JanusGraph，由于对于 Atlas 底层数据结构并不太清楚，所以希望能够通过 Gremlin Console 来操作 Atlas 的 JanusGraph，使用 Gremlin Query Language 执行一些更加灵活的查询，并直观的查询数据结构。适配的思路参考了 <a target="_blank" rel="noopener" href="https://github.com/sburn/docker-apache-atlas">docker-apache-atlas</a> 项目。</p>
<h3 id="Gremlin-Server"><a href="#Gremlin-Server" class="headerlink" title="Gremlin Server"></a>Gremlin Server</h3><ol>
<li>下载相同版本的 gremlin server ，解压，并添加 Atlas 相关的依赖包</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ATLAS_HOME=/home/anchor/apache-atlas-2.1.0/</span><br><span class="line">GREMLIN_SERVER_HOME=/home/anchor/gremlin/apache-tinkerpop-gremlin-server-3.4.6</span><br><span class="line">ln -s $&#123;ATLAS_HOME&#125;/server/webapp/atlas/WEB-INF/lib/*.jar $&#123;GREMLIN_SERVER_HOME&#125;/lib 2&gt;/dev/null</span><br><span class="line">rm -f $&#123;GREMLIN_SERVER_HOME&#125;/lib/atlas-webapp-2.1.0.jar</span><br><span class="line">rm -f $&#123;GREMLIN_SERVER_HOME&#125;/lib/netty-3.10.5.Final.jar</span><br><span class="line">rm -f $&#123;GREMLIN_SERVER_HOME&#125;/lib/netty-all-4.0.52.Final.jar</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>gremlin server 配置</li>
</ol>
<p>gremlin-server-atlas-wshttp.yaml 配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">host: 0.0.0.0</span><br><span class="line">port: 8182</span><br><span class="line">scriptEvaluationTimeout: 30000</span><br><span class="line">#channelizer: org.apache.tinkerpop.gremlin.server.channel.WebSocketChannelizer</span><br><span class="line">channelizer: org.apache.tinkerpop.gremlin.server.channel.WsAndHttpChannelizer</span><br><span class="line">graphs: &#123;</span><br><span class="line">  graph: conf/janusgraph-hbase-es.properties</span><br><span class="line">&#125;</span><br><span class="line">scriptEngines: &#123;</span><br><span class="line">  gremlin-groovy: &#123;</span><br><span class="line">    plugins: &#123; org.apache.tinkerpop.gremlin.server.jsr223.GremlinServerGremlinPlugin: &#123;&#125;,</span><br><span class="line">               org.apache.tinkerpop.gremlin.tinkergraph.jsr223.TinkerGraphGremlinPlugin: &#123;&#125;,</span><br><span class="line">               org.apache.tinkerpop.gremlin.jsr223.ImportGremlinPlugin: &#123;classImports: [java.lang.Math], methodImports: [java.lang.Math#*]&#125;,</span><br><span class="line">               org.apache.tinkerpop.gremlin.jsr223.ScriptFileGremlinPlugin: &#123;files: [scripts/empty-sample.groovy]&#125;&#125;&#125;&#125;</span><br><span class="line"># JanusGraph sets default serializers. You need to uncomment the following lines, if you require any custom serializers.</span><br><span class="line">#</span><br><span class="line"># serializers:</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GraphBinaryMessageSerializerV1, config: &#123; ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry] &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GraphBinaryMessageSerializerV1, config: &#123; serializeResultToString: true &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV3d0, config: &#123; ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry] &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV3d0, config: &#123; serializeResultToString: true &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GraphSONMessageSerializerV3d0, config: &#123; ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry] &#125;&#125;</span><br><span class="line">#   # Older serialization versions for backwards compatibility:</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV1d0, config: &#123; ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry] &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GryoLiteMessageSerializerV1d0, config: &#123;ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry] &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV1d0, config: &#123; serializeResultToString: true &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GraphSONMessageSerializerV2d0, config: &#123; ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry] &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GraphSONMessageSerializerGremlinV1d0, config: &#123; ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistryV1d0] &#125;&#125;</span><br><span class="line">#   - &#123; className: org.apache.tinkerpop.gremlin.driver.ser.GraphSONMessageSerializerV1d0, config: &#123; ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistryV1d0] &#125;&#125;</span><br><span class="line">processors:</span><br><span class="line">  - &#123; className: org.apache.tinkerpop.gremlin.server.op.session.SessionOpProcessor, config: &#123; sessionTimeout: 28800000 &#125;&#125;</span><br><span class="line">  - &#123; className: org.apache.tinkerpop.gremlin.server.op.traversal.TraversalOpProcessor, config: &#123; cacheExpirationTime: 600000, cacheMaxSize: 1000 &#125;&#125;</span><br><span class="line">metrics: &#123;</span><br><span class="line">  consoleReporter: &#123;enabled: true, interval: 180000&#125;,</span><br><span class="line">  csvReporter: &#123;enabled: true, interval: 180000, fileName: /tmp/gremlin-server-metrics.csv&#125;,</span><br><span class="line">  jmxReporter: &#123;enabled: true&#125;,</span><br><span class="line">  slf4jReporter: &#123;enabled: true, interval: 180000&#125;,</span><br><span class="line">  graphiteReporter: &#123;enabled: false, interval: 180000&#125;&#125;</span><br><span class="line">maxInitialLineLength: 4096</span><br><span class="line">maxHeaderSize: 8192</span><br><span class="line">maxChunkSize: 8192</span><br><span class="line">maxContentLength: 65536</span><br><span class="line">maxAccumulationBufferComponents: 1024</span><br><span class="line">resultIterationBatchSize: 64</span><br><span class="line">writeBufferLowWaterMark: 32768</span><br><span class="line">writeBufferHighWaterMark: 65536</span><br></pre></td></tr></table></figure>

<p>janusgraph-hbase-es.properties 配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">gremlin.graph=org.janusgraph.core.JanusGraphFactory</span><br><span class="line">storage.backend=hbase</span><br><span class="line">storage.hostname=127.0.0.1:2181</span><br><span class="line">cache.db-cache = true</span><br><span class="line">cache.db-cache-clean-wait = 20</span><br><span class="line">cache.db-cache-time = 180000</span><br><span class="line">cache.db-cache-size = 0.5</span><br><span class="line">storage.hbase.table=apache_atlas_janus</span><br><span class="line">storage.hbase.ext.hbase.security.authentication=kerberos</span><br><span class="line">storage.hbase.ext.hbase.security.authorization=true</span><br><span class="line"></span><br><span class="line">index.search.backend=elasticsearch</span><br><span class="line">index.search.hostname=127.0.0.1:9200</span><br></pre></td></tr></table></figure>

<p>将 HBASE_CONF_DIR 加入 gramlin-server classpath 中（避免 kerberos 认证，连接失败等问题）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># vim gremlin-server.sh</span><br><span class="line">CP=&quot;$GREMLIN_HOME/conf/:$HBASE_CONF_DIR&quot;</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>启动 gramlin server</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup bin/gremlin-server.sh conf/gremlin-server-atlas-wshttp.yaml &gt; gramlin-server.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>


<h3 id="Gremlin-Console"><a href="#Gremlin-Console" class="headerlink" title="Gremlin Console"></a>Gremlin Console</h3><ol>
<li>下载并解压 gremlin console ，启动</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/gremlin.sh</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>连接 gremlin server</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">:remote connect tinkerpop.server conf/remote.yaml session</span><br><span class="line">:remote console</span><br></pre></td></tr></table></figure>

<h3 id="Gremlin-Console-操作"><a href="#Gremlin-Console-操作" class="headerlink" title="Gremlin Console 操作"></a>Gremlin Console 操作</h3><img src="/2020/12/14/Gremlin-Server-Console-%E9%80%82%E9%85%8D-Atlas-JanusGraph/console.png" class="" title="[Gremlin Console]">

<p>查询一个 hive_table 节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// hive_table where qualifiedName = &quot;aaa.test@test&quot;</span><br><span class="line">g = graph.traversal()</span><br><span class="line">g.V().has(&quot;__typeName&quot;, &quot;hive_table&quot;).has(&quot;Referenceable.qualifiedName&quot;, &quot;aaa.test@test&quot;).values()</span><br></pre></td></tr></table></figure>

<p>查看 Graph 的一些信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mgmt = graph.openManagement()</span><br><span class="line"></span><br><span class="line">mgmt.printVertexLabels()	// 打印 VertexLabels 信息</span><br><span class="line">mgmt.printEdgeLabels()		// 打印 EdgeLabels 信息</span><br><span class="line">mgmt.printPropertyKeys()	// 打印 PropertyKeys 信息</span><br><span class="line">mgmt.printIndexes()			// 打印所有索引信息</span><br><span class="line"></span><br><span class="line">mgmt.printSchema()    // 打印 Schema，包括上面的所有信息</span><br><span class="line"></span><br><span class="line">index = mgmt.getGraphIndex(&quot;vertex_index&quot;);  // 获取索引对象</span><br></pre></td></tr></table></figure>

<p>查询 Patch 节点信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">g.V().has(&quot;patch.type&quot;, &quot;TYPEDEF_PATCH&quot;)</span><br><span class="line"></span><br><span class="line">g.V().has(&quot;patch.type&quot;, &quot;JAVA_PATCH&quot;)</span><br></pre></td></tr></table></figure>

<h3 id="Graphexp-安装"><a href="#Graphexp-安装" class="headerlink" title="Graphexp 安装"></a>Graphexp 安装</h3><img src="/2020/12/14/Gremlin-Server-Console-%E9%80%82%E9%85%8D-Atlas-JanusGraph/graphexp.png" class="" title="[Graphexp]">

<p>Graphexp 是一个前端项目，结合 gremlin server 提供图数据的可视化。项目地址：<a target="_blank" rel="noopener" href="https://github.com/bricaud/graphexp">https://github.com/bricaud/graphexp</a></p>
<p>拉取 github 代码，安装 Nginx 并进行如下配置。完成后访问：<a target="_blank" rel="noopener" href="http://localhost:9990/graphexp.html">http://localhost:9990/graphexp.html</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># vim /etc/nginx/conf.d/graphexp-9990.conf</span><br><span class="line">server &#123;</span><br><span class="line">	keepalive_requests 120; #单连接请求上限次数。</span><br><span class="line">	listen       9990;   #监听端口</span><br><span class="line">	location  ~*^.+$ &#123;       #请求的url过滤，正则匹配，~为区分大小写，~*为不区分大小写。</span><br><span class="line">	   root /data/nginx/graphexp;  #插件目录</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h3><ol>
<li>Max frame length of 65536 has been exceeded.</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># io.netty.handler.codec.http.websocketx.CorruptedWebSocketFrameException: Max frame length of 65536 has been exceeded.</span><br><span class="line"># vim conf/remote.yaml</span><br><span class="line">connectionPool: &#123;maxContentLength: 655360&#125;</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/">&lt;i class&#x3D;&quot;fa fa-angle-right&quot;&gt;&lt;&#x2F;i&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">44</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">27</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/wForget" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wangz</span>

  
</div>









        







  <div style="display: none;">
    <script src="//s23.cnzz.com/z_stat.php?id=1276876819&web_id=1276876819" language="JavaScript"></script>
  </div>



        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  













  





  

  

  

  
  

  

  

  

</body>
</html>
